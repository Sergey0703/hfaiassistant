# backend/services/flan_t5_service.py - –ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ô FLAN-T5 –°–ï–†–í–ò–°
"""
–ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Å–µ—Ä–≤–∏—Å –¥–ª—è FLAN-T5 Small –º–æ–¥–µ–ª–∏
–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω –¥–ª—è < 1GB RAM –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
–ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –°–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ import torch –Ω–∞ —Å—Ç—Ä–æ–∫–µ 157
"""

import logging
import time
import os
import asyncio
from typing import List, Dict, Optional
from dataclasses import dataclass

logger = logging.getLogger(__name__)

@dataclass
class T5Response:
    content: str
    model: str
    tokens_used: int
    response_time: float
    success: bool
    error: Optional[str] = None

class FlanT5Service:
    """–£–ø—Ä–æ—â–µ–Ω–Ω—ã–π —Å–µ—Ä–≤–∏—Å –¥–ª—è FLAN-T5 Small"""
    
    def __init__(self):
        self.service_type = "flan_t5"
        self.model_name = os.getenv("LLM_MODEL", "google/flan-t5-small")
        self.hf_token = os.getenv("HF_TOKEN")
        self.ready = False
        self.model = None
        self.tokenizer = None
        
        logger.info(f"ü§ñ Initializing FLAN-T5 service: {self.model_name}")
        self._load_model()
    
    def _load_model(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å FLAN-T5"""
        try:
            from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
            
            logger.info("üì• Loading FLAN-T5 model and tokenizer...")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_name,
                token=self.hf_token if self.hf_token else None
            )
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
            self.model = AutoModelForSeq2SeqLM.from_pretrained(
                self.model_name,
                token=self.hf_token if self.hf_token else None,
                torch_dtype="auto",
                device_map="auto" if self._has_cuda() else "cpu"
            )
            
            self.ready = True
            logger.info("‚úÖ FLAN-T5 model loaded successfully")
            
        except Exception as e:
            logger.error(f"‚ùå Failed to load FLAN-T5 model: {e}")
            self.ready = False
    
    def _has_cuda(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å CUDA"""
        try:
            import torch
            return torch.cuda.is_available()
        except:
            return False
    
    async def answer_legal_question(self, question: str, context_documents: List[Dict], language: str = "en"):
        """–û—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–π –≤–æ–ø—Ä–æ—Å –∏—Å–ø–æ–ª—å–∑—É—è FLAN-T5"""
        start_time = time.time()
        
        try:
            if not self.ready:
                return self._generate_error_response(
                    question, language, "Model not loaded", start_time
                )
            
            # –°—Ç—Ä–æ–∏–º –ø—Ä–æ–º–ø—Ç –¥–ª—è T5
            prompt = self._build_t5_prompt(question, context_documents, language)
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
            response = await self._generate_with_t5(prompt)
            
            if response.success:
                logger.info(f"‚úÖ Generated response: {len(response.content)} chars")
                return response
            else:
                logger.warning(f"‚ùå Generation failed: {response.error}")
                return self._generate_fallback_response(question, context_documents, language, start_time)
                
        except Exception as e:
            logger.error(f"‚ùå Error in answer_legal_question: {e}")
            return self._generate_error_response(question, language, str(e), start_time)
    
    async def _generate_with_t5(self, prompt: str) -> T5Response:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç —Å FLAN-T5"""
        start_time = time.time()
        
        try:
            # –í—ã–ø–æ–ª–Ω—è–µ–º –≤ executor –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏
            result = await asyncio.get_event_loop().run_in_executor(
                None, self._generate_sync, prompt
            )
            
            response_time = time.time() - start_time
            
            if result:
                return T5Response(
                    content=result,
                    model=self.model_name,
                    tokens_used=len(result.split()),
                    response_time=response_time,
                    success=True
                )
            else:
                return T5Response(
                    content="",
                    model=self.model_name,
                    tokens_used=0,
                    response_time=response_time,
                    success=False,
                    error="Empty generation result"
                )
                
        except Exception as e:
            return T5Response(
                content="",
                model=self.model_name,
                tokens_used=0,
                response_time=time.time() - start_time,
                success=False,
                error=str(e)
            )
    
    def _generate_sync(self, prompt: str) -> str:
        """–°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Å T5"""
        try:
            # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
            inputs = self.tokenizer(
                prompt,
                return_tensors="pt",
                max_length=512,
                truncation=True,
                padding=True
            )
            
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è
            max_new_tokens = int(os.getenv("LLM_MAX_TOKENS", "150"))
            
            # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π –∏–º–ø–æ—Ä—Ç torch
            import torch
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_new_tokens,
                    temperature=float(os.getenv("LLM_TEMPERATURE", "0.3")),
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            # –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # –û—á–∏—â–∞–µ–º –æ—Ç–≤–µ—Ç –æ—Ç –ø—Ä–æ–º–ø—Ç–∞
            response = self._clean_t5_response(response, prompt)
            
            return response
            
        except Exception as e:
            logger.error(f"Sync generation error: {e}")
            return ""
    
    def _build_t5_prompt(self, question: str, context_documents: List[Dict], language: str) -> str:
        """–°—Ç—Ä–æ–∏—Ç –ø—Ä–æ–º–ø—Ç –¥–ª—è FLAN-T5 –≤ text2text —Ñ–æ—Ä–º–∞—Ç–µ"""
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
        context = ""
        if context_documents:
            # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–π –¥–æ–∫—É–º–µ–Ω—Ç –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
            doc = context_documents[0]
            content = doc.get('content', '')
            # –û–±—Ä–µ–∑–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è T5 Small
            max_context = int(os.getenv("CONTEXT_TRUNCATE_LENGTH", "300"))
            context = content[:max_context] + "..." if len(content) > max_context else content
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —è–∑—ã–∫–∞
        if language == "uk":
            if context:
                prompt = f"–ö–æ–Ω—Ç–µ–∫—Å—Ç: {context}\n\n–ü–∏—Ç–∞–Ω–Ω—è: {question}\n\n–î–∞–π—Ç–µ —é—Ä–∏–¥–∏—á–Ω—É –≤—ñ–¥–ø–æ–≤—ñ–¥—å –Ω–∞ –æ—Å–Ω–æ–≤—ñ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É:"
            else:
                prompt = f"–ü–∏—Ç–∞–Ω–Ω—è: {question}\n\n–î–∞–π—Ç–µ –∫–æ—Ä–æ—Ç–∫—É —é—Ä–∏–¥–∏—á–Ω—É –≤—ñ–¥–ø–æ–≤—ñ–¥—å:"
        else:
            if context:
                prompt = f"Context: {context}\n\nQuestion: {question}\n\nProvide a legal answer based on the context:"
            else:
                prompt = f"Question: {question}\n\nProvide a brief legal answer:"
        
        return prompt
    
    def _clean_t5_response(self, response: str, prompt: str) -> str:
        """–û—á–∏—â–∞–µ—Ç –æ—Ç–≤–µ—Ç T5 –æ—Ç –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤"""
        if not response:
            return "I need more information to provide a proper legal analysis."
        
        # –£–±–∏—Ä–∞–µ–º –ø—Ä–æ–º–ø—Ç –∏–∑ –æ—Ç–≤–µ—Ç–∞ –µ—Å–ª–∏ –æ–Ω —Ç–∞–º –µ—Å—Ç—å
        if prompt in response:
            response = response.replace(prompt, "").strip()
        
        # –£–±–∏—Ä–∞–µ–º –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è —Ñ—Ä–∞–∑—ã
        lines = response.split('\n')
        cleaned_lines = []
        for line in lines:
            line = line.strip()
            if line and len(line) > 3:
                cleaned_lines.append(line)
                if len(cleaned_lines) >= 3:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É –æ—Ç–≤–µ—Ç–∞
                    break
        
        cleaned = ' '.join(cleaned_lines)
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É
        max_length = 400
        if len(cleaned) > max_length:
            cleaned = cleaned[:max_length] + "..."
        
        return cleaned or "Unable to generate a proper response."
    
    def _generate_fallback_response(self, question: str, context_documents: List[Dict], 
                                  language: str, start_time: float):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç fallback –æ—Ç–≤–µ—Ç —Å –Ω–∞–π–¥–µ–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º"""
        
        if context_documents:
            context_info = f"Found {len(context_documents)} relevant documents"
            source_files = [doc.get('filename', 'Unknown') for doc in context_documents[:2]]
            sources = ', '.join(source_files)
        else:
            context_info = "No relevant documents found"
            sources = "None"
        
        if language == "uk":
            content = f"""üîç **–†–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ—à—É–∫—É**

**–ü–∏—Ç–∞–Ω–Ω—è:** {question}

üìö {context_info}
üìÑ **–î–∂–µ—Ä–µ–ª–∞:** {sources}

‚ö†Ô∏è AI –≤—ñ–¥–ø–æ–≤—ñ–¥—å —Ç–∏–º—á–∞—Å–æ–≤–æ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞, –∞–ª–µ –∑–Ω–∞–π–¥–µ–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ñ –¥–æ–∫—É–º–µ–Ω—Ç–∏ –¥–ª—è —Ä—É—á–Ω–æ–≥–æ –∞–Ω–∞–ª—ñ–∑—É."""
        else:
            content = f"""üîç **Search Results**

**Question:** {question}

üìö {context_info}
üìÑ **Sources:** {sources}

‚ö†Ô∏è AI response temporarily unavailable, but found relevant documents for manual analysis."""
        
        return T5Response(
            content=content,
            model="fallback",
            tokens_used=len(content.split()),
            response_time=time.time() - start_time,
            success=True
        )
    
    def _generate_error_response(self, question: str, language: str, error: str, start_time: float):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –ø—Ä–∏ –æ—à–∏–±–∫–µ"""
        
        if language == "uk":
            content = f"""‚ùå **–ü–æ–º–∏–ª–∫–∞ AI —Å–µ—Ä–≤—ñ—Å—É**

**–ü–∏—Ç–∞–Ω–Ω—è:** {question}

üîß **–ü—Ä–æ–±–ª–µ–º–∞:** {error}

üí° **–†—ñ—à–µ–Ω–Ω—è:**
‚Ä¢ –°–ø—Ä–æ–±—É–π—Ç–µ —â–µ —Ä–∞–∑ —á–µ—Ä–µ–∑ –∫—ñ–ª—å–∫–∞ —Å–µ–∫—É–Ω–¥
‚Ä¢ –ü–µ—Ä–µ—Ñ—Ä–∞–∑—É–π—Ç–µ –ø–∏—Ç–∞–Ω–Ω—è –ø—Ä–æ—Å—Ç—ñ—à–µ
‚Ä¢ –ó–≤–µ—Ä–Ω—ñ—Ç—å—Å—è –¥–æ –∞–¥–º—ñ–Ω—ñ—Å—Ç—Ä–∞—Ç–æ—Ä–∞"""
        else:
            content = f"""‚ùå **AI Service Error**

**Question:** {question}

üîß **Issue:** {error}

üí° **Solutions:**
‚Ä¢ Try again in a few seconds
‚Ä¢ Rephrase question more simply
‚Ä¢ Contact administrator"""
        
        return T5Response(
            content=content,
            model="error",
            tokens_used=len(content.split()),
            response_time=time.time() - start_time,
            success=False,
            error=error
        )
    
    async def get_service_status(self):
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç—É—Å —Å–µ—Ä–≤–∏—Å–∞"""
        return {
            "service_type": self.service_type,
            "model": self.model_name,
            "ready": self.ready,
            "hf_token_configured": bool(self.hf_token),
            "cuda_available": self._has_cuda(),
            "memory_usage": "~400 MB"
        }

def create_flan_t5_service():
    """–°–æ–∑–¥–∞–µ—Ç —ç–∫–∑–µ–º–ø–ª—è—Ä FLAN-T5 —Å–µ—Ä–≤–∏—Å–∞"""
    try:
        return FlanT5Service()
    except Exception as e:
        logger.error(f"‚ùå Failed to create FLAN-T5 service: {e}")
        return create_fallback_service()

def create_fallback_service():
    """–°–æ–∑–¥–∞–µ—Ç fallback —Å–µ—Ä–≤–∏—Å"""
    
    class FallbackService:
        def __init__(self):
            self.service_type = "fallback"
            self.ready = True
        
        async def answer_legal_question(self, question: str, context_documents: list, language: str = "en"):
            content = f"FLAN-T5 service unavailable. Question: {question}. Found {len(context_documents)} documents."
            return T5Response(
                content=content,
                model="fallback",
                tokens_used=len(content.split()),
                response_time=0.1,
                success=True
            )
        
        async def get_service_status(self):
            return {
                "service_type": "fallback",
                "ready": True,
                "error": "FLAN-T5 service not available"
            }
    
    return FallbackService()