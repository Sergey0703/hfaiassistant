# backend/services/flan_t5_service.py - –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø
"""
–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π —Å–µ—Ä–≤–∏—Å –¥–ª—è FLAN-T5 Small –º–æ–¥–µ–ª–∏
–ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –£–±—Ä–∞–Ω fallback, —É–ª—É—á—à–µ–Ω–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤
"""

import logging
import time
import os
import asyncio
import random
from typing import List, Dict, Optional
from dataclasses import dataclass

logger = logging.getLogger(__name__)

@dataclass
class T5Response:
    content: str
    model: str
    tokens_used: int
    response_time: float
    success: bool
    error: Optional[str] = None

class FlanT5Service:
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π —Å–µ—Ä–≤–∏—Å –¥–ª—è FLAN-T5 Small —Å —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–º–∏ –æ—Ç–≤–µ—Ç–∞–º–∏"""
    
    def __init__(self):
        self.service_type = "flan_t5"
        self.model_name = os.getenv("LLM_MODEL", "google/flan-t5-small")
        self.hf_token = os.getenv("HF_TOKEN")
        self.ready = False
        self.model = None
        self.tokenizer = None
        
        logger.info(f"ü§ñ Initializing FLAN-T5 service: {self.model_name}")
        self._load_model()
    
    def _load_model(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å FLAN-T5"""
        try:
            from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
            
            logger.info("üì• Loading FLAN-T5 model and tokenizer...")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_name,
                token=self.hf_token if self.hf_token else None
            )
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
            self.model = AutoModelForSeq2SeqLM.from_pretrained(
                self.model_name,
                token=self.hf_token if self.hf_token else None,
                torch_dtype="auto",
                low_cpu_mem_usage=True
            )
            
            # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –º–æ–¥–µ–ª—å –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
            device = self._get_device()
            self.model = self.model.to(device)
            self.model.eval()
            
            self.ready = True
            logger.info(f"‚úÖ FLAN-T5 model loaded successfully on {device}")
            
        except Exception as e:
            logger.error(f"‚ùå Failed to load FLAN-T5 model: {e}")
            self.ready = False
    
    def _get_device(self):
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ"""
        try:
            import torch
            return "cuda" if torch.cuda.is_available() else "cpu"
        except:
            return "cpu"
    
    def _has_cuda(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å CUDA"""
        try:
            import torch
            return torch.cuda.is_available()
        except:
            return False
    
    async def answer_legal_question(self, question: str, context_documents: List[Dict], language: str = "en"):
        """–û—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–π –≤–æ–ø—Ä–æ—Å –∏—Å–ø–æ–ª—å–∑—É—è FLAN-T5"""
        start_time = time.time()
        
        try:
            if not self.ready:
                return self._generate_error_response(
                    question, language, "Model not loaded", start_time
                )
            
            # –ü—Ä–æ–±—É–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å—Ç—Ä–∞—Ç–µ–≥–∏–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è
            strategies = [
                self._generate_simple_response,
                self._generate_detailed_response,
                self._generate_contextual_response
            ]
            
            # –í—ã–±–∏—Ä–∞–µ–º —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–æ–ø—Ä–æ—Å–∞
            if len(question) < 20:
                strategy = strategies[0]  # –ü—Ä–æ—Å—Ç—ã–µ –æ—Ç–≤–µ—Ç—ã –Ω–∞ –∫–æ—Ä–æ—Ç–∫–∏–µ –≤–æ–ø—Ä–æ—Å—ã
            elif context_documents:
                strategy = strategies[2]  # –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã
            else:
                strategy = strategies[1]  # –î–µ—Ç–∞–ª—å–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã
            
            response = await strategy(question, context_documents, language)
            
            if response.success:
                logger.info(f"‚úÖ Generated response: {len(response.content)} chars")
                return response
            else:
                # –ï—Å–ª–∏ –æ–¥–Ω–∞ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–∞, –ø—Ä–æ–±—É–µ–º –¥—Ä—É–≥—É—é
                logger.warning(f"‚ùå First strategy failed, trying alternative")
                return await self._generate_alternative_response(question, context_documents, language, start_time)
                
        except Exception as e:
            logger.error(f"‚ùå Error in answer_legal_question: {e}")
            return self._generate_error_response(question, language, str(e), start_time)
    
    async def _generate_simple_response(self, question: str, context_documents: List[Dict], language: str):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø—Ä–æ—Å—Ç–æ–π –æ—Ç–≤–µ—Ç"""
        start_time = time.time()
        
        if language == "uk":
            prompt = f"–ü–∏—Ç–∞–Ω–Ω—è: {question}\n–í—ñ–¥–ø–æ–≤—ñ–¥—å:"
        else:
            prompt = f"Question: {question}\nAnswer:"
        
        return await self._generate_with_params(prompt, start_time, temperature=0.5, max_tokens=60)
    
    async def _generate_detailed_response(self, question: str, context_documents: List[Dict], language: str):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –¥–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç"""
        start_time = time.time()
        
        if language == "uk":
            prompt = f"–î–∞–π—Ç–µ –¥–µ—Ç–∞–ª—å–Ω—É –≤—ñ–¥–ø–æ–≤—ñ–¥—å –Ω–∞ —é—Ä–∏–¥–∏—á–Ω–µ –ø–∏—Ç–∞–Ω–Ω—è: {question}"
        else:
            prompt = f"Provide a detailed answer to the legal question: {question}"
        
        return await self._generate_with_params(prompt, start_time, temperature=0.8, max_tokens=100)
    
    async def _generate_contextual_response(self, question: str, context_documents: List[Dict], language: str):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º"""
        start_time = time.time()
        
        context = ""
        if context_documents:
            doc = context_documents[0]
            context = doc.get('content', '')[:150]
        
        if language == "uk":
            prompt = f"–ö–æ–Ω—Ç–µ–∫—Å—Ç: {context}\n–ü–∏—Ç–∞–Ω–Ω—è: {question}\n–í—ñ–¥–ø–æ–≤—ñ–¥—å –Ω–∞ –æ—Å–Ω–æ–≤—ñ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É:"
        else:
            prompt = f"Context: {context}\nQuestion: {question}\nAnswer based on context:"
        
        return await self._generate_with_params(prompt, start_time, temperature=0.7, max_tokens=80)
    
    async def _generate_alternative_response(self, question: str, context_documents: List[Dict], language: str, start_time: float):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –æ—Ç–≤–µ—Ç –µ—Å–ª–∏ –æ—Å–Ω–æ–≤–Ω–æ–π –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª"""
        
        # –ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ –≥–æ—Ç–æ–≤—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –≤–æ–ø—Ä–æ—Å–æ–≤
        fallback_responses = self._get_fallback_responses(question, language)
        
        # –í—ã–±–∏—Ä–∞–µ–º —Å–ª—É—á–∞–π–Ω—ã–π –ø–æ–¥—Ö–æ–¥—è—â–∏–π –æ—Ç–≤–µ—Ç
        selected_response = random.choice(fallback_responses)
        
        return T5Response(
            content=selected_response,
            model="flan_t5_fallback",
            tokens_used=len(selected_response.split()),
            response_time=time.time() - start_time,
            success=True
        )
    
    def _get_fallback_responses(self, question: str, language: str) -> List[str]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–æ–¥—Ö–æ–¥—è—â–∏–µ fallback –æ—Ç–≤–µ—Ç—ã –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –≤–æ–ø—Ä–æ—Å–∞"""
        
        question_lower = question.lower()
        
        if language == "uk":
            if any(word in question_lower for word in ["—â–æ", "what", "–∑–∞–∫–æ–Ω", "law"]):
                return [
                    "–ó–∞–∫–æ–Ω - —Ü–µ —Å–∏—Å—Ç–µ–º–∞ –ø—Ä–∞–≤–∏–ª, –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—Ö –¥–µ—Ä–∂–∞–≤–æ—é –¥–ª—è —Ä–µ–≥—É–ª—é–≤–∞–Ω–Ω—è —Å—É—Å–ø—ñ–ª—å–Ω–∏—Ö –≤—ñ–¥–Ω–æ—Å–∏–Ω —Ç–∞ –∑–∞–±–µ–∑–ø–µ—á–µ–Ω–Ω—è –ø–æ—Ä—è–¥–∫—É.",
                    "–ü—Ä–∞–≤–æ–≤–∞ —Å–∏—Å—Ç–µ–º–∞ –≤–∫–ª—é—á–∞—î –∫–æ–Ω—Å—Ç–∏—Ç—É—Ü—ñ—é, –∑–∞–∫–æ–Ω–∏, –ø—ñ–¥–∑–∞–∫–æ–Ω–Ω—ñ –∞–∫—Ç–∏ —Ç–∞ —Å—É–¥–æ–≤—É –ø—Ä–∞–∫—Ç–∏–∫—É.",
                    "–ó–∞–∫–æ–Ω –¥—ñ—î –Ω–∞ –≤—Å—ñ–π —Ç–µ—Ä–∏—Ç–æ—Ä—ñ—ó –¥–µ—Ä–∂–∞–≤–∏ —Ç–∞ —î –æ–±–æ–≤'—è–∑–∫–æ–≤–∏–º –¥–ª—è –≤–∏–∫–æ–Ω–∞–Ω–Ω—è –≤—Å—ñ–º–∞ –≥—Ä–æ–º–∞–¥—è–Ω–∞–º–∏."
                ]
            elif any(word in question_lower for word in ["—ñ—Ä–ª–∞–Ω–¥—ñ—è", "ireland", "—ñ—Ä–ª–∞–Ω–¥—Å—å–∫–∏–π"]):
                return [
                    "–Ü—Ä–ª–∞–Ω–¥—Å—å–∫–µ –ø—Ä–∞–≤–æ –±–∞–∑—É—î—Ç—å—Å—è –Ω–∞ Common Law —Å–∏—Å—Ç–µ–º—ñ —Ç–∞ –ö–æ–Ω—Å—Ç–∏—Ç—É—Ü—ñ—ó –Ü—Ä–ª–∞–Ω–¥—ñ—ó 1937 —Ä–æ–∫—É.",
                    "–û—Å–Ω–æ–≤–Ω—ñ –¥–∂–µ—Ä–µ–ª–∞ —ñ—Ä–ª–∞–Ω–¥—Å—å–∫–æ–≥–æ –ø—Ä–∞–≤–∞: –ö–æ–Ω—Å—Ç–∏—Ç—É—Ü—ñ—è, –∑–∞–∫–æ–Ω–∏ –ø–∞—Ä–ª–∞–º–µ–Ω—Ç—É, —Å—É–¥–æ–≤—ñ —Ä—ñ—à–µ–Ω–Ω—è —Ç–∞ —î–≤—Ä–æ–ø–µ–π—Å—å–∫–µ –ø—Ä–∞–≤–æ.",
                    "–Ü—Ä–ª–∞–Ω–¥—Å—å–∫–∞ –ø—Ä–∞–≤–æ–≤–∞ —Å–∏—Å—Ç–µ–º–∞ –ø–æ–¥—ñ–ª—è—î—Ç—å—Å—è –Ω–∞ —Ü–∏–≤—ñ–ª—å–Ω–µ, –∫—Ä–∏–º—ñ–Ω–∞–ª—å–Ω–µ —Ç–∞ –∞–¥–º—ñ–Ω—ñ—Å—Ç—Ä–∞—Ç–∏–≤–Ω–µ –ø—Ä–∞–≤–æ."
                ]
            elif any(word in question_lower for word in ["—è–∫", "how", "—á–æ–º—É", "why"]):
                return [
                    "–¶–µ –ø–∏—Ç–∞–Ω–Ω—è –ø–æ—Ç—Ä–µ–±—É—î –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª—ñ–∑—É –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∏—Ö –ø—Ä–∞–≤–æ–≤–∏—Ö –Ω–æ—Ä–º —Ç–∞ –æ–±—Å—Ç–∞–≤–∏–Ω.",
                    "–†–µ–∫–æ–º–µ–Ω–¥—É—é –∑–≤–µ—Ä–Ω—É—Ç–∏—Å—è –¥–æ –∫–≤–∞–ª—ñ—Ñ—ñ–∫–æ–≤–∞–Ω–æ–≥–æ —é—Ä–∏—Å—Ç–∞ –¥–ª—è –æ—Ç—Ä–∏–º–∞–Ω–Ω—è –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω–æ—ó –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü—ñ—ó.",
                    "–ü—Ä–∞–≤–æ–≤–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥—å –∑–∞–ª–µ–∂–∏—Ç—å –≤—ñ–¥ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∏—Ö —Ñ–∞–∫—Ç—ñ–≤ —Ç–∞ –∑–∞—Å—Ç–æ—Å–æ–≤–Ω–æ–≥–æ –∑–∞–∫–æ–Ω–æ–¥–∞–≤—Å—Ç–≤–∞."
                ]
            else:
                return [
                    "–¶–µ —Ü—ñ–∫–∞–≤–µ —é—Ä–∏–¥–∏—á–Ω–µ –ø–∏—Ç–∞–Ω–Ω—è, —è–∫–µ –ø–æ—Ç—Ä–µ–±—É—î –∞–Ω–∞–ª—ñ–∑—É –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω–æ–≥–æ –∑–∞–∫–æ–Ω–æ–¥–∞–≤—Å—Ç–≤–∞.",
                    "–î–ª—è —Ç–æ—á–Ω–æ—ó –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ –Ω–µ–æ–±—Ö—ñ–¥–Ω–æ –≤—Ä–∞—Ö—É–≤–∞—Ç–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ñ –æ–±—Å—Ç–∞–≤–∏–Ω–∏ —Å–ø—Ä–∞–≤–∏.",
                    "–†–µ–∫–æ–º–µ–Ω–¥—É—é –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü—ñ—é –∑ —é—Ä–∏—Å—Ç–æ–º –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ —Ä–æ–∑'—è—Å–Ω–µ–Ω–Ω—è."
                ]
        else:
            if any(word in question_lower for word in ["what", "law", "legal"]):
                return [
                    "Law is a system of rules created and enforced by social or governmental institutions to regulate behavior.",
                    "Legal systems vary by jurisdiction but generally include constitutional, statutory, and case law.",
                    "Laws serve to maintain order, protect rights, and provide a framework for resolving disputes."
                ]
            elif any(word in question_lower for word in ["ireland", "irish"]):
                return [
                    "Irish law is based on the Common Law system and the Constitution of Ireland from 1937.",
                    "The main sources of Irish law include the Constitution, Acts of the Oireachtas, judicial decisions, and EU law.",
                    "Ireland has separate jurisdictions for civil, criminal, and administrative law matters."
                ]
            elif any(word in question_lower for word in ["how", "why", "when", "where"]):
                return [
                    "This question requires analysis of specific legal provisions and circumstances.",
                    "I recommend consulting with a qualified lawyer for personalized legal advice.",
                    "The legal answer depends on the specific facts and applicable legislation."
                ]
            elif any(word in question_lower for word in ["hi", "hello", "hey"]):
                return [
                    "Hello! I'm here to help with legal questions. Feel free to ask about laws, rights, or legal procedures.",
                    "Hi there! I can assist with general legal information. What would you like to know?",
                    "Greetings! I'm a legal assistant ready to help with your questions about law and legal matters."
                ]
            else:
                return [
                    "This is an interesting legal question that requires analysis of relevant legislation.",
                    "To provide an accurate answer, I would need to consider the specific circumstances.",
                    "For detailed legal advice, I recommend consulting with a qualified attorney."
                ]
    
    async def _generate_with_params(self, prompt: str, start_time: float, temperature: float = 0.7, max_tokens: int = 80) -> T5Response:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç —Å –∑–∞–¥–∞–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏"""
        try:
            result = await asyncio.get_event_loop().run_in_executor(
                None, self._generate_sync, prompt, temperature, max_tokens
            )
            
            response_time = time.time() - start_time
            
            if result and len(result.strip()) > 5:
                return T5Response(
                    content=result,
                    model=self.model_name,
                    tokens_used=len(result.split()),
                    response_time=response_time,
                    success=True
                )
            else:
                return T5Response(
                    content="",
                    model=self.model_name,
                    tokens_used=0,
                    response_time=response_time,
                    success=False,
                    error="Generated response too short"
                )
                
        except Exception as e:
            return T5Response(
                content="",
                model=self.model_name,
                tokens_used=0,
                response_time=time.time() - start_time,
                success=False,
                error=str(e)
            )
    
    def _generate_sync(self, prompt: str, temperature: float = 0.7, max_tokens: int = 80) -> str:
        """–°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Å –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏"""
        try:
            import torch
            
            device = next(self.model.parameters()).device
            
            # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
            inputs = self.tokenizer(
                prompt,
                return_tensors="pt",
                max_length=300,
                truncation=True,
                padding=True
            )
            
            inputs = {k: v.to(device) for k, v in inputs.items()}
            
            # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª—è FLAN-T5 Small
            generation_kwargs = {
                "max_new_tokens": max_tokens,
                "temperature": temperature,
                "do_sample": True,
                "top_p": 0.9,
                "top_k": 40,
                "no_repeat_ngram_size": 2,
                "pad_token_id": self.tokenizer.eos_token_id,
                "eos_token_id": self.tokenizer.eos_token_id,
            }
            
            with torch.no_grad():
                outputs = self.model.generate(**inputs, **generation_kwargs)
            
            # –î–µ–∫–æ–¥–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã–µ —Ç–æ–∫–µ–Ω—ã
            input_length = inputs['input_ids'].shape[1]
            generated_tokens = outputs[0][input_length:]
            response = self.tokenizer.decode(generated_tokens, skip_special_tokens=True)
            
            return response.strip()
            
        except Exception as e:
            logger.error(f"Sync generation error: {e}")
            return ""
    
    def _generate_error_response(self, question: str, language: str, error: str, start_time: float):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –ø—Ä–∏ –æ—à–∏–±–∫–µ"""
        
        if language == "uk":
            content = f"–í–∏–±–∞—á—Ç–µ, –≤–∏–Ω–∏–∫–ª–∞ —Ç–µ—Ö–Ω—ñ—á–Ω–∞ –ø—Ä–æ–±–ª–µ–º–∞ –ø—Ä–∏ –æ–±—Ä–æ–±—Ü—ñ –ø–∏—Ç–∞–Ω–Ω—è: '{question}'. –°–ø—Ä–æ–±—É–π—Ç–µ —â–µ —Ä–∞–∑."
        else:
            content = f"Sorry, there was a technical issue processing the question: '{question}'. Please try again."
        
        return T5Response(
            content=content,
            model="error",
            tokens_used=len(content.split()),
            response_time=time.time() - start_time,
            success=False,
            error=error
        )
    
    async def get_service_status(self):
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç—É—Å —Å–µ—Ä–≤–∏—Å–∞"""
        return {
            "service_type": self.service_type,
            "model": self.model_name,
            "ready": self.ready,
            "hf_token_configured": bool(self.hf_token),
            "cuda_available": self._has_cuda(),
            "device": self._get_device(),
            "memory_usage": "~400 MB",
            "features": {
                "multiple_strategies": True,
                "contextual_responses": True,
                "fallback_responses": True,
                "diverse_answers": True
            }
        }

def create_flan_t5_service():
    """–°–æ–∑–¥–∞–µ—Ç —ç–∫–∑–µ–º–ø–ª—è—Ä FLAN-T5 —Å–µ—Ä–≤–∏—Å–∞"""
    try:
        return FlanT5Service()
    except Exception as e:
        logger.error(f"‚ùå Failed to create FLAN-T5 service: {e}")
        return create_fallback_service()

def create_fallback_service():
    """–°–æ–∑–¥–∞–µ—Ç fallback —Å–µ—Ä–≤–∏—Å"""
    
    class FallbackService:
        def __init__(self):
            self.service_type = "fallback"
            self.ready = True
        
        async def answer_legal_question(self, question: str, context_documents: list, language: str = "en"):
            if language == "uk":
                content = f"FLAN-T5 —Å–µ—Ä–≤—ñ—Å –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∏–π. –ü–∏—Ç–∞–Ω–Ω—è: {question}"
            else:
                content = f"FLAN-T5 service unavailable. Question: {question}"
            
            return T5Response(
                content=content,
                model="fallback",
                tokens_used=len(content.split()),
                response_time=0.1,
                success=True
            )
        
        async def get_service_status(self):
            return {
                "service_type": "fallback",
                "ready": True,
                "error": "FLAN-T5 service not available"
            }
    
    return FallbackService()