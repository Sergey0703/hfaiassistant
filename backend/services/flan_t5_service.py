# backend/services/flan_t5_service.py - –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–ê–Ø –í–ï–†–°–ò–Ø
"""
–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–µ—Ä–≤–∏—Å –¥–ª—è FLAN-T5 Small –º–æ–¥–µ–ª–∏
–ò–°–ü–†–ê–í–õ–ï–ù–ò–Ø: –£–ª—É—á—à–µ–Ω–Ω—ã–µ –ø—Ä–æ–º–ø—Ç—ã, –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏, —Å–∫–æ—Ä–æ—Å—Ç—å
"""

import logging
import time
import os
import asyncio
from typing import List, Dict, Optional
from dataclasses import dataclass

logger = logging.getLogger(__name__)

@dataclass
class T5Response:
    content: str
    model: str
    tokens_used: int
    response_time: float
    success: bool
    error: Optional[str] = None

class FlanT5Service:
    """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–µ—Ä–≤–∏—Å –¥–ª—è FLAN-T5 Small"""
    
    def __init__(self):
        self.service_type = "flan_t5"
        self.model_name = os.getenv("LLM_MODEL", "google/flan-t5-small")
        self.hf_token = os.getenv("HF_TOKEN")
        self.ready = False
        self.model = None
        self.tokenizer = None
        
        logger.info(f"ü§ñ Initializing FLAN-T5 service: {self.model_name}")
        self._load_model()
    
    def _load_model(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å FLAN-T5 —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π"""
        try:
            from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
            
            logger.info("üì• Loading FLAN-T5 model and tokenizer...")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_name,
                token=self.hf_token if self.hf_token else None
            )
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π
            self.model = AutoModelForSeq2SeqLM.from_pretrained(
                self.model_name,
                token=self.hf_token if self.hf_token else None,
                torch_dtype="auto",
                low_cpu_mem_usage=True  # –≠–∫–æ–Ω–æ–º–∏—è –ø–∞–º—è—Ç–∏
            )
            
            # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –º–æ–¥–µ–ª—å –Ω–∞ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
            device = self._get_device()
            self.model = self.model.to(device)
            
            # –û–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å –¥–ª—è inference
            self.model.eval()  # –ü–µ—Ä–µ–≤–æ–¥–∏–º –≤ —Ä–µ–∂–∏–º –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞
            
            self.ready = True
            logger.info(f"‚úÖ FLAN-T5 model loaded successfully on {device}")
            
        except Exception as e:
            logger.error(f"‚ùå Failed to load FLAN-T5 model: {e}")
            self.ready = False
    
    def _get_device(self):
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ"""
        try:
            import torch
            if torch.cuda.is_available():
                return "cuda"
            else:
                return "cpu"
        except:
            return "cpu"
    
    def _has_cuda(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å CUDA"""
        try:
            import torch
            return torch.cuda.is_available()
        except:
            return False
    
    async def answer_legal_question(self, question: str, context_documents: List[Dict], language: str = "en"):
        """–û—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–π –≤–æ–ø—Ä–æ—Å –∏—Å–ø–æ–ª—å–∑—É—è FLAN-T5"""
        start_time = time.time()
        
        try:
            if not self.ready:
                return self._generate_error_response(
                    question, language, "Model not loaded", start_time
                )
            
            # –°—Ç—Ä–æ–∏–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è T5
            prompt = self._build_optimized_t5_prompt(question, context_documents, language)
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
            response = await self._generate_with_t5(prompt)
            
            if response.success and len(response.content.strip()) > 15:  # –£–≤–µ–ª–∏—á–∏–ª–∏ –ø–æ—Ä–æ–≥ —Å 10 –¥–æ 15
                logger.info(f"‚úÖ Generated response: {len(response.content)} chars")
                return response
            else:
                logger.warning(f"‚ùå Generation failed or too short ({len(response.content.strip())} chars): {response.error or 'Short response'}")
                return self._generate_fallback_response(question, context_documents, language, start_time)
                
        except Exception as e:
            logger.error(f"‚ùå Error in answer_legal_question: {e}")
            return self._generate_error_response(question, language, str(e), start_time)
    
    async def _generate_with_t5(self, prompt: str) -> T5Response:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç —Å FLAN-T5 –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ"""
        start_time = time.time()
        
        try:
            # –í—ã–ø–æ–ª–Ω—è–µ–º –≤ executor –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏
            result = await asyncio.get_event_loop().run_in_executor(
                None, self._generate_sync_optimized, prompt
            )
            
            response_time = time.time() - start_time
            
            if result and len(result.strip()) > 5:
                return T5Response(
                    content=result,
                    model=self.model_name,
                    tokens_used=len(result.split()),
                    response_time=response_time,
                    success=True
                )
            else:
                return T5Response(
                    content="",
                    model=self.model_name,
                    tokens_used=0,
                    response_time=response_time,
                    success=False,
                    error="Generated response too short or empty"
                )
                
        except Exception as e:
            return T5Response(
                content="",
                model=self.model_name,
                tokens_used=0,
                response_time=time.time() - start_time,
                success=False,
                error=str(e)
            )
    
    def _generate_sync_optimized(self, prompt: str) -> str:
        """–û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–ê–Ø —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Å T5"""
        try:
            import torch
            
            # –ü–æ–ª—É—á–∞–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏
            device = next(self.model.parameters()).device
            
            # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
            inputs = self.tokenizer(
                prompt,
                return_tensors="pt",
                max_length=400,  # –£–º–µ–Ω—å—à–µ–Ω–æ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
                truncation=True,
                padding=True
            )
            
            # –ü–µ—Ä–µ–º–µ—â–∞–µ–º inputs –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏
            inputs = {k: v.to(device) for k, v in inputs.items()}
            
            # –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–´–ï –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            max_new_tokens = int(os.getenv("LLM_MAX_TOKENS", "120"))  # –£–≤–µ–ª–∏—á–µ–Ω–æ –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤
            temperature = float(os.getenv("LLM_TEMPERATURE", "0.8"))  # –ï—â–µ –±–æ–ª—å—à–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è
            
            generation_kwargs = {
                "max_new_tokens": max_new_tokens,
                "min_new_tokens": 15,  # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: min_new_tokens –≤–º–µ—Å—Ç–æ min_length
                "temperature": temperature,
                "do_sample": True,
                "top_p": 0.9,
                "top_k": 50,
                "no_repeat_ngram_size": 2,  # –£–º–µ–Ω—å—à–µ–Ω–æ –¥–ª—è –º–µ–Ω—å—à–∏—Ö –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π
                "pad_token_id": self.tokenizer.eos_token_id,
                "eos_token_id": self.tokenizer.eos_token_id,
                "num_beams": 1,  # –£–±—Ä–∞–ª–∏ early_stopping - –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è
                "length_penalty": 1.2,  # –î–û–ë–ê–í–õ–ï–ù–û: –ø–æ–æ—â—Ä—è–µ–º –±–æ–ª–µ–µ –¥–ª–∏–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã
            }
            
            logger.debug(f"üîß Generation params: max_tokens={max_new_tokens}, temp={temperature}")
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    **generation_kwargs
                )
            
            # –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ - –±–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã–µ —Ç–æ–∫–µ–Ω—ã
            input_length = inputs['input_ids'].shape[1]
            generated_tokens = outputs[0][input_length:]
            response = self.tokenizer.decode(generated_tokens, skip_special_tokens=True)
            
            # –û—á–∏—â–∞–µ–º –æ—Ç–≤–µ—Ç
            response = self._clean_t5_response_optimized(response)
            
            logger.debug(f"üéØ Raw response length: {len(response)} chars")
            
            return response
            
        except Exception as e:
            logger.error(f"Sync generation error: {e}")
            return ""
    
    def _build_optimized_t5_prompt(self, question: str, context_documents: List[Dict], language: str) -> str:
        """–û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–´–ô –ø—Ä–æ–º–ø—Ç –¥–ª—è FLAN-T5 - –±–æ–ª–µ–µ –¥–µ—Ç–∞–ª—å–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏"""
        
        # –ë–æ–ª–µ–µ –¥–µ—Ç–∞–ª—å–Ω—ã–µ –ø—Ä–æ–º–ø—Ç—ã –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –¥–ª–∏–Ω–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤
        if language == "uk":
            if context_documents:
                doc = context_documents[0]
                content = doc.get('content', '')[:250]  # –ù–µ–º–Ω–æ–≥–æ –±–æ–ª—å—à–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
                prompt = f"–í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—á–∏ –Ω–∞—Å—Ç—É–ø–Ω–∏–π —Ç–µ–∫—Å—Ç: {content}\n\n–î–∞–π—Ç–µ –¥–µ—Ç–∞–ª—å–Ω—É –≤—ñ–¥–ø–æ–≤—ñ–¥—å –Ω–∞ –ø–∏—Ç–∞–Ω–Ω—è: {question}\n–í—ñ–¥–ø–æ–≤—ñ–¥—å –ø–æ–≤–∏–Ω–Ω–∞ –±—É—Ç–∏ –º—ñ–Ω—ñ–º—É–º 3-4 —Ä–µ—á–µ–Ω–Ω—è:"
            else:
                prompt = f"–î–∞–π—Ç–µ –¥–µ—Ç–∞–ª—å–Ω—É –≤—ñ–¥–ø–æ–≤—ñ–¥—å –Ω–∞ —é—Ä–∏–¥–∏—á–Ω–µ –ø–∏—Ç–∞–Ω–Ω—è: {question}\n–í—ñ–¥–ø–æ–≤—ñ–¥—å –ø–æ–≤–∏–Ω–Ω–∞ –º—ñ—Å—Ç–∏—Ç–∏ –ø–æ—è—Å–Ω–µ–Ω–Ω—è —Ç–∞ –±—É—Ç–∏ –º—ñ–Ω—ñ–º—É–º 3-4 —Ä–µ—á–µ–Ω–Ω—è:"
        else:
            if context_documents:
                doc = context_documents[0]
                content = doc.get('content', '')[:250]  # –ù–µ–º–Ω–æ–≥–æ –±–æ–ª—å—à–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
                prompt = f"Using the following text: {content}\n\nProvide a detailed answer to the question: {question}\nThe answer should be at least 3-4 sentences long:"
            else:
                prompt = f"Provide a detailed answer to the legal question: {question}\nThe answer should include explanations and be at least 3-4 sentences long:"
        
        return prompt
    
    def _clean_t5_response_optimized(self, response: str) -> str:
        """–û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–ê–Ø –æ—á–∏—Å—Ç–∫–∞ –æ—Ç–≤–µ—Ç–∞ T5"""
        if not response:
            return "Law is a system of rules and regulations that govern society and ensure order, justice, and protection of individual rights."
        
        # –ü—Ä–æ—Å—Ç–∞—è –æ—á–∏—Å—Ç–∫–∞
        response = response.strip()
        
        # –£–±–∏—Ä–∞–µ–º –æ—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∏–µ –æ—Ç–≤–µ—Ç—ã - –∑–∞–º–µ–Ω—è–µ–º –Ω–∞ –±–∞–∑–æ–≤—ã–π –æ—Ç–≤–µ—Ç
        if len(response) < 15:
            if "law" in response.lower():
                return "Law is a system of rules and regulations established by society to maintain order, protect rights, and ensure justice for all citizens."
            else:
                return "I need more information to provide a comprehensive legal analysis of your question."
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É —Ä–∞–∑—É–º–Ω—ã–º–∏ –ø—Ä–µ–¥–µ–ª–∞–º–∏
        if len(response) > 400:
            # –ò—â–µ–º –ø–æ—Å–ª–µ–¥–Ω–µ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ
            sentences = response.split('.')
            if len(sentences) > 1:
                response = '.'.join(sentences[:-1]) + '.'
            else:
                response = response[:400] + "..."
        
        return response
    
    def _generate_fallback_response(self, question: str, context_documents: List[Dict], 
                                  language: str, start_time: float):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç fallback –æ—Ç–≤–µ—Ç —Å –Ω–∞–π–¥–µ–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º"""
        
        if context_documents:
            context_info = f"Found {len(context_documents)} relevant documents"
            source_files = [doc.get('filename', 'Unknown') for doc in context_documents[:2]]
            sources = ', '.join(source_files)
        else:
            context_info = "No relevant documents found"
            sources = "None"
        
        if language == "uk":
            content = f"""üîç **–†–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ—à—É–∫—É**

**–ü–∏—Ç–∞–Ω–Ω—è:** {question}

üìö {context_info}
üìÑ **–î–∂–µ—Ä–µ–ª–∞:** {sources}

‚ö†Ô∏è AI –≤—ñ–¥–ø–æ–≤—ñ–¥—å —Ç–∏–º—á–∞—Å–æ–≤–æ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞ –∞–±–æ –∑–∞–Ω–∞–¥—Ç–æ –∫–æ—Ä–æ—Ç–∫–∞. –ó–Ω–∞–π–¥–µ–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ñ –¥–æ–∫—É–º–µ–Ω—Ç–∏ –¥–ª—è —Ä—É—á–Ω–æ–≥–æ –∞–Ω–∞–ª—ñ–∑—É."""
        else:
            content = f"""üîç **Search Results**

**Question:** {question}

üìö {context_info}
üìÑ **Sources:** {sources}

‚ö†Ô∏è AI response temporarily unavailable or too short. Found relevant documents for manual analysis."""
        
        return T5Response(
            content=content,
            model="fallback",
            tokens_used=len(content.split()),
            response_time=time.time() - start_time,
            success=True
        )
    
    def _generate_error_response(self, question: str, language: str, error: str, start_time: float):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –ø—Ä–∏ –æ—à–∏–±–∫–µ"""
        
        if language == "uk":
            content = f"""‚ùå **–ü–æ–º–∏–ª–∫–∞ AI —Å–µ—Ä–≤—ñ—Å—É**

**–ü–∏—Ç–∞–Ω–Ω—è:** {question}

üîß **–ü—Ä–æ–±–ª–µ–º–∞:** {error}

üí° **–†—ñ—à–µ–Ω–Ω—è:**
‚Ä¢ –°–ø—Ä–æ–±—É–π—Ç–µ —â–µ —Ä–∞–∑ —á–µ—Ä–µ–∑ –∫—ñ–ª—å–∫–∞ —Å–µ–∫—É–Ω–¥
‚Ä¢ –ü–µ—Ä–µ—Ñ—Ä–∞–∑—É–π—Ç–µ –ø–∏—Ç–∞–Ω–Ω—è –ø—Ä–æ—Å—Ç—ñ—à–µ
‚Ä¢ –ó–≤–µ—Ä–Ω—ñ—Ç—å—Å—è –¥–æ –∞–¥–º—ñ–Ω—ñ—Å—Ç—Ä–∞—Ç–æ—Ä–∞"""
        else:
            content = f"""‚ùå **AI Service Error**

**Question:** {question}

üîß **Issue:** {error}

üí° **Solutions:**
‚Ä¢ Try again in a few seconds
‚Ä¢ Rephrase question more simply
‚Ä¢ Contact administrator"""
        
        return T5Response(
            content=content,
            model="error",
            tokens_used=len(content.split()),
            response_time=time.time() - start_time,
            success=False,
            error=error
        )
    
    async def get_service_status(self):
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç—É—Å —Å–µ—Ä–≤–∏—Å–∞"""
        return {
            "service_type": self.service_type,
            "model": self.model_name,
            "ready": self.ready,
            "hf_token_configured": bool(self.hf_token),
            "cuda_available": self._has_cuda(),
            "device": self._get_device(),
            "memory_usage": "~400 MB",
            "optimization": {
                "eval_mode": True,
                "low_cpu_mem_usage": True,
                "optimized_prompts": True,
                "min_response_length": 20
            }
        }

def create_flan_t5_service():
    """–°–æ–∑–¥–∞–µ—Ç —ç–∫–∑–µ–º–ø–ª—è—Ä FLAN-T5 —Å–µ—Ä–≤–∏—Å–∞"""
    try:
        return FlanT5Service()
    except Exception as e:
        logger.error(f"‚ùå Failed to create FLAN-T5 service: {e}")
        return create_fallback_service()

def create_fallback_service():
    """–°–æ–∑–¥–∞–µ—Ç fallback —Å–µ—Ä–≤–∏—Å"""
    
    class FallbackService:
        def __init__(self):
            self.service_type = "fallback"
            self.ready = True
        
        async def answer_legal_question(self, question: str, context_documents: list, language: str = "en"):
            if language == "uk":
                content = f"""üìö **FLAN-T5 —Å–µ—Ä–≤—ñ—Å –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∏–π**

**–ü–∏—Ç–∞–Ω–Ω—è:** {question}

–ó–Ω–∞–π–¥–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤: {len(context_documents)}

üîß AI —Ç–∏–º—á–∞—Å–æ–≤–æ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∏–π, –∞–ª–µ –ø–æ—à—É–∫ –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤ –ø—Ä–∞—Ü—é—î."""
            else:
                content = f"""üìö **FLAN-T5 Service Unavailable**

**Question:** {question}

Documents found: {len(context_documents)}

üîß AI temporarily unavailable, but document search is working."""
            
            return T5Response(
                content=content,
                model="fallback",
                tokens_used=len(content.split()),
                response_time=0.1,
                success=True
            )
        
        async def get_service_status(self):
            return {
                "service_type": "fallback",
                "ready": True,
                "error": "FLAN-T5 service not available"
            }
    
    return FallbackService()