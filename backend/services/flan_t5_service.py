# backend/services/flan_t5_service.py - –ü–û–õ–ù–û–°–¢–¨–Æ –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø
"""
–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π —Å–µ—Ä–≤–∏—Å –¥–ª—è FLAN-T5 Small –º–æ–¥–µ–ª–∏
–ö–†–ò–¢–ò–ß–ï–°–ö–ò–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–Ø: –ü—Ä–∞–≤–∏–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª—è FLAN-T5
"""

import logging
import time
import os
import asyncio
import random
from typing import List, Dict, Optional
from dataclasses import dataclass

logger = logging.getLogger(__name__)

@dataclass
class T5Response:
    content: str
    model: str
    tokens_used: int
    response_time: float
    success: bool
    error: Optional[str] = None

class FlanT5Service:
    """–ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ô —Å–µ—Ä–≤–∏—Å –¥–ª—è FLAN-T5 Small —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏"""
    
    def __init__(self):
        self.service_type = "flan_t5"
        self.model_name = os.getenv("LLM_MODEL", "google/flan-t5-small")
        self.hf_token = os.getenv("HF_TOKEN")
        self.ready = False
        self.model = None
        self.tokenizer = None
        
        logger.info(f"ü§ñ Initializing FLAN-T5 service: {self.model_name}")
        self._load_model()
    
    def _load_model(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å FLAN-T5"""
        try:
            from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
            
            logger.info("üì• Loading FLAN-T5 model and tokenizer...")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_name,
                token=self.hf_token if self.hf_token else None
            )
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
            self.model = AutoModelForSeq2SeqLM.from_pretrained(
                self.model_name,
                token=self.hf_token if self.hf_token else None,
                torch_dtype="auto",
                low_cpu_mem_usage=True
            )
            
            # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –º–æ–¥–µ–ª—å –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
            device = self._get_device()
            self.model = self.model.to(device)
            self.model.eval()
            
            self.ready = True
            logger.info(f"‚úÖ FLAN-T5 model loaded successfully on {device}")
            
        except Exception as e:
            logger.error(f"‚ùå Failed to load FLAN-T5 model: {e}")
            self.ready = False
    
    def _get_device(self):
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ"""
        try:
            import torch
            return "cuda" if torch.cuda.is_available() else "cpu"
        except:
            return "cpu"
    
    def _has_cuda(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å CUDA"""
        try:
            import torch
            return torch.cuda.is_available()
        except:
            return False
    
    async def answer_legal_question(self, question: str, context_documents: List[Dict], language: str = "en"):
        """–ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ô –º–µ—Ç–æ–¥ –æ—Ç–≤–µ—Ç–∞ –Ω–∞ —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–π –≤–æ–ø—Ä–æ—Å"""
        start_time = time.time()
        
        try:
            if not self.ready:
                return self._generate_error_response(
                    question, language, "Model not loaded", start_time
                )
            
            # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤ –ø–æ—Ä—è–¥–∫–µ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
            strategies = [
                ("simple", self._generate_simple_response),
                ("detailed", self._generate_detailed_response),
                ("contextual", self._generate_contextual_response)
            ]
            
            # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –í—ã–±–∏—Ä–∞–µ–º —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–ª–∏–Ω—ã –≤–æ–ø—Ä–æ—Å–∞
            if len(question) < 20:
                strategy_name, strategy_func = strategies[0]  # –ü—Ä–æ—Å—Ç—ã–µ –æ—Ç–≤–µ—Ç—ã
            elif context_documents and len(context_documents) > 0:
                strategy_name, strategy_func = strategies[2]  # –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã
            else:
                strategy_name, strategy_func = strategies[1]  # –î–µ—Ç–∞–ª—å–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã
            
            logger.info(f"üéØ Using strategy: {strategy_name} for question length: {len(question)}")
            
            response = await strategy_func(question, context_documents, language)
            
            if response.success and len(response.content.strip()) > 3:  # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –°–Ω–∏–∂–µ–Ω –ø–æ—Ä–æ–≥
                logger.info(f"‚úÖ Generated response ({strategy_name}): {len(response.content)} chars")
                return response
            else:
                # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ï—Å–ª–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–∞, –ø—Ä–æ–±—É–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—É—é
                logger.warning(f"‚ùå Strategy {strategy_name} failed, trying alternative")
                return await self._generate_fallback_response(question, context_documents, language, start_time)
                
        except Exception as e:
            logger.error(f"‚ùå Error in answer_legal_question: {e}")
            return self._generate_error_response(question, language, str(e), start_time)
    
    async def _generate_simple_response(self, question: str, context_documents: List[Dict], language: str):
        """–ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–æ—Å—Ç–æ–≥–æ –æ—Ç–≤–µ—Ç–∞"""
        start_time = time.time()
        
        # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –£–ø—Ä–æ—â–µ–Ω–Ω—ã–µ –ø—Ä–æ–º–ø—Ç—ã –¥–ª—è FLAN-T5
        if language == "uk":
            prompt = f"–ü–∏—Ç–∞–Ω–Ω—è: {question}\n–í—ñ–¥–ø–æ–≤—ñ–¥—å:"
        else:
            prompt = f"Question: {question}\nAnswer:"
        
        # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è FLAN-T5 Small
        return await self._generate_with_params(
            prompt, 
            start_time, 
            temperature=0.3,  # –°–Ω–∏–∂–µ–Ω–∞ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
            max_new_tokens=120  # –£–≤–µ–ª–∏—á–µ–Ω–æ
        )
    
    async def _generate_detailed_response(self, question: str, context_documents: List[Dict], language: str):
        """–ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞"""
        start_time = time.time()
        
        # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ü—Ä–æ—Å—Ç—ã–µ –ø—Ä–æ–º–ø—Ç—ã –±–µ–∑ —Å–ª–æ–∂–Ω—ã—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π
        if language == "uk":
            prompt = f"–Æ—Ä–∏–¥–∏—á–Ω–µ –ø–∏—Ç–∞–Ω–Ω—è: {question}\n–î–µ—Ç–∞–ª—å–Ω–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥—å:"
        else:
            prompt = f"Legal question: {question}\nDetailed answer:"
        
        return await self._generate_with_params(
            prompt, 
            start_time, 
            temperature=0.5,  # –£–º–µ—Ä–µ–Ω–Ω–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞
            max_new_tokens=200  # –ë–æ–ª—å—à–µ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ—Å—Ç–∏
        )
    
    async def _generate_contextual_response(self, question: str, context_documents: List[Dict], language: str):
        """–ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º"""
        start_time = time.time()
        
        # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –°–æ–∫—Ä–∞—â–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è FLAN-T5 Small
        context = ""
        if context_documents:
            doc = context_documents[0]
            context = doc.get('content', '')[:200]  # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–æ 200 —Å–∏–º–≤–æ–ª–∞–º–∏
        
        # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ü—Ä–æ—Å—Ç–æ–π —Ñ–æ—Ä–º–∞—Ç –ø—Ä–æ–º–ø—Ç–∞
        if language == "uk":
            prompt = f"–ö–æ–Ω—Ç–µ–∫—Å—Ç: {context}\n–ü–∏—Ç–∞–Ω–Ω—è: {question}\n–í—ñ–¥–ø–æ–≤—ñ–¥—å:"
        else:
            prompt = f"Context: {context}\nQuestion: {question}\nAnswer:"
        
        return await self._generate_with_params(
            prompt, 
            start_time, 
            temperature=0.4,  # –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞
            max_new_tokens=150  # –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞
        )
    
    async def _generate_fallback_response(self, question: str, context_documents: List[Dict], language: str, start_time: float):
        """–ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ô fallback —Å –ø—Ä–µ–¥—É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–º–∏ –æ—Ç–≤–µ—Ç–∞–º–∏"""
        
        # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ü—ã—Ç–∞–µ–º—Å—è –ø—Ä–æ—Å—Ç–µ–π—à–∏–π –ø—Ä–æ–º–ø—Ç
        try:
            simple_prompt = question if len(question) < 50 else question[:50]
            
            response = await self._generate_with_params(
                simple_prompt, 
                start_time, 
                temperature=0.1,  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞
                max_new_tokens=100
            )
            
            if response.success and len(response.content.strip()) > 3:
                return response
        except Exception as e:
            logger.error(f"Simple prompt also failed: {e}")
        
        # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –£–º–Ω—ã–µ –ø—Ä–µ–¥—É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã
        fallback_responses = self._get_smart_fallback_responses(question, language)
        selected_response = random.choice(fallback_responses)
        
        return T5Response(
            content=selected_response,
            model="flan_t5_fallback",
            tokens_used=len(selected_response.split()),
            response_time=time.time() - start_time,
            success=True
        )
    
    def _get_smart_fallback_responses(self, question: str, language: str) -> List[str]:
        """–ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ï —É–º–Ω—ã–µ fallback –æ—Ç–≤–µ—Ç—ã"""
        
        question_lower = question.lower()
        
        if language == "uk":
            # –£–∫—Ä–∞–∏–Ω—Å–∫–∏–µ –æ—Ç–≤–µ—Ç—ã
            if any(word in question_lower for word in ["—â–æ", "what", "–∑–∞–∫–æ–Ω", "law"]):
                return [
                    "–ó–∞–∫–æ–Ω - —Ü–µ —Å–∏—Å—Ç–µ–º–∞ –ø—Ä–∞–≤–∏–ª, —â–æ –≤—Å—Ç–∞–Ω–æ–≤–ª—é—î—Ç—å—Å—è –¥–µ—Ä–∂–∞–≤–æ—é –¥–ª—è —Ä–µ–≥—É–ª—é–≤–∞–Ω–Ω—è —Å—É—Å–ø—ñ–ª—å–Ω–∏—Ö –≤—ñ–¥–Ω–æ—Å–∏–Ω.",
                    "–ü—Ä–∞–≤–æ–≤–∞ —Å–∏—Å—Ç–µ–º–∞ –≤–∫–ª—é—á–∞—î –∫–æ–Ω—Å—Ç–∏—Ç—É—Ü—ñ—é, –∑–∞–∫–æ–Ω–∏ —Ç–∞ –ø—ñ–¥–∑–∞–∫–æ–Ω–Ω—ñ –∞–∫—Ç–∏.",
                    "–ó–∞–∫–æ–Ω —î –æ–±–æ–≤'—è–∑–∫–æ–≤–∏–º –¥–ª—è –≤–∏–∫–æ–Ω–∞–Ω–Ω—è –≤—Å—ñ–º–∞ –≥—Ä–æ–º–∞–¥—è–Ω–∞–º–∏ –Ω–∞ —Ç–µ—Ä–∏—Ç–æ—Ä—ñ—ó –¥–µ—Ä–∂–∞–≤–∏."
                ]
            elif any(word in question_lower for word in ["—ñ—Ä–ª–∞–Ω–¥—ñ—è", "ireland"]):
                return [
                    "–Ü—Ä–ª–∞–Ω–¥—Å—å–∫–µ –ø—Ä–∞–≤–æ –±–∞–∑—É—î—Ç—å—Å—è –Ω–∞ —Å–∏—Å—Ç–µ–º—ñ Common Law —Ç–∞ –ö–æ–Ω—Å—Ç–∏—Ç—É—Ü—ñ—ó –Ü—Ä–ª–∞–Ω–¥—ñ—ó 1937 —Ä–æ–∫—É.",
                    "–û—Å–Ω–æ–≤–Ω—ñ –¥–∂–µ—Ä–µ–ª–∞ —ñ—Ä–ª–∞–Ω–¥—Å—å–∫–æ–≥–æ –ø—Ä–∞–≤–∞: –ö–æ–Ω—Å—Ç–∏—Ç—É—Ü—ñ—è, –∑–∞–∫–æ–Ω–∏ –ø–∞—Ä–ª–∞–º–µ–Ω—Ç—É —Ç–∞ —Å—É–¥–æ–≤—ñ —Ä—ñ—à–µ–Ω–Ω—è.",
                    "–Ü—Ä–ª–∞–Ω–¥—Å—å–∫–∞ –ø—Ä–∞–≤–æ–≤–∞ —Å–∏—Å—Ç–µ–º–∞ –≤–∫–ª—é—á–∞—î —Ü–∏–≤—ñ–ª—å–Ω–µ, –∫—Ä–∏–º—ñ–Ω–∞–ª—å–Ω–µ —Ç–∞ –∞–¥–º—ñ–Ω—ñ—Å—Ç—Ä–∞—Ç–∏–≤–Ω–µ –ø—Ä–∞–≤–æ."
                ]
            elif any(word in question_lower for word in ["—è–∫", "how", "—á–æ–º—É", "why"]):
                return [
                    "–¶–µ –ø–∏—Ç–∞–Ω–Ω—è –ø–æ—Ç—Ä–µ–±—É—î –∞–Ω–∞–ª—ñ–∑—É –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∏—Ö –ø—Ä–∞–≤–æ–≤–∏—Ö –Ω–æ—Ä–º —Ç–∞ –æ–±—Å—Ç–∞–≤–∏–Ω —Å–ø—Ä–∞–≤–∏.",
                    "–î–ª—è —Ç–æ—á–Ω–æ—ó –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ —Ä–µ–∫–æ–º–µ–Ω–¥—É—é –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü—ñ—é –∑ –∫–≤–∞–ª—ñ—Ñ—ñ–∫–æ–≤–∞–Ω–∏–º —é—Ä–∏—Å—Ç–æ–º.",
                    "–ü—Ä–∞–≤–æ–≤–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥—å –∑–∞–ª–µ–∂–∏—Ç—å –≤—ñ–¥ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∏—Ö —Ñ–∞–∫—Ç—ñ–≤ —Ç–∞ –∑–∞—Å—Ç–æ—Å–æ–≤–Ω–æ–≥–æ –∑–∞–∫–æ–Ω–æ–¥–∞–≤—Å—Ç–≤–∞."
                ]
            else:
                return [
                    "–¶–µ —Ü—ñ–∫–∞–≤–µ —é—Ä–∏–¥–∏—á–Ω–µ –ø–∏—Ç–∞–Ω–Ω—è, —è–∫–µ –ø–æ—Ç—Ä–µ–±—É—î –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª—ñ–∑—É.",
                    "–î–ª—è –ø–æ–≤–Ω–æ—ó –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ –Ω–µ–æ–±—Ö—ñ–¥–Ω–æ –≤—Ä–∞—Ö—É–≤–∞—Ç–∏ –≤—Å—ñ –æ–±—Å—Ç–∞–≤–∏–Ω–∏ —Å–ø—Ä–∞–≤–∏.",
                    "–†–µ–∫–æ–º–µ–Ω–¥—É—é –∑–≤–µ—Ä–Ω—É—Ç–∏—Å—è –¥–æ —Å–ø–µ—Ü—ñ–∞–ª—ñ—Å—Ç–∞ –∑ –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω–æ—ó –≥–∞–ª—É–∑—ñ –ø—Ä–∞–≤–∞."
                ]
        else:
            # –ê–Ω–≥–ª–∏–π—Å–∫–∏–µ –æ—Ç–≤–µ—Ç—ã
            if any(word in question_lower for word in ["what", "law", "legal"]):
                return [
                    "Law is a system of rules created by governmental institutions to regulate behavior.",
                    "Legal systems include constitutional law, statutory law, and case law.",
                    "Laws serve to maintain order, protect rights, and resolve disputes."
                ]
            elif any(word in question_lower for word in ["ireland", "irish"]):
                return [
                    "Irish law is based on the Common Law system and the Constitution of Ireland from 1937.",
                    "Main sources of Irish law include the Constitution, Acts of Parliament, and judicial decisions.",
                    "Ireland has civil, criminal, and administrative law jurisdictions."
                ]
            elif any(word in question_lower for word in ["how", "why", "when", "where"]):
                return [
                    "This question requires analysis of specific legal provisions and circumstances.",
                    "I recommend consulting with a qualified lawyer for personalized advice.",
                    "The legal answer depends on the specific facts and applicable legislation."
                ]
            elif any(word in question_lower for word in ["hi", "hello", "hey"]):
                return [
                    "Hello! I'm here to help with legal questions about laws and regulations.",
                    "Hi! I can assist with general legal information. What would you like to know?",
                    "Greetings! I'm ready to help with questions about law and legal procedures."
                ]
            else:
                return [
                    "This is an interesting legal question that requires careful analysis.",
                    "To provide an accurate answer, I would need to consider specific circumstances.",
                    "For detailed legal advice, I recommend consulting with a qualified attorney."
                ]
    
    async def _generate_with_params(self, prompt: str, start_time: float, temperature: float = 0.3, max_new_tokens: int = 150) -> T5Response:
        """–ö–†–ò–¢–ò–ß–ï–°–ö–ò –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –¥–ª—è FLAN-T5"""
        try:
            # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è FLAN-T5 Small
            if len(prompt) > 300:  # –ë—ã–ª–æ 512
                prompt = prompt[:300]
                logger.info(f"Truncated prompt to 300 chars for FLAN-T5 Small")
            
            result = await asyncio.get_event_loop().run_in_executor(
                None, self._generate_sync, prompt, temperature, max_new_tokens
            )
            
            response_time = time.time() - start_time
            
            # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ë–æ–ª–µ–µ –º—è–≥–∫–∏–µ –∫—Ä–∏—Ç–µ—Ä–∏–∏ —É—Å–ø–µ—Ö–∞
            if result and len(result.strip()) > 3:  # –ë—ã–ª–æ > 5
                clean_result = result.strip()
                
                # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞—á–µ—Å—Ç–≤–æ –æ—Ç–≤–µ—Ç–∞
                if self._is_meaningful_response(clean_result):
                    return T5Response(
                        content=clean_result,
                        model=self.model_name,
                        tokens_used=len(clean_result.split()),
                        response_time=response_time,
                        success=True
                    )
                else:
                    logger.warning(f"Generated response not meaningful: '{clean_result[:50]}...'")
            
            return T5Response(
                content="",
                model=self.model_name,
                tokens_used=0,
                response_time=response_time,
                success=False,
                error="Generated response too short or not meaningful"
            )
                
        except Exception as e:
            return T5Response(
                content="",
                model=self.model_name,
                tokens_used=0,
                response_time=time.time() - start_time,
                success=False,
                error=str(e)
            )
    
    def _generate_sync(self, prompt: str, temperature: float = 0.3, max_new_tokens: int = 150) -> str:
        """–ö–†–ò–¢–ò–ß–ï–°–ö–ò –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–ª—è FLAN-T5"""
        try:
            import torch
            
            device = next(self.model.parameters()).device
            
            # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –¥–ª—è FLAN-T5
            inputs = self.tokenizer(
                prompt,
                return_tensors="pt",
                max_length=512,  # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –£–≤–µ–ª–∏—á–µ–Ω–æ —Å 300 –¥–æ 512
                truncation=True,
                padding=True
            )
            
            inputs = {k: v.to(device) for k, v in inputs.items()}
            
            # –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ü—Ä–∞–≤–∏–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è FLAN-T5
            generation_kwargs = {
                "max_new_tokens": max_new_tokens,  # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ò—Å–ø–æ–ª—å–∑—É–µ–º max_new_tokens –≤–º–µ—Å—Ç–æ max_length
                "temperature": temperature,
                "do_sample": True,  # –í–∫–ª—é—á–∞–µ–º sampling
                "top_p": 0.85,  # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –°–Ω–∏–∂–µ–Ω–æ —Å 0.9 –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
                "top_k": 50,    # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –£–≤–µ–ª–∏—á–µ–Ω–æ —Å 40
                "no_repeat_ngram_size": 3,  # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –£–≤–µ–ª–∏—á–µ–Ω–æ —Å 2
                "pad_token_id": self.tokenizer.eos_token_id,
                "eos_token_id": self.tokenizer.eos_token_id,
                "early_stopping": True,  # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –î–æ–±–∞–≤–ª–µ–Ω–æ –¥–ª—è FLAN-T5
                "length_penalty": 1.0,   # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –î–æ–±–∞–≤–ª–µ–Ω–æ –¥–ª—è –∫–æ–Ω—Ç—Ä–æ–ª—è –¥–ª–∏–Ω—ã
                "repetition_penalty": 1.1  # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –î–æ–±–∞–≤–ª–µ–Ω–æ –ø—Ä–æ—Ç–∏–≤ –ø–æ–≤—Ç–æ—Ä–æ–≤
            }
            
            # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –î–æ–±–∞–≤–ª—è–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É –∏—Å–∫–ª—é—á–µ–Ω–∏–π –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            with torch.no_grad():
                try:
                    outputs = self.model.generate(**inputs, **generation_kwargs)
                except Exception as gen_error:
                    logger.error(f"Generation error: {gen_error}")
                    # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: Fallback —Å –±–æ–ª–µ–µ –ø—Ä–æ—Å—Ç—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
                    simple_kwargs = {
                        "max_new_tokens": 80,
                        "temperature": 0.1,
                        "do_sample": False,  # Greedy decoding
                        "pad_token_id": self.tokenizer.eos_token_id,
                        "eos_token_id": self.tokenizer.eos_token_id
                    }
                    outputs = self.model.generate(**inputs, **simple_kwargs)
            
            # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ü—Ä–∞–≤–∏–ª—å–Ω–æ–µ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤
            input_length = inputs['input_ids'].shape[1]
            generated_tokens = outputs[0][input_length:]
            response = self.tokenizer.decode(generated_tokens, skip_special_tokens=True)
            
            # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –û—á–∏—Å—Ç–∫–∞ –∏ –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞
            response = response.strip()
            
            # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –£–¥–∞–ª—è–µ–º –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            response = self._clean_generated_response(response)
            
            logger.info(f"Generated {len(response)} chars with {max_new_tokens} max_new_tokens")
            return response
            
        except Exception as e:
            logger.error(f"Sync generation error: {e}")
            return ""
    
    def _clean_generated_response(self, response: str) -> str:
        """–ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –û—á–∏—Å—Ç–∫–∞ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞"""
        if not response:
            return ""
        
        # –£–¥–∞–ª—è–µ–º –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è —Ñ—Ä–∞–∑—ã
        lines = response.split('\n')
        unique_lines = []
        seen = set()
        
        for line in lines:
            line = line.strip()
            if line and line not in seen and len(line) > 5:
                unique_lines.append(line)
                seen.add(line)
        
        cleaned = ' '.join(unique_lines)
        
        # –£–¥–∞–ª—è–µ–º –Ω–µ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –æ–∫–æ–Ω—á–∞–Ω–∏—è
        if cleaned.endswith(('...', '..', '..')):
            cleaned = cleaned.rstrip('.') + '.'
        
        return cleaned
    
    def _is_meaningful_response(self, response: str) -> bool:
        """–ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ—Å–º—ã—Å–ª–µ–Ω–Ω–æ—Å—Ç–∏ –æ—Ç–≤–µ—Ç–∞"""
        if not response or len(response) < 3:
            return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è —Å–∏–º–≤–æ–ª—ã
        if len(set(response.replace(' ', ''))) < 5:
            return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –±–µ—Å—Å–º—ã—Å–ª–µ–Ω–Ω—ã–µ —Ñ—Ä–∞–∑—ã
        meaningless_patterns = [
            "aaaa", "bbbb", "cccc", "####", "....", "----",
            "unknown", "error", "failed", "none", "null"
        ]
        
        response_lower = response.lower()
        for pattern in meaningless_patterns:
            if pattern in response_lower:
                return False
        
        return True
    
    def _generate_error_response(self, question: str, language: str, error: str, start_time: float):
        """–ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ –ø—Ä–∏ –æ—à–∏–±–∫–µ"""
        
        if language == "uk":
            content = f"–í–∏–±–∞—á—Ç–µ, –≤–∏–Ω–∏–∫–ª–∞ —Ç–µ—Ö–Ω—ñ—á–Ω–∞ –ø—Ä–æ–±–ª–µ–º–∞ –ø—Ä–∏ –æ–±—Ä–æ–±—Ü—ñ –≤–∞—à–æ–≥–æ –ø–∏—Ç–∞–Ω–Ω—è. –°–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª—é–≤–∞—Ç–∏ –∑–∞–ø–∏—Ç –ø—Ä–æ—Å—Ç—ñ—à–µ."
        else:
            content = f"Sorry, there was a technical issue processing your question. Please try rephrasing it more simply."
        
        return T5Response(
            content=content,
            model="error",
            tokens_used=len(content.split()),
            response_time=time.time() - start_time,
            success=False,
            error=error
        )
    
    async def get_service_status(self):
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç—É—Å —Å–µ—Ä–≤–∏—Å–∞"""
        return {
            "service_type": self.service_type,
            "model": self.model_name,
            "ready": self.ready,
            "hf_token_configured": bool(self.hf_token),
            "cuda_available": self._has_cuda(),
            "device": self._get_device(),
            "memory_usage": "~300 MB",
            "generation_params": {
                "max_new_tokens": "120-200",
                "temperature": "0.1-0.5",
                "top_p": "0.85",
                "top_k": "50"
            },
            "fixes_applied": [
                "max_new_tokens instead of max_length",
                "simplified prompts for FLAN-T5",
                "proper input length handling",
                "meaningful response validation",
                "improved error handling"
            ]
        }

def create_flan_t5_service():
    """–°–æ–∑–¥–∞–µ—Ç —ç–∫–∑–µ–º–ø–ª—è—Ä FLAN-T5 —Å–µ—Ä–≤–∏—Å–∞"""
    try:
        return FlanT5Service()
    except Exception as e:
        logger.error(f"‚ùå Failed to create FLAN-T5 service: {e}")
        return create_fallback_service()

def create_fallback_service():
    """–°–æ–∑–¥–∞–µ—Ç fallback —Å–µ—Ä–≤–∏—Å"""
    
    class FallbackService:
        def __init__(self):
            self.service_type = "fallback"
            self.ready = True
        
        async def answer_legal_question(self, question: str, context_documents: list, language: str = "en"):
            if language == "uk":
                content = f"FLAN-T5 —Å–µ—Ä–≤—ñ—Å –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∏–π. –í–∞—à–µ –ø–∏—Ç–∞–Ω–Ω—è: {question[:100]}..."
            else:
                content = f"FLAN-T5 service unavailable. Your question: {question[:100]}..."
            
            return T5Response(
                content=content,
                model="fallback",
                tokens_used=len(content.split()),
                response_time=0.1,
                success=True
            )
        
        async def get_service_status(self):
            return {
                "service_type": "fallback",
                "ready": True,
                "error": "FLAN-T5 service not available"
            }
    
    return FallbackService()