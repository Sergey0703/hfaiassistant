# backend/services/huggingface_llm_service.py - –í–û–°–°–¢–ê–ù–û–í–õ–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø
"""
LLM Service using HuggingFace Transformers with GPTQ quantization
–í–∞—à–∏ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ GPTQ –º–æ–¥–µ–ª–∏
"""

import logging
import time
import os
from typing import List, Dict, Optional
from dataclasses import dataclass

logger = logging.getLogger(__name__)

@dataclass
class LLMResponse:
    content: str
    model: str
    tokens_used: int
    response_time: float
    success: bool
    error: Optional[str] = None

class HuggingFaceLLMService:
    """LLM Service using HuggingFace Transformers with GPTQ - –í–ê–®–ò –ú–û–î–ï–õ–ò"""
    
    def __init__(self, model_name: str = "TheBloke/Llama-2-7B-Chat-GPTQ"):
        self.model_name = model_name
        self.model = None
        self.tokenizer = None
        self.service_type = "huggingface_gptq"
        self.model_loaded = False
        self._initialize_model()
    
    def _initialize_model(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –í–ê–®–ò GPTQ –º–æ–¥–µ–ª–∏"""
        try:
            logger.info(f"ü§ñ Loading YOUR GPTQ model: {self.model_name}")
            
            # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –±–∏–±–ª–∏–æ—Ç–µ–∫–∏
            from transformers import AutoTokenizer, AutoModelForCausalLM
            import torch
            
            # –í–ê–®–ò –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ GPTQ –º–æ–¥–µ–ª–∏ (–≤ –ø–æ—Ä—è–¥–∫–µ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è)
            your_gptq_models = [
                "TheBloke/Llama-2-7B-Chat-GPTQ",  # –í–∞—à–∞ –æ—Å–Ω–æ–≤–Ω–∞—è –º–æ–¥–µ–ª—å
                "TheBloke/Mistral-7B-Instruct-v0.1-GPTQ",  # –í–∞—à–∞ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞
                "TheBloke/Llama-2-13B-Chat-GPTQ",  # –ï—Å–ª–∏ –Ω—É–∂–Ω–∞ –±–æ–ª–µ–µ –º–æ—â–Ω–∞—è
            ]
            
            for model_name in your_gptq_models:
                try:
                    logger.info(f"üîÑ Trying to load YOUR model: {model_name}")
                    
                    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä
                    self.tokenizer = AutoTokenizer.from_pretrained(
                        model_name,
                        trust_remote_code=True,
                        use_fast=True
                    )
                    
                    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è –í–ê–®–ò–• GPTQ –º–æ–¥–µ–ª–µ–π
                    model_kwargs = {
                        "torch_dtype": torch.float16,
                        "device_map": "auto",
                        "trust_remote_code": True,
                        "low_cpu_mem_usage": True,
                    }
                    
                    # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å GPTQ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–µ–π
                    try:
                        logger.info(f"üîÑ Loading {model_name} with GPTQ quantization...")
                        self.model = AutoModelForCausalLM.from_pretrained(
                            model_name,
                            **model_kwargs,
                            quantization_config={"load_in_4bit": True}
                        )
                        logger.info(f"‚úÖ Loaded with GPTQ quantization: {model_name}")
                    except Exception as gptq_error:
                        logger.warning(f"‚ö†Ô∏è GPTQ quantization failed for {model_name}: {gptq_error}")
                        logger.info(f"üîÑ Trying {model_name} without quantization...")
                        # Fallback –±–µ–∑ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏
                        self.model = AutoModelForCausalLM.from_pretrained(
                            model_name,
                            **model_kwargs
                        )
                        logger.info(f"‚úÖ Loaded without quantization: {model_name}")
                    
                    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–∞
                    if self.tokenizer.pad_token is None:
                        self.tokenizer.pad_token = self.tokenizer.eos_token
                    
                    self.model_name = model_name
                    self.model_loaded = True
                    logger.info(f"üéâ YOUR model ready: {model_name}")
                    return
                    
                except Exception as e:
                    logger.warning(f"‚ùå Failed to load YOUR model {model_name}: {e}")
                    continue
            
            # –ï—Å–ª–∏ –≤–∞—à–∏ –º–æ–¥–µ–ª–∏ –Ω–µ –∑–∞–≥—Ä—É–∑–∏–ª–∏—Å—å
            raise Exception("Failed to load any of YOUR GPTQ models")
            
        except ImportError as e:
            logger.error(f"‚ùå Missing dependencies for YOUR models: {e}")
            logger.error("Install: pip install transformers torch auto-gptq accelerate")
            raise
        except Exception as e:
            logger.error(f"‚ùå YOUR model initialization failed: {e}")
            raise
    
    async def answer_legal_question(self, question: str, context_documents: List[Dict], language: str = "en"):
        """–û—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–π –≤–æ–ø—Ä–æ—Å –∏—Å–ø–æ–ª—å–∑—É—è –í–ê–®–ò –º–æ–¥–µ–ª–∏"""
        start_time = time.time()
        
        try:
            if not self.model_loaded:
                return LLMResponse(
                    content="Your GPTQ model not loaded. Please check logs for errors.",
                    model=self.model_name,
                    tokens_used=0,
                    response_time=time.time() - start_time,
                    success=False,
                    error="Your model not initialized"
                )
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç –¥–ª—è –í–ê–®–ò–• –º–æ–¥–µ–ª–µ–π
            prompt = self._build_legal_prompt(question, context_documents, language)
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç —Å –í–ê–®–ò–ú–ò –º–æ–¥–µ–ª—è–º–∏
            response_text = await self._generate_response(prompt)
            
            response_time = time.time() - start_time
            
            return LLMResponse(
                content=response_text,
                model=self.model_name,
                tokens_used=len(response_text.split()),
                response_time=response_time,
                success=True
            )
            
        except Exception as e:
            logger.error(f"‚ùå Error generating response with YOUR model: {e}")
            return LLMResponse(
                content="I apologize, but I'm experiencing technical difficulties with your GPTQ model. Please try again later.",
                model=self.model_name,
                tokens_used=0,
                response_time=time.time() - start_time,
                success=False,
                error=str(e)
            )
    
    def _build_legal_prompt(self, question: str, context_documents: List[Dict], language: str) -> str:
        """–°—Ç—Ä–æ–∏—Ç –ø—Ä–æ–º–ø—Ç –¥–ª—è —é—Ä–∏–¥–∏—á–µ—Å–∫–æ–≥–æ –≤–æ–ø—Ä–æ—Å–∞ - –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –¥–ª—è –í–ê–®–ò–• –º–æ–¥–µ–ª–µ–π"""
        
        if language == "uk":
            system_prompt = """–í–∏ - –¥–æ—Å–≤—ñ–¥—á–µ–Ω–∏–π —é—Ä–∏–¥–∏—á–Ω–∏–π –∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç –∑ expertise –≤ —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ–º—É —Ç–∞ —ñ—Ä–ª–∞–Ω–¥—Å—å–∫–æ–º—É –ø—Ä–∞–≤—ñ. 
–í—ñ–¥–ø–æ–≤—ñ–¥–∞–π—Ç–µ –Ω–∞ –ø–∏—Ç–∞–Ω–Ω—è —á—ñ—Ç–∫–æ —Ç–∞ –ø—Ä–æ—Ñ–µ—Å—ñ–π–Ω–æ, –±–∞–∑—É—é—á–∏—Å—å –Ω–∞ –Ω–∞–¥–∞–Ω–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö.

–ü—Ä–∞–≤–∏–ª–∞:
- –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π—Ç–µ —Ç—ñ–ª—å–∫–∏ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –∑ –Ω–∞–¥–∞–Ω–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤
- –Ø–∫—â–æ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó –Ω–µ–¥–æ—Å—Ç–∞—Ç–Ω—å–æ, —Å–∫–∞–∂—ñ—Ç—å –ø—Ä–æ —Ü–µ —á–µ—Å–Ω–æ
- –î–∞–≤–∞–π—Ç–µ –ø—Ä–∞–∫—Ç–∏—á–Ω—ñ –ø–æ—Ä–∞–¥–∏ –∑ –ø–æ—Å–∏–ª–∞–Ω–Ω—è–º–∏ –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ñ –∑–∞–∫–æ–Ω–∏
- –í—ñ–¥–ø–æ–≤—ñ–¥–∞–π—Ç–µ —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—é –º–æ–≤–æ—é"""
            
            context_intro = "üìö –Æ—Ä–∏–¥–∏—á–Ω—ñ –¥–æ–∫—É–º–µ–Ω—Ç–∏ –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É:"
            answer_intro = f"‚ùì –ü–∏—Ç–∞–Ω–Ω—è: {question}\n\nüìã –ü—Ä–æ—Ñ–µ—Å—ñ–π–Ω–∞ —é—Ä–∏–¥–∏—á–Ω–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥—å:"
        else:
            system_prompt = """You are an experienced legal consultant with expertise in Irish and Ukrainian law. 
Answer questions clearly and professionally based on the provided legal documents.

Rules:
- Use only information from the provided documents
- If information is insufficient, say so honestly
- Provide practical advice with specific legal references
- Be concise but thorough"""
            
            context_intro = "üìö Legal documents for analysis:"
            answer_intro = f"‚ùì Question: {question}\n\nüìã Professional legal response:"
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        context_parts = []
        for i, doc in enumerate(context_documents[:3]):  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ 3 –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            filename = doc.get('filename', f'Document {i+1}')
            content = doc.get('content', '')[:800]  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –ª–∏–º–∏—Ç –¥–ª—è –≤–∞—à–∏—Ö –º–æ–¥–µ–ª–µ–π
            context_parts.append(f"üìÑ {filename}:\n{content}")
        
        context = "\n\n".join(context_parts) if context_parts else "No relevant documents found."
        
        # –°–æ–±–∏—Ä–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –ø—Ä–æ–º–ø—Ç
        prompt = f"{system_prompt}\n\n{context_intro}\n{context}\n\n{answer_intro}"
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –æ–±—â—É—é –¥–ª–∏–Ω—É –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è –≤–∞—à–∏—Ö –º–æ–¥–µ–ª–µ–π
        max_length = 3000  # –£–≤–µ–ª–∏—á–µ–Ω–æ –¥–ª—è GPTQ –º–æ–¥–µ–ª–µ–π
        if len(prompt) > max_length:
            prompt = prompt[:max_length] + "..."
        
        return prompt
    
    async def _generate_response(self, prompt: str, max_new_tokens: int = 800) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –∏—Å–ø–æ–ª—å–∑—É—è –í–ê–®–ò GPTQ –º–æ–¥–µ–ª–∏"""
        try:
            import torch
            
            # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç
            inputs = self.tokenizer.encode(
                prompt, 
                return_tensors="pt", 
                truncation=True, 
                max_length=2000  # –£–≤–µ–ª–∏—á–µ–Ω–æ –¥–ª—è –≤–∞—à–∏—Ö –º–æ–¥–µ–ª–µ–π
            )
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç —Å –í–ê–®–ò–ú–ò –º–æ–¥–µ–ª—è–º–∏
            with torch.no_grad():
                outputs = self.model.generate(
                    inputs,
                    max_new_tokens=max_new_tokens,
                    num_return_sequences=1,
                    temperature=0.3,  # –ö–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–ª—è —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö –≤–æ–ø—Ä–æ—Å–æ–≤
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                    attention_mask=torch.ones_like(inputs)
                )
            
            # –î–µ–∫–æ–¥–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
            full_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ –Ω–æ–≤—É—é —á–∞—Å—Ç—å (–æ—Ç–≤–µ—Ç)
            response = full_response[len(prompt):].strip()
            
            # –û—á–∏—â–∞–µ–º –æ—Ç–≤–µ—Ç
            response = self._clean_response(response)
            
            return response if response else "I need more specific information to provide a proper legal analysis."
            
        except Exception as e:
            logger.error(f"‚ùå Error in response generation with YOUR model: {e}")
            return f"Technical error occurred while generating response with your GPTQ model. Please try with a simpler question."
    
    def _clean_response(self, response: str) -> str:
        """–û—á–∏—â–∞–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç –æ—Ç –í–ê–®–ò–• –º–æ–¥–µ–ª–µ–π"""
        # –£–±–∏—Ä–∞–µ–º –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è –∏ –ª–∏—à–Ω–∏–µ —Å–∏–º–≤–æ–ª—ã
        lines = response.split('\n')
        cleaned_lines = []
        seen_lines = set()
        
        for line in lines:
            line = line.strip()
            if line and line not in seen_lines and len(line) > 10:
                cleaned_lines.append(line)
                seen_lines.add(line)
        
        cleaned = '\n'.join(cleaned_lines)
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É –¥–ª—è –≤–∞—à–∏—Ö –º–æ–¥–µ–ª–µ–π
        if len(cleaned) > 2000:  # –£–≤–µ–ª–∏—á–µ–Ω–æ –¥–ª—è GPTQ
            cleaned = cleaned[:2000] + "..."
        
        return cleaned
    
    async def get_service_status(self):
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç—É—Å –í–ê–®–ò–• GPTQ –º–æ–¥–µ–ª–µ–π"""
        return {
            "service_type": "your_gptq_models",
            "model_loaded": self.model_loaded,
            "model_name": self.model_name,
            "gptq_quantization": True,
            "your_models": [
                "TheBloke/Llama-2-7B-Chat-GPTQ",
                "TheBloke/Mistral-7B-Instruct-v0.1-GPTQ"
            ],
            "supported_languages": ["en", "uk"],
            "memory_usage": "~4-6GB with GPTQ quantization",
            "capabilities": [
                "High-quality legal analysis",
                "Multi-language support",
                "GPTQ quantization efficiency",
                "Professional legal responses"
            ],
            "recommendations": [
                "Using YOUR chosen GPTQ models",
                "Optimized for legal consultations",
                "Memory-efficient quantization",
                "Professional response quality"
            ]
        }

def create_llm_service(model_name: str = "TheBloke/Llama-2-7B-Chat-GPTQ"):
    """–°–æ–∑–¥–∞–µ—Ç LLM —Å–µ—Ä–≤–∏—Å —Å –í–ê–®–ò–ú–ò GPTQ –º–æ–¥–µ–ª—è–º–∏"""
    try:
        return HuggingFaceLLMService(model_name)
    except Exception as e:
        logger.error(f"Failed to create YOUR GPTQ LLM service: {e}")
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º fallback —Ç–æ–ª—å–∫–æ –≤ –∫—Ä–∞–π–Ω–µ–º —Å–ª—É—á–∞–µ
        from app.dependencies import FallbackLLMService
        return FallbackLLMService()