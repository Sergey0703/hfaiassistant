# backend/services/huggingface_llm_service.py - –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø –î–õ–Ø HF SPACES
"""
LLM Service using HuggingFace Transformers with GPTQ quantization
–ò–°–ü–†–ê–í–õ–ï–ù–ò–Ø: –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è HF Spaces, —É–ª—É—á—à–µ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫, memory management
"""

import logging
import time
import os
import gc
from typing import List, Dict, Optional
from dataclasses import dataclass

logger = logging.getLogger(__name__)

@dataclass
class LLMResponse:
    content: str
    model: str
    tokens_used: int
    response_time: float
    success: bool
    error: Optional[str] = None

class HuggingFaceLLMService:
    """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π LLM Service –¥–ª—è HF Spaces —Å GPTQ –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π"""
    
    def __init__(self, model_name: str = "TheBloke/Llama-2-7B-Chat-GPTQ"):
        self.model_name = model_name
        self.model = None
        self.tokenizer = None
        self.service_type = "huggingface_gptq_optimized"
        self.model_loaded = False
        self.loading_started = False
        self.loading_error = None
        self.hf_spaces = os.getenv("SPACE_ID") is not None
        
        # HF Spaces –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        self.max_memory_gb = 14 if self.hf_spaces else 32  # HF Spaces –ª–∏–º–∏—Ç ~16GB
        self.loading_timeout = 300 if self.hf_spaces else 180  # 5 –º–∏–Ω—É—Ç –¥–ª—è HF Spaces
        
        logger.info(f"ü§ñ Initializing GPTQ LLM service for: {model_name}")
        logger.info(f"üåç Platform: {'HuggingFace Spaces' if self.hf_spaces else 'Local'}")
        
        # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –Ω–∞—á–∏–Ω–∞–µ–º –∑–∞–≥—Ä—É–∑–∫—É
        self._start_model_loading()
    
    def _start_model_loading(self):
        """–ë–µ–∑–æ–ø–∞—Å–Ω–æ –Ω–∞—á–∏–Ω–∞–µ—Ç –∑–∞–≥—Ä—É–∑–∫—É –º–æ–¥–µ–ª–∏"""
        if self.loading_started:
            return
        
        self.loading_started = True
        
        try:
            logger.info(f"üîÑ Starting GPTQ model loading: {self.model_name}")
            self._load_model_with_optimizations()
        except Exception as e:
            self.loading_error = str(e)
            logger.error(f"‚ùå Model loading initiation failed: {e}")
    
    def _load_model_with_optimizations(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏ –¥–ª—è HF Spaces"""
        start_time = time.time()
        
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –±–∏–±–ª–∏–æ—Ç–µ–∫
            import torch
            from transformers import AutoTokenizer, AutoModelForCausalLM
            
            logger.info("üìö Dependencies available, starting model load...")
            
            # HF Spaces –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
            if self.hf_spaces:
                # –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º –ø–∞–º—è—Ç—å –ø–µ—Ä–µ–¥ –∑–∞–≥—Ä—É–∑–∫–æ–π
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                gc.collect()
                logger.info("üßπ Memory cleaned for HF Spaces")
            
            # –°–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π –¥–ª—è –ø–æ–ø—ã—Ç–∫–∏ (–≤–∞—à–∞ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω–∞—è –ø–µ—Ä–≤–∞—è)
            model_candidates = [
                self.model_name,  # –í–∞—à–∞ –æ—Å–Ω–æ–≤–Ω–∞—è –º–æ–¥–µ–ª—å
                "TheBloke/Mistral-7B-Instruct-v0.1-GPTQ",  # –ë–æ–ª–µ–µ –ª–µ–≥–∫–∞—è –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞
                "TheBloke/Llama-2-7B-Chat-GPTQ"  # Fallback
            ]
            
            for attempt, candidate_model in enumerate(model_candidates):
                try:
                    logger.info(f"üîÑ Attempt {attempt + 1}: Loading {candidate_model}")
                    
                    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä
                    self.tokenizer = AutoTokenizer.from_pretrained(
                        candidate_model,
                        trust_remote_code=True,
                        use_fast=True,
                        cache_dir="./.cache" if self.hf_spaces else None
                    )
                    
                    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è GPTQ —Å HF Spaces –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏
                    # –ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ï –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è GPTQ —Å –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –ø–∞–º—è—Ç–∏
                    model_kwargs = {
                    "torch_dtype": torch.float16,  # –ò–∑–º–µ–Ω–µ–Ω–æ —Å float32
                    "trust_remote_code": True,
                    "low_cpu_mem_usage": True,
                    "cache_dir": "./.cache" if self.hf_spaces else None,
                    "device_map": "auto",
                    "max_memory": {"cpu": "12GB", 0: "3GB"},  # –ñ–µ—Å—Ç–∫–∏–µ –ª–∏–º–∏—Ç—ã
                    "offload_folder": "./offload",  # CPU offloading
                    "use_cache": False  # –û—Ç–∫–ª—é—á–∞–µ–º –∫—ç—à
                    }
                    else:
                        model_kwargs["device_map"] = "auto"
                    
                    # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å GPTQ
                    try:
                        logger.info(f"üîÑ Loading {candidate_model} with GPTQ quantization...")
                        
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ auto-gptq
                        try:
                            import auto_gptq
                            logger.info("‚úÖ auto-gptq available")
                        except ImportError:
                            logger.warning("‚ö†Ô∏è auto-gptq not available, trying without")
                        
                        self.model = AutoModelForCausalLM.from_pretrained(
                            candidate_model,
                            **model_kwargs
                        )
                        
                        logger.info(f"‚úÖ Successfully loaded with GPTQ: {candidate_model}")
                        
                    except Exception as gptq_error:
                        logger.warning(f"‚ö†Ô∏è GPTQ loading failed: {gptq_error}")
                        logger.info(f"üîÑ Trying {candidate_model} without GPTQ optimizations...")
                        
                        # Fallback –±–µ–∑ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö GPTQ –Ω–∞—Å—Ç—Ä–æ–µ–∫
                        simplified_kwargs = {
                            "torch_dtype": torch.float16,
                            "device_map": "cpu" if self.hf_spaces and not torch.cuda.is_available() else "auto",
                            "low_cpu_mem_usage": True
                        }
                        
                        self.model = AutoModelForCausalLM.from_pretrained(
                            candidate_model,
                            **simplified_kwargs
                        )
                        
                        logger.info(f"‚úÖ Loaded without GPTQ optimizations: {candidate_model}")
                    
                    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–∞
                    if self.tokenizer.pad_token is None:
                        self.tokenizer.pad_token = self.tokenizer.eos_token
                    
                    # –û–±–Ω–æ–≤–ª—è–µ–º model_name –Ω–∞ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å
                    self.model_name = candidate_model
                    self.model_loaded = True
                    
                    loading_time = time.time() - start_time
                    logger.info(f"üéâ GPTQ model ready: {candidate_model} (loaded in {loading_time:.1f}s)")
                    
                    # –û—á–∏—â–∞–µ–º –ø–∞–º—è—Ç—å –ø–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏
                    if self.hf_spaces:
                        gc.collect()
                        if torch.cuda.is_available():
                            torch.cuda.empty_cache()
                    
                    return  # –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∑–∏–ª–∏, –≤—ã—Ö–æ–¥–∏–º
                    
                except Exception as model_error:
                    logger.warning(f"‚ùå Failed to load {candidate_model}: {model_error}")
                    
                    # –û—á–∏—â–∞–µ–º –ø–∞–º—è—Ç—å –ø–µ—Ä–µ–¥ —Å–ª–µ–¥—É—é—â–µ–π –ø–æ–ø—ã—Ç–∫–æ–π
                    if hasattr(self, 'model') and self.model is not None:
                        del self.model
                        self.model = None
                    if hasattr(self, 'tokenizer') and self.tokenizer is not None:
                        del self.tokenizer  
                        self.tokenizer = None
                    
                    gc.collect()
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()
                    
                    if attempt == len(model_candidates) - 1:
                        # –ü–æ—Å–ª–µ–¥–Ω—è—è –ø–æ–ø—ã—Ç–∫–∞
                        raise model_error
                    else:
                        logger.info(f"üîÑ Trying next model candidate...")
                        time.sleep(2)  # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ –º–µ–∂–¥—É –ø–æ–ø—ã—Ç–∫–∞–º–∏
            
        except ImportError as e:
            error_msg = f"Missing dependencies: {e}"
            self.loading_error = error_msg
            logger.error(f"‚ùå {error_msg}")
            logger.error("Install: pip install transformers torch auto-gptq accelerate")
            raise
            
        except Exception as e:
            self.loading_error = str(e)
            logger.error(f"‚ùå Model loading failed completely: {e}")
            raise
    
    async def answer_legal_question(self, question: str, context_documents: List[Dict], language: str = "en"):
        """–û—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–π –≤–æ–ø—Ä–æ—Å —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –¥–ª—è HF Spaces"""
        start_time = time.time()
        
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∞—Ç—É—Å –º–æ–¥–µ–ª–∏
            if not self.model_loaded:
                if self.loading_error:
                    return LLMResponse(
                        content=self._generate_loading_error_response(question, language),
                        model=self.model_name,
                        tokens_used=0,
                        response_time=time.time() - start_time,
                        success=False,
                        error=f"Model loading failed: {self.loading_error}"
                    )
                else:
                    return LLMResponse(
                        content=self._generate_loading_response(question, language),
                        model=self.model_name,
                        tokens_used=0,
                        response_time=time.time() - start_time,
                        success=True,
                        error="Model still loading"
                    )
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –¥–ª—è HF Spaces
            prompt = self._build_optimized_prompt(question, context_documents, language)
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
            response_text = await self._generate_response_optimized(prompt)
            
            response_time = time.time() - start_time
            
            return LLMResponse(
                content=response_text,
                model=self.model_name,
                tokens_used=len(response_text.split()),
                response_time=response_time,
                success=True
            )
            
        except Exception as e:
            logger.error(f"‚ùå Error generating response: {e}")
            return LLMResponse(
                content=self._generate_error_response(question, language, str(e)),
                model=self.model_name,
                tokens_used=0,
                response_time=time.time() - start_time,
                success=False,
                error=str(e)
            )
    
    def _build_optimized_prompt(self, question: str, context_documents: List[Dict], language: str) -> str:
        """–°—Ç—Ä–æ–∏—Ç –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è HF Spaces (–∫–æ—Ä–æ—á–µ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ)"""
        
        # –°–æ–∫—Ä–∞—â–µ–Ω–Ω—ã–µ —Å–∏—Å—Ç–µ–º–Ω—ã–µ –ø—Ä–æ–º–ø—Ç—ã –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ —Ç–æ–∫–µ–Ω–æ–≤
        if language == "uk":
            system_prompt = "–í–∏ - —é—Ä–∏–¥–∏—á–Ω–∏–π –∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç. –í—ñ–¥–ø–æ–≤—ñ–¥–∞–π—Ç–µ –∫–æ—Ä–æ—Ç–∫–æ —Ç–∞ –ø–æ —Å—É—Ç—ñ –Ω–∞ –æ—Å–Ω–æ–≤—ñ –Ω–∞–¥–∞–Ω–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤."
            context_intro = "–î–æ–∫—É–º–µ–Ω—Ç–∏:"
            answer_intro = f"–ü–∏—Ç–∞–Ω–Ω—è: {question}\n–í—ñ–¥–ø–æ–≤—ñ–¥—å:"
        else:
            system_prompt = "You are a legal consultant. Answer concisely based on provided documents."
            context_intro = "Documents:"
            answer_intro = f"Question: {question}\nAnswer:"
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è HF Spaces
        max_docs = 2 if self.hf_spaces else 3
        max_doc_length = 800 if self.hf_spaces else 1200
        
        context_parts = []
        for i, doc in enumerate(context_documents[:max_docs]):
            filename = doc.get('filename', f'Doc {i+1}')
            content = doc.get('content', '')[:max_doc_length]
            context_parts.append(f"{filename}: {content}")
        
        context = f"\n{context_intro}\n" + "\n".join(context_parts) if context_parts else ""
        
        # –°–æ–±–∏—Ä–∞–µ–º –∫–æ–º–ø–∞–∫—Ç–Ω—ã–π –ø—Ä–æ–º–ø—Ç
        prompt = f"{system_prompt}{context}\n\n{answer_intro}"
        
        # –ñ–µ—Å—Ç–∫–æ–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–ª–∏–Ω—ã –¥–ª—è HF Spaces
        max_prompt_length = 2000 if self.hf_spaces else 3000
        if len(prompt) > max_prompt_length:
            prompt = prompt[:max_prompt_length] + "..."
        
        return prompt
    
    async def _generate_response_optimized(self, prompt: str) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –¥–ª—è HF Spaces"""
        try:
            import torch
            
            # –°–æ–∫—Ä–∞—â–µ–Ω–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è HF Spaces
            max_new_tokens = 20 if self.hf_spaces else 800
            max_input_length = 1500 if self.hf_spaces else 2000
            
            # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è–º–∏
            inputs = self.tokenizer.encode(
                prompt, 
                return_tensors="pt", 
                truncation=True, 
                max_length=max_input_length
            )
            
            # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–ª—è HF Spaces
            generation_kwargs = {
                "max_new_tokens": max_new_tokens,
                "num_return_sequences": 1,
                "temperature": 0.1,  # –ë–æ–ª–µ–µ –∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω–∞—è –¥–ª—è —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö –≤–æ–ø—Ä–æ—Å–æ–≤
                "do_sample": True,
                "pad_token_id": self.tokenizer.eos_token_id,
                "eos_token_id": self.tokenizer.eos_token_id,
                "no_repeat_ngram_size": 2,  # –ò–∑–±–µ–≥–∞–µ–º –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–π
                "use_cache": False
            }
            
            # HF Spaces specific optimizations
            if self.hf_spaces:
                generation_kwargs.update({
                    "use_cache": True,
                    "attention_mask": torch.ones_like(inputs)
                })
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å –∫–æ–Ω—Ç—Ä–æ–ª–µ–º –ø–∞–º—è—Ç–∏
            with torch.no_grad():
                if self.hf_spaces:
                    # –û—á–∏—â–∞–µ–º –∫—ç—à –ø–µ—Ä–µ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()
                
                outputs = self.model.generate(inputs, **generation_kwargs)
            
            # –î–µ–∫–æ–¥–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
            full_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ –Ω–æ–≤—É—é —á–∞—Å—Ç—å
            response = full_response[len(prompt):].strip()
            
            # –û—á–∏—â–∞–µ–º –æ—Ç–≤–µ—Ç
            response = self._clean_response_optimized(response)
            
            # –û—á–∏—â–∞–µ–º –ø–∞–º—è—Ç—å –ø–æ—Å–ª–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            if self.hf_spaces:
                del outputs, inputs
                gc.collect()
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
            
            return response if response else "I need more specific information to provide a proper legal analysis."
            
        except torch.cuda.OutOfMemoryError:
            logger.error("‚ùå CUDA out of memory during generation")
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            return "Memory limit reached. Please try with a shorter question."
            
        except Exception as e:
            logger.error(f"‚ùå Generation error: {e}")
            return f"Technical error during response generation. Please try again."
    
    def _clean_response_optimized(self, response: str) -> str:
        """–ë—ã—Å—Ç—Ä–∞—è –æ—á–∏—Å—Ç–∫–∞ –æ—Ç–≤–µ—Ç–∞ –¥–ª—è HF Spaces"""
        if not response:
            return response
        
        # –£–¥–∞–ª—è–µ–º –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è –∏ –æ–±—Ä–µ–∑–∞–µ–º
        lines = response.split('\n')
        cleaned_lines = []
        seen = set()
        
        for line in lines[:10]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫
            line = line.strip()
            if line and len(line) > 10 and line not in seen:
                cleaned_lines.append(line)
                seen.add(line)
        
        cleaned = '\n'.join(cleaned_lines)
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É –¥–ª—è HF Spaces
        max_length = 1500 if self.hf_spaces else 2000
        if len(cleaned) > max_length:
            cleaned = cleaned[:max_length] + "..."
        
        return cleaned
    
    def _generate_loading_response(self, question: str, language: str) -> str:
        """–û—Ç–≤–µ—Ç –ø–æ–∫–∞ –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è"""
        if language == "uk":
            return f"""ü§ñ **GPTQ –º–æ–¥–µ–ª—å –∑–∞–≤–∞–Ω—Ç–∞–∂—É—î—Ç—å—Å—è...**

**–í–∞—à–µ –ø–∏—Ç–∞–Ω–Ω—è:** {question}

‚è≥ –ú–æ–¥–µ–ª—å `{self.model_name}` —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑—É—î—Ç—å—Å—è. –¶–µ –∑–∞–∑–≤–∏—á–∞–π –∑–∞–π–º–∞—î 1-2 —Ö–≤–∏–ª–∏–Ω–∏ –¥–ª—è –ø–µ—Ä—à–æ–≥–æ –∑–∞–ø—É—Å–∫—É.

üîÑ **–°—Ç–∞—Ç—É—Å:** –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è GPTQ –∫–≤–∞–Ω—Ç–∏–∑–æ–≤–∞–Ω–æ—ó –º–æ–¥–µ–ª—ñ...
üéØ **–Ø–∫—ñ—Å—Ç—å:** –í–∏—Å–æ–∫–∞ —è–∫—ñ—Å—Ç—å —é—Ä–∏–¥–∏—á–Ω–∏—Ö –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü—ñ–π –ø—ñ—Å–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—è
üåç **–ú–æ–≤–∏:** –ê–Ω–≥–ª—ñ–π—Å—å–∫–∞ —Ç–∞ —É–∫—Ä–∞—ó–Ω—Å—å–∫–∞
‚ö° **–û–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è:** GPTQ 4-bit –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü—ñ—è –¥–ª—è –µ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ

üí° **–ü–æ—Ä–∞–¥–∞:** –°–ø—Ä–æ–±—É–π—Ç–µ —â–µ —Ä–∞–∑ —á–µ—Ä–µ–∑ —Ö–≤–∏–ª–∏–Ω—É –¥–ª—è –æ—Ç—Ä–∏–º–∞–Ω–Ω—è –ø–æ–≤–Ω–æ—ó AI –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ."""
        else:
            return f"""ü§ñ **GPTQ Model Loading...**

**Your Question:** {question}

‚è≥ Model `{self.model_name}` is initializing. This typically takes 1-2 minutes for first startup.

üîÑ **Status:** Loading GPTQ quantized model...
üéØ **Quality:** High-quality legal consultations when complete
üåç **Languages:** English and Ukrainian  
‚ö° **Optimization:** GPTQ 4-bit quantization for efficiency

üí° **Tip:** Try again in a minute for full AI response."""
    
    def _generate_loading_error_response(self, question: str, language: str) -> str:
        """–û—Ç–≤–µ—Ç –ø—Ä–∏ –æ—à–∏–±–∫–µ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏"""
        if language == "uk":
            return f"""‚ùå **–ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è GPTQ –º–æ–¥–µ–ª—ñ**

**–í–∞—à–µ –ø–∏—Ç–∞–Ω–Ω—è:** {question}

üö´ **–ü—Ä–æ–±–ª–µ–º–∞:** –ú–æ–¥–µ–ª—å `{self.model_name}` –Ω–µ –∑–º–æ–≥–ª–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏—Å—è.
üìã **–ü–æ–º–∏–ª–∫–∞:** {self.loading_error}

üîß **–ú–æ–∂–ª–∏–≤—ñ –ø—Ä–∏—á–∏–Ω–∏:**
‚Ä¢ –ù–µ–¥–æ—Å—Ç–∞—Ç–Ω—å–æ –ø–∞–º'—è—Ç—ñ –¥–ª—è GPTQ –º–æ–¥–µ–ª—ñ
‚Ä¢ –í—ñ–¥—Å—É—Ç–Ω—ñ –∑–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ (auto-gptq, transformers)
‚Ä¢ –¢–∏–º—á–∞—Å–æ–≤—ñ –ø—Ä–æ–±–ª–µ–º–∏ –∑ HuggingFace Hub

üí° **–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó:**
‚Ä¢ –°–ø—Ä–æ–±—É–π—Ç–µ —â–µ —Ä–∞–∑ —á–µ—Ä–µ–∑ –∫—ñ–ª—å–∫–∞ —Ö–≤–∏–ª–∏–Ω
‚Ä¢ –ú–æ–¥–µ–ª—å –º–æ–∂–µ –∑–∞–≤–∞–Ω—Ç–∞–∂—É–≤–∞—Ç–∏—Å—è —É —Ñ–æ–Ω–æ–≤–æ–º—É —Ä–µ–∂–∏–º—ñ
‚Ä¢ –ü–µ—Ä–µ–≤—ñ—Ä—Ç–µ –¥–æ—Å—Ç—É–ø–Ω—ñ—Å—Ç—å HuggingFace —Å–µ—Ä–≤—ñ—Å—ñ–≤"""
        else:
            return f"""‚ùå **GPTQ Model Loading Error**

**Your Question:** {question}

üö´ **Issue:** Model `{self.model_name}` failed to load.
üìã **Error:** {self.loading_error}

üîß **Possible Causes:**
‚Ä¢ Insufficient memory for GPTQ model
‚Ä¢ Missing dependencies (auto-gptq, transformers)
‚Ä¢ Temporary HuggingFace Hub issues

üí° **Recommendations:**
‚Ä¢ Try again in a few minutes
‚Ä¢ Model may be loading in background
‚Ä¢ Check HuggingFace services availability"""
    
    def _generate_error_response(self, question: str, language: str, error: str) -> str:
        """–û—Ç–≤–µ—Ç –ø—Ä–∏ –æ—à–∏–±–∫–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏"""
        if language == "uk":
            return f"""‚ö†Ô∏è **–¢–∏–º—á–∞—Å–æ–≤–∞ —Ç–µ—Ö–Ω—ñ—á–Ω–∞ –ø—Ä–æ–±–ª–µ–º–∞**

–í–∏–Ω–∏–∫–ª–∞ –ø–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –æ–±—Ä–æ–±—Ü—ñ –∑–∞–ø–∏—Ç—É: "{question}"

üîß –î–µ—Ç–∞–ª—ñ: {error}

–°–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª—é–≤–∞—Ç–∏ –ø–∏—Ç–∞–Ω–Ω—è –∞–±–æ –ø–æ–≤—Ç–æ—Ä—ñ—Ç—å —Å–ø—Ä–æ–±—É."""
        else:
            return f"""‚ö†Ô∏è **Temporary Technical Issue**

Error processing query: "{question}"

üîß Details: {error}

Please try rephrasing your question or try again."""
    
    async def get_service_status(self):
        """–î–µ—Ç–∞–ª—å–Ω–∏–π —Å—Ç–∞—Ç—É—Å —Å–µ—Ä–≤–∏—Å–∞ –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏"""
        status = {
            "service_type": "huggingface_gptq_optimized",
            "model_name": self.model_name,
            "model_loaded": self.model_loaded,
            "loading_started": self.loading_started,
            "loading_error": self.loading_error,
            "platform": "HuggingFace Spaces" if self.hf_spaces else "Local",
            "supported_languages": ["en", "uk"],
            "optimizations": {
                "gptq_quantization": True,
                "memory_optimized": self.hf_spaces,
                "hf_spaces_mode": self.hf_spaces,
                "max_memory_gb": self.max_memory_gb,
                "loading_timeout": self.loading_timeout
            }
        }
        
        # –î–æ–±–∞–≤–ª—è–µ–º –¥–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
        try:
            import torch
            status["torch_available"] = True
            status["cuda_available"] = torch.cuda.is_available()
            if torch.cuda.is_available():
                status["cuda_device_count"] = torch.cuda.device_count()
        except ImportError:
            status["torch_available"] = False
        
        try:
            import transformers
            status["transformers_version"] = transformers.__version__
        except ImportError:
            status["transformers_available"] = False
        
        try:
            import auto_gptq
            status["auto_gptq_available"] = True
        except ImportError:
            status["auto_gptq_available"] = False
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ç–∞—Ç—É—Å–∞
        recommendations = []
        if not self.model_loaded:
            if self.loading_error:
                recommendations.extend([
                    "Check available memory and dependencies",
                    "Model loading failed - see error details",
                    "Try restarting the application"
                ])
            else:
                recommendations.extend([
                    "Model is loading in background",
                    "First load may take 1-2 minutes",
                    "Check /startup-progress for loading status"
                ])
        else:
            recommendations.extend([
                "Model ready for high-quality responses",
                "Supports Ukrainian legal consultations",
                "Optimized for HuggingFace Spaces"
            ])
        
        status["recommendations"] = recommendations
        
        return status

def create_llm_service(model_name: str = "TheBloke/Llama-2-7B-Chat-GPTQ"):
    """–°–æ–∑–¥–∞–µ—Ç –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π LLM —Å–µ—Ä–≤–∏—Å"""
    try:
        return HuggingFaceLLMService(model_name)
    except Exception as e:
        logger.error(f"Failed to create HuggingFace LLM service: {e}")
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —É–ª—É—á—à–µ–Ω–Ω—ã–π fallback
        from app.dependencies import ImprovedFallbackLLMService
        return ImprovedFallbackLLMService()