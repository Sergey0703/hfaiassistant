# backend/services/huggingface_llm_service.py
"""
LLM Service using HuggingFace Transformers with GPTQ quantization
–ó–∞–º–µ–Ω—è–µ—Ç Ollama –¥–ª—è —Ä–∞–±–æ—Ç—ã –Ω–∞ HuggingFace Spaces
"""

import logging
import time
import os
from typing import List, Dict, Optional
from dataclasses import dataclass

logger = logging.getLogger(__name__)

@dataclass
class LLMResponse:
    content: str
    model: str
    tokens_used: int
    response_time: float
    success: bool
    error: Optional[str] = None

class HuggingFaceLLMService:
    """LLM Service using HuggingFace Transformers with GPTQ"""
    
    def __init__(self, model_name: str = "TheBloke/Llama-2-7B-Chat-GPTQ"):
        self.model_name = model_name
        self.model = None
        self.tokenizer = None
        self.service_type = "huggingface_gptq"
        self.model_loaded = False
        self._initialize_model()
    
    def _initialize_model(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç GPTQ –º–æ–¥–µ–ª—å"""
        try:
            # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å
            logger.info(f"ü§ñ Loading GPTQ model: {self.model_name}")
            
            # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –±–∏–±–ª–∏–æ—Ç–µ–∫–∏
            from transformers import AutoTokenizer, AutoModelForCausalLM
            import torch
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã–µ GPTQ –º–æ–¥–µ–ª–∏ (–≤ –ø–æ—Ä—è–¥–∫–µ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è)
            gptq_models = [
                "TheBloke/Llama-2-7B-Chat-GPTQ",  # –°—Ç–∞–±–∏–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å
                "TheBloke/Mistral-7B-Instruct-v0.1-GPTQ",  # –ë—ã—Å—Ç—Ä–∞—è –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞
                "microsoft/DialoGPT-medium",  # Fallback –º–æ–¥–µ–ª—å
            ]
            
            for model_name in gptq_models:
                try:
                    logger.info(f"üîÑ Trying to load: {model_name}")
                    
                    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä
                    self.tokenizer = AutoTokenizer.from_pretrained(
                        model_name,
                        trust_remote_code=True,
                        use_fast=True
                    )
                    
                    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è GPTQ
                    model_kwargs = {
                        "torch_dtype": torch.float16,
                        "device_map": "auto",
                        "trust_remote_code": True,
                        "low_cpu_mem_usage": True,
                    }
                    
                    # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å GPTQ
                    try:
                        self.model = AutoModelForCausalLM.from_pretrained(
                            model_name,
                            **model_kwargs,
                            quantization_config={"load_in_4bit": True}
                        )
                        logger.info(f"‚úÖ Loaded with 4-bit quantization: {model_name}")
                    except:
                        # Fallback –±–µ–∑ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏
                        self.model = AutoModelForCausalLM.from_pretrained(
                            model_name,
                            **model_kwargs
                        )
                        logger.info(f"‚úÖ Loaded without quantization: {model_name}")
                    
                    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–∞
                    if self.tokenizer.pad_token is None:
                        self.tokenizer.pad_token = self.tokenizer.eos_token
                    
                    self.model_name = model_name
                    self.model_loaded = True
                    logger.info(f"üéâ Model ready: {model_name}")
                    return
                    
                except Exception as e:
                    logger.warning(f"‚ùå Failed to load {model_name}: {e}")
                    continue
            
            # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –∑–∞–≥—Ä—É–∑–∏–ª–æ—Å—å
            raise Exception("Failed to load any GPTQ model")
            
        except ImportError as e:
            logger.error(f"‚ùå Missing dependencies: {e}")
            logger.error("Install: pip install transformers torch auto-gptq")
            raise
        except Exception as e:
            logger.error(f"‚ùå Model initialization failed: {e}")
            raise
    
    async def answer_legal_question(self, question: str, context_documents: List[Dict], language: str = "en"):
        """–û—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–π –≤–æ–ø—Ä–æ—Å"""
        start_time = time.time()
        
        try:
            if not self.model_loaded:
                return LLMResponse(
                    content="Model not loaded. Please check logs for errors.",
                    model=self.model_name,
                    tokens_used=0,
                    response_time=time.time() - start_time,
                    success=False,
                    error="Model not initialized"
                )
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç
            prompt = self._build_legal_prompt(question, context_documents, language)
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
            response_text = await self._generate_response(prompt)
            
            response_time = time.time() - start_time
            
            return LLMResponse(
                content=response_text,
                model=self.model_name,
                tokens_used=len(response_text.split()),  # –ü—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–æ
                response_time=response_time,
                success=True
            )
            
        except Exception as e:
            logger.error(f"‚ùå Error generating response: {e}")
            return LLMResponse(
                content="I apologize, but I'm experiencing technical difficulties. Please try again later.",
                model=self.model_name,
                tokens_used=0,
                response_time=time.time() - start_time,
                success=False,
                error=str(e)
            )
    
    def _build_legal_prompt(self, question: str, context_documents: List[Dict], language: str) -> str:
        """–°—Ç—Ä–æ–∏—Ç –ø—Ä–æ–º–ø—Ç –¥–ª—è —é—Ä–∏–¥–∏—á–µ—Å–∫–æ–≥–æ –≤–æ–ø—Ä–æ—Å–∞"""
        
        if language == "uk":
            system_prompt = """–í–∏ - –¥–æ—Å–≤—ñ–¥—á–µ–Ω–∏–π —é—Ä–∏–¥–∏—á–Ω–∏–π –∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç. –í—ñ–¥–ø–æ–≤—ñ–¥–∞–π—Ç–µ –Ω–∞ –ø–∏—Ç–∞–Ω–Ω—è —á—ñ—Ç–∫–æ —Ç–∞ –ø—Ä–æ—Ñ–µ—Å—ñ–π–Ω–æ, –±–∞–∑—É—é—á–∏—Å—å –Ω–∞ –Ω–∞–¥–∞–Ω–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö.

–ü—Ä–∞–≤–∏–ª–∞:
- –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π—Ç–µ —Ç—ñ–ª—å–∫–∏ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –∑ –Ω–∞–¥–∞–Ω–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤
- –Ø–∫—â–æ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó –Ω–µ–¥–æ—Å—Ç–∞—Ç–Ω—å–æ, —Å–∫–∞–∂—ñ—Ç—å –ø—Ä–æ —Ü–µ —á–µ—Å–Ω–æ
- –î–∞–≤–∞–π—Ç–µ –ø—Ä–∞–∫—Ç–∏—á–Ω—ñ –ø–æ—Ä–∞–¥–∏
- –í—ñ–¥–ø–æ–≤—ñ–¥–∞–π—Ç–µ —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—é –º–æ–≤–æ—é"""
            
            context_intro = "üìö –î–æ–∫—É–º–µ–Ω—Ç–∏ –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É:"
            answer_intro = f"‚ùì –ü–∏—Ç–∞–Ω–Ω—è: {question}\n\nüìã –í—ñ–¥–ø–æ–≤—ñ–¥—å:"
        else:
            system_prompt = """You are an experienced legal consultant. Answer questions clearly and professionally based on the provided documents.

Rules:
- Use only information from the provided documents
- If information is insufficient, say so honestly
- Provide practical advice
- Be concise but thorough"""
            
            context_intro = "üìö Documents for analysis:"
            answer_intro = f"‚ùì Question: {question}\n\nüìã Answer:"
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        context_parts = []
        for i, doc in enumerate(context_documents[:3]):  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ 3 –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            filename = doc.get('filename', f'Document {i+1}')
            content = doc.get('content', '')[:600]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É
            context_parts.append(f"üìÑ {filename}:\n{content}")
        
        context = "\n\n".join(context_parts) if context_parts else "No relevant documents found."
        
        # –°–æ–±–∏—Ä–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –ø—Ä–æ–º–ø—Ç
        prompt = f"{system_prompt}\n\n{context_intro}\n{context}\n\n{answer_intro}"
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –æ–±—â—É—é –¥–ª–∏–Ω—É –ø—Ä–æ–º–ø—Ç–∞
        max_length = 2000
        if len(prompt) > max_length:
            prompt = prompt[:max_length] + "..."
        
        return prompt
    
    async def _generate_response(self, prompt: str, max_new_tokens: int = 500) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –∏—Å–ø–æ–ª—å–∑—É—è –º–æ–¥–µ–ª—å"""
        try:
            import torch
            
            # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç
            inputs = self.tokenizer.encode(
                prompt, 
                return_tensors="pt", 
                truncation=True, 
                max_length=1500
            )
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
            with torch.no_grad():
                outputs = self.model.generate(
                    inputs,
                    max_new_tokens=max_new_tokens,
                    num_return_sequences=1,
                    temperature=0.7,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                    attention_mask=torch.ones_like(inputs)
                )
            
            # –î–µ–∫–æ–¥–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
            full_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ –Ω–æ–≤—É—é —á–∞—Å—Ç—å (–æ—Ç–≤–µ—Ç)
            response = full_response[len(prompt):].strip()
            
            # –û—á–∏—â–∞–µ–º –æ—Ç–≤–µ—Ç
            response = self._clean_response(response)
            
            return response if response else "I need more specific information to provide a proper answer."
            
        except Exception as e:
            logger.error(f"‚ùå Error in response generation: {e}")
            return f"Technical error occurred while generating response. Please try with a simpler question."
    
    def _clean_response(self, response: str) -> str:
        """–û—á–∏—â–∞–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç"""
        # –£–±–∏—Ä–∞–µ–º –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è –∏ –ª–∏—à–Ω–∏–µ —Å–∏–º–≤–æ–ª—ã
        lines = response.split('\n')
        cleaned_lines = []
        seen_lines = set()
        
        for line in lines:
            line = line.strip()
            if line and line not in seen_lines and len(line) > 10:
                cleaned_lines.append(line)
                seen_lines.add(line)
        
        cleaned = '\n'.join(cleaned_lines)
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É
        if len(cleaned) > 1500:
            cleaned = cleaned[:1500] + "..."
        
        return cleaned
    
    async def get_service_status(self):
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç—É—Å —Å–µ—Ä–≤–∏—Å–∞"""
        return {
            "service_type": "huggingface_gptq",
            "model_loaded": self.model_loaded,
            "model_name": self.model_name,
            "ollama_available": False,
            "huggingface_available": True,
            "models_available": [self.model_name] if self.model_loaded else [],
            "default_model": self.model_name,
            "base_url": "local_transformers",
            "supported_languages": ["en", "uk"],
            "memory_usage": "~4-6GB with GPTQ quantization",
            "recommendations": [
                "Using HuggingFace Transformers with GPTQ quantization",
                "Model runs locally without external dependencies",
                "Optimized for HuggingFace Spaces environment"
            ]
        }

def create_llm_service(model_name: str = "TheBloke/Llama-2-7B-Chat-GPTQ"):
    """–°–æ–∑–¥–∞–µ—Ç LLM —Å–µ—Ä–≤–∏—Å –¥–ª—è HuggingFace Spaces"""
    try:
        return HuggingFaceLLMService(model_name)
    except Exception as e:
        logger.error(f"Failed to create HuggingFace LLM service: {e}")
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º fallback —Å–µ—Ä–≤–∏—Å
        from app.dependencies import FallbackLLMService
        return FallbackLLMService()