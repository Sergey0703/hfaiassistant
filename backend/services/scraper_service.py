# ====================================
# –§–ê–ô–õ: backend/services/scraper_service.py (–ë–ï–ó –î–ï–ú–û –†–ï–ñ–ò–ú–ê)
# –ó–∞–º–µ–Ω–∏—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π —Ñ–∞–π–ª –ø–æ–ª–Ω–æ—Å—Ç—å—é
# ====================================

"""
Legal Site Scraper Service - –†–µ–∞–ª—å–Ω—ã–π —Å–µ—Ä–≤–∏—Å –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö —Å–∞–π—Ç–æ–≤
"""

import aiohttp
import asyncio
import logging
import time
from typing import Dict, List, Optional, Any
from dataclasses import dataclass
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup

logger = logging.getLogger(__name__)

@dataclass
class ScrapedDocument:
    """–°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Å–ø–∞—Ä—Å–µ–Ω–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
    url: str
    title: str
    content: str
    metadata: Dict[str, Any]
    category: str = "scraped"

class LegalSiteScraper:
    """–°–µ—Ä–≤–∏—Å –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö —Å–∞–π—Ç–æ–≤"""
    
    def __init__(self):
        self.session = None
        self.legal_sites_config = {
            # –£–∫—Ä–∞–∏–Ω—Å–∫–∏–µ —Å–∞–π—Ç—ã
            "zakon.rada.gov.ua": {
                "title": "h1, .page-title, .document-title, .field-name-title",
                "content": ".field-item, .document-content, .law-text, .content, main, .field-type-text-long",
                "exclude": "nav, footer, .sidebar, .menu, script, style, .breadcrumb, .tabs"
            },
            "court.gov.ua": {
                "title": "h1, .title, .page-title",
                "content": ".content, .text, main, .field-item",
                "exclude": "nav, footer, .menu, .sidebar"
            },
            
            # –ò—Ä–ª–∞–Ω–¥—Å–∫–∏–µ —Å–∞–π—Ç—ã
            "irishstatutebook.ie": {
                "title": "h1, .act-title, .page-title",
                "content": ".act-text, .section, .content, .akn-akomaNtoso",
                "exclude": "nav, footer, .navigation, .sidebar"
            },
            "citizensinformation.ie": {
                "title": "h1, .page-title",
                "content": ".content, .article-content, .main-content",
                "exclude": "nav, footer, .sidebar, .navigation"
            },
            "courts.ie": {
                "title": "h1, .page-title",
                "content": ".content, .main-content",
                "exclude": "nav, footer, .menu, .sidebar"
            }
        }
        
        logger.info("üåê Legal Site Scraper initialized (Real mode only)")
    
    async def _get_session(self):
        """–ü–æ–ª—É—á–∞–µ—Ç –∏–ª–∏ —Å–æ–∑–¥–∞–µ—Ç HTTP —Å–µ—Å—Å–∏—é"""
        if self.session is None or self.session.closed:
            timeout = aiohttp.ClientTimeout(total=30, connect=10)
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'en-US,en;q=0.9,uk;q=0.8',
                'Accept-Encoding': 'gzip, deflate',
                'Connection': 'keep-alive'
            }
            self.session = aiohttp.ClientSession(timeout=timeout, headers=headers)
        return self.session
    
    async def scrape_legal_site(self, url: str) -> Optional[ScrapedDocument]:
        """–ü–∞—Ä—Å–∏—Ç —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–π —Å–∞–π—Ç - —Ç–æ–ª—å–∫–æ —Ä–µ–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥"""
        
        try:
            session = await self._get_session()
            
            logger.info(f"üîç Scraping URL: {url}")
            
            async with session.get(url) as response:
                if response.status != 200:
                    logger.error(f"‚ùå HTTP {response.status} for {url}")
                    raise Exception(f"HTTP {response.status}")
                
                html_content = await response.text()
                
                # –ü–∞—Ä—Å–∏–º HTML
                soup = BeautifulSoup(html_content, 'html.parser')
                
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –¥–ª—è —Å–∞–π—Ç–∞
                domain = urlparse(url).netloc
                config = self._get_site_config(domain)
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
                title = self._extract_title(soup, config)
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç
                content = self._extract_content(soup, config)
                
                if not content or len(content.strip()) < 100:
                    logger.error(f"‚ùå Insufficient content extracted from {url}")
                    raise Exception("Insufficient content extracted")
                
                # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
                metadata = {
                    "url": url,
                    "scraped_at": time.time(),
                    "status_code": response.status,
                    "content_length": len(content),
                    "word_count": len(content.split()),
                    "real_scraping": True,
                    "domain": domain,
                    "scraper": "LegalSiteScraper"
                }
                
                logger.info(f"‚úÖ Successfully scraped {url}: {len(content)} chars")
                
                return ScrapedDocument(
                    url=url,
                    title=title,
                    content=content,
                    metadata=metadata,
                    category=self._categorize_by_domain(domain)
                )
                
        except aiohttp.ClientError as e:
            logger.error(f"‚ùå Network error scraping {url}: {e}")
            raise Exception(f"Network error: {e}")
        except Exception as e:
            logger.error(f"‚ùå Error scraping {url}: {e}")
            raise Exception(f"Scraping error: {e}")
    
    def _get_site_config(self, domain: str) -> Dict[str, str]:
        """–ü–æ–ª—É—á–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –¥–æ–º–µ–Ω–∞"""
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
        if domain in self.legal_sites_config:
            return self.legal_sites_config[domain]
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–¥–¥–æ–º–µ–Ω—ã
        for site_domain, config in self.legal_sites_config.items():
            if domain.endswith(site_domain) or site_domain in domain:
                return config
        
        # –î–µ—Ñ–æ–ª—Ç–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
        return {
            "title": "h1, title, .title, .page-title",
            "content": "main, .content, .main-content, article, .article, body",
            "exclude": "nav, footer, .sidebar, .menu, script, style, .ads, .navigation"
        }
    
    def _extract_title(self, soup, config: Dict[str, str]) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        title_selectors = config.get("title", "h1, title").split(", ")
        
        for selector in title_selectors:
            try:
                element = soup.select_one(selector.strip())
                if element and element.get_text(strip=True):
                    title = element.get_text(strip=True)
                    if len(title) > 5:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –∑–∞–≥–æ–ª–æ–≤–∫–∞
                        return title[:200]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É
            except Exception:
                continue
        
        # Fallback –∫ title —Ç–µ–≥—É
        title_tag = soup.find('title')
        if title_tag and title_tag.get_text(strip=True):
            return title_tag.get_text(strip=True)[:200]
        
        return "Untitled Document"
    
    def _extract_content(self, soup, config: Dict[str, str]) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        try:
            # –£–¥–∞–ª—è–µ–º –Ω–µ–∂–µ–ª–∞—Ç–µ–ª—å–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
            exclude_selectors = config.get("exclude", "").split(", ")
            for selector in exclude_selectors:
                if selector.strip():
                    for element in soup.select(selector.strip()):
                        element.decompose()
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç
            content_selectors = config.get("content", "main, .content").split(", ")
            content_parts = []
            
            for selector in content_selectors:
                try:
                    elements = soup.select(selector.strip())
                    for element in elements:
                        text = element.get_text(separator='\n', strip=True)
                        if text and len(text) > 50:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞
                            content_parts.append(text)
                except Exception:
                    continue
            
            if content_parts:
                content = '\n\n'.join(content_parts)
                # –û—á–∏—â–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ –ø–µ—Ä–µ–Ω–æ—Å—ã
                lines = []
                for line in content.split('\n'):
                    cleaned_line = line.strip()
                    if cleaned_line and len(cleaned_line) > 5:  # –£–±–∏—Ä–∞–µ–º –æ—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∏–µ —Å—Ç—Ä–æ–∫–∏
                        lines.append(cleaned_line)
                
                return '\n'.join(lines)
            
            # Fallback - –≤–µ—Å—å —Ç–µ–∫—Å—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            body_text = soup.get_text(separator='\n', strip=True)
            if body_text and len(body_text) > 100:
                return body_text
            
            return ""
            
        except Exception as e:
            logger.error(f"Error extracting content: {e}")
            return ""
    
    def _categorize_by_domain(self, domain: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏—é –¥–æ–∫—É–º–µ–Ω—Ç–∞ –ø–æ –¥–æ–º–µ–Ω—É"""
        domain_lower = domain.lower()
        
        # –£–∫—Ä–∞–∏–Ω—Å–∫–∏–µ —Å–∞–π—Ç—ã
        if any(ua_domain in domain_lower for ua_domain in [
            "zakon.rada.gov.ua", "rada.gov.ua", "court.gov.ua", 
            "minjust.gov.ua", "ccu.gov.ua", "npu.gov.ua"
        ]):
            return "ukraine_legal"
        
        # –ò—Ä–ª–∞–Ω–¥—Å–∫–∏–µ —Å–∞–π—Ç—ã
        if any(ie_domain in domain_lower for ie_domain in [
            "irishstatutebook.ie", "courts.ie", "citizensinformation.ie",
            "justice.ie", "oireachtas.ie", "gov.ie"
        ]):
            return "ireland_legal"
        
        # –û–±—â–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        if any(keyword in domain_lower for keyword in ["law", "legal", "court", "justice"]):
            return "legislation"
        elif any(keyword in domain_lower for keyword in ["zakon", "pravo", "sud"]):
            return "legislation"
        elif "court" in domain_lower or "sud" in domain_lower:
            return "jurisprudence"
        elif any(keyword in domain_lower for keyword in ["gov", "government", "ministry"]):
            return "government"
        elif any(keyword in domain_lower for keyword in ["citizen", "immigration", "rights"]):
            return "civil_rights"
        else:
            return "scraped"
    
    async def scrape_multiple_urls(self, urls: List[str], delay: float = 1.0) -> List[Optional[ScrapedDocument]]:
        """–ü–∞—Ä—Å–∏—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ URL —Å –∑–∞–¥–µ—Ä–∂–∫–æ–π"""
        results = []
        
        for i, url in enumerate(urls):
            if i > 0 and delay > 0:
                await asyncio.sleep(delay)
            
            try:
                document = await self.scrape_legal_site(url)
                results.append(document)
                logger.info(f"‚úÖ Successfully processed {i+1}/{len(urls)}: {url}")
                
            except Exception as e:
                logger.error(f"‚ùå Failed {i+1}/{len(urls)}: {url} - {e}")
                results.append(None)
        
        successful = len([r for r in results if r is not None])
        logger.info(f"üéØ Bulk scrape completed: {successful}/{len(urls)} successful")
        
        return results
    
    async def validate_url(self, url: str) -> Dict[str, Any]:
        """–í–∞–ª–∏–¥–∏—Ä—É–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å URL"""
        try:
            session = await self._get_session()
            
            async with session.head(url, timeout=aiohttp.ClientTimeout(total=10)) as response:
                return {
                    "url": url,
                    "valid": True,
                    "status_code": response.status,
                    "reachable": response.status < 400,
                    "content_type": response.headers.get('content-type', 'unknown'),
                    "content_length": response.headers.get('content-length', 'unknown')
                }
                
        except Exception as e:
            return {
                "url": url,
                "valid": False,
                "reachable": False,
                "error": str(e)
            }
    
    def get_supported_sites(self) -> Dict[str, Any]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã—Ö —Å–∞–π—Ç–æ–≤"""
        return {
            "sites": list(self.legal_sites_config.keys()),
            "total": len(self.legal_sites_config),
            "real_scraping_available": True  # –í—Å–µ–≥–¥–∞ True, —Ç–∞–∫ –∫–∞–∫ –Ω–µ—Ç –¥–µ–º–æ —Ä–µ–∂–∏–º–∞
        }
    
    async def close(self):
        """–ó–∞–∫—Ä—ã–≤–∞–µ—Ç HTTP —Å–µ—Å—Å–∏—é"""
        if self.session and not self.session.closed:
            await self.session.close()
            logger.debug("üîí Scraper session closed")