# backend/services/llamacpp_llm_service.py - –ù–û–í–´–ô LLM –°–ï–†–í–ò–° –ù–ê LLAMA.CPP
"""
LLM Service using llama-cpp-python for stable CPU inference on HuggingFace Spaces
–ù–û–í–´–ô –ü–û–î–•–û–î: llama.cpp –≤–º–µ—Å—Ç–æ transformers –¥–ª—è –ª—É—á—à–µ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å HF Spaces
"""

import logging
import time
import os
import gc
from typing import List, Dict, Optional
from dataclasses import dataclass
import asyncio

logger = logging.getLogger(__name__)

@dataclass
class LLMResponse:
    content: str
    model: str
    tokens_used: int
    response_time: float
    success: bool
    error: Optional[str] = None

class LlamaCppLLMService:
    """LLM Service –Ω–∞ –æ—Å–Ω–æ–≤–µ llama-cpp-python –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ–π —Ä–∞–±–æ—Ç—ã –Ω–∞ HF Spaces"""
    
    def __init__(self, model_name: str = "TheBloke/Llama-2-7B-Chat-GGUF"):
        self.model_name = model_name
        self.model_file = "llama-2-7b-chat.Q4_K_M.gguf"  # –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —Ñ–∞–π–ª –¥–ª—è CPU
        self.model = None
        self.service_type = "llamacpp_cpu_optimized"
        self.model_loaded = False
        self.loading_started = False
        self.loading_error = None
        self.hf_spaces = os.getenv("SPACE_ID") is not None
        
        # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è HF Spaces
        self.max_tokens = 150 if self.hf_spaces else 300
        self.context_length = 2048 if self.hf_spaces else 4096
        self.n_threads = 2 if self.hf_spaces else 4  # CPU –ø–æ—Ç–æ–∫–∏
        
        logger.info(f"ü¶ô Initializing LlamaCpp LLM service for: {model_name}")
        logger.info(f"üåç Platform: {'HuggingFace Spaces' if self.hf_spaces else 'Local'}")
        logger.info(f"üìÅ Model file: {self.model_file}")
        
        # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –Ω–∞—á–∏–Ω–∞–µ–º –∑–∞–≥—Ä—É–∑–∫—É
        self._start_model_loading()
    
    def _start_model_loading(self):
        """–ë–µ–∑–æ–ø–∞—Å–Ω–æ –Ω–∞—á–∏–Ω–∞–µ—Ç –∑–∞–≥—Ä—É–∑–∫—É –º–æ–¥–µ–ª–∏"""
        if self.loading_started:
            return
        
        self.loading_started = True
        
        try:
            logger.info(f"üîÑ Starting LlamaCpp model loading: {self.model_name}")
            self._load_model_llamacpp()
        except Exception as e:
            self.loading_error = str(e)
            logger.error(f"‚ùå LlamaCpp model loading initiation failed: {e}")
    
    def _load_model_llamacpp(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å —á–µ—Ä–µ–∑ llama-cpp-python"""
        start_time = time.time()
        
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å llama-cpp-python
            try:
                from llama_cpp import Llama
                logger.info("üìö llama-cpp-python available, starting model load...")
            except ImportError as e:
                error_msg = f"llama-cpp-python not installed: {e}"
                self.loading_error = error_msg
                logger.error(f"‚ùå {error_msg}")
                logger.error("Install: pip install llama-cpp-python")
                raise
            
            # –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏ –ø–µ—Ä–µ–¥ –∑–∞–≥—Ä—É–∑–∫–æ–π
            if self.hf_spaces:
                gc.collect()
                logger.info("üßπ Memory cleanup for HF Spaces")
            
            # –°–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤ –º–æ–¥–µ–ª–∏ –¥–ª—è –ø–æ–ø—ã—Ç–∫–∏ (–æ—Ç –ª–µ–≥–∫–∏—Ö –∫ —Ç—è–∂–µ–ª—ã–º)
            model_files = [
                "llama-2-7b-chat.Q4_K_S.gguf",  # –°–∞–º—ã–π –ª–µ–≥–∫–∏–π
                "llama-2-7b-chat.Q4_K_M.gguf",  # –°—Ä–µ–¥–Ω–∏–π (–Ω–∞—à –æ—Å–Ω–æ–≤–Ω–æ–π)
                "llama-2-7b-chat.Q5_K_M.gguf",  # –ë–æ–ª–µ–µ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π
            ]
            
            for attempt, model_file in enumerate(model_files):
                try:
                    logger.info(f"üîÑ Attempt {attempt + 1}: Loading {model_file}")
                    
                    # –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è HF Spaces
                    model_kwargs = {
                        "model_path": None,  # –ë—É–¥–µ—Ç –∑–∞–ø–æ–ª–Ω–µ–Ω–æ –Ω–∏–∂–µ
                        "n_ctx": self.context_length,  # –ö–æ–Ω—Ç–µ–∫—Å—Ç
                        "n_threads": self.n_threads,  # CPU –ø–æ—Ç–æ–∫–∏
                        "n_gpu_layers": 0,  # –¢–æ–ª—å–∫–æ CPU
                        "use_mmap": True,  # Memory mapping –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
                        "use_mlock": False,  # –ù–µ –±–ª–æ–∫–∏—Ä—É–µ–º –ø–∞–º—è—Ç—å
                        "verbose": False,  # –¢–∏—Ö–∏–π —Ä–µ–∂–∏–º
                        "n_batch": 128 if self.hf_spaces else 512,  # –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
                    }
                    
                    # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å –∏–∑ —Ä–∞–∑–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
                    model_sources = [
                        f"hf/{self.model_name}/{model_file}",  # HuggingFace Hub
                        f"./{model_file}",  # –õ–æ–∫–∞–ª—å–Ω—ã–π —Ñ–∞–π–ª
                        f"./models/{model_file}",  # –ü–∞–ø–∫–∞ models
                    ]
                    
                    for source in model_sources:
                        try:
                            logger.info(f"üîç Trying to load from: {source}")
                            model_kwargs["model_path"] = source
                            
                            self.model = Llama(**model_kwargs)
                            
                            # –ï—Å–ª–∏ –¥–æ—à–ª–∏ —Å—é–¥–∞ - –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∑–∏–ª–∞—Å—å
                            self.model_file = model_file
                            self.model_loaded = True
                            
                            loading_time = time.time() - start_time
                            logger.info(f"‚úÖ LlamaCpp model loaded successfully!")
                            logger.info(f"üìÅ File: {model_file}")
                            logger.info(f"üìç Source: {source}")
                            logger.info(f"‚è±Ô∏è Load time: {loading_time:.1f}s")
                            
                            # –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏ –ø–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏
                            if self.hf_spaces:
                                gc.collect()
                                logger.info("üßπ Post-loading memory cleanup")
                            
                            return  # –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∑–∏–ª–∏, –≤—ã—Ö–æ–¥–∏–º
                            
                        except Exception as source_error:
                            logger.debug(f"Failed to load from {source}: {source_error}")
                            continue
                    
                    # –ï—Å–ª–∏ –≤—Å–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–∏ –¥–ª—è —ç—Ç–æ–≥–æ —Ñ–∞–π–ª–∞
                    raise Exception(f"Could not load {model_file} from any source")
                    
                except Exception as file_error:
                    logger.warning(f"‚ùå Failed to load {model_file}: {file_error}")
                    
                    # –û—á–∏—â–∞–µ–º –ø–∞–º—è—Ç—å –ø–µ—Ä–µ–¥ —Å–ª–µ–¥—É—é—â–µ–π –ø–æ–ø—ã—Ç–∫–æ–π
                    if hasattr(self, 'model') and self.model is not None:
                        del self.model
                        self.model = None
                    
                    gc.collect()
                    
                    if attempt == len(model_files) - 1:
                        # –ü–æ—Å–ª–µ–¥–Ω—è—è –ø–æ–ø—ã—Ç–∫–∞
                        raise file_error
                    else:
                        logger.info(f"üîÑ Trying next model file...")
                        time.sleep(1)  # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞
            
        except Exception as e:
            self.loading_error = str(e)
            logger.error(f"‚ùå LlamaCpp model loading failed completely: {e}")
            
            # –°–æ–∑–¥–∞–µ–º fallback –º–æ–¥–µ–ª—å (–∑–∞–≥–ª—É—à–∫–∞)
            self._create_fallback_model()
    
    def _create_fallback_model(self):
        """–°–æ–∑–¥–∞–µ—Ç fallback –º–æ–¥–µ–ª—å –µ—Å–ª–∏ –æ—Å–Ω–æ–≤–Ω–∞—è –Ω–µ –∑–∞–≥—Ä—É–∑–∏–ª–∞—Å—å"""
        logger.info("üîÑ Creating fallback model...")
        
        class FallbackModel:
            def __call__(self, prompt: str, max_tokens: int = 100, **kwargs):
                return {
                    "choices": [{
                        "text": f"LlamaCpp model loading failed. This is a fallback response for: {prompt[:50]}..."
                    }]
                }
        
        self.model = FallbackModel()
        self.model_loaded = True
        self.service_type = "llamacpp_fallback"
        logger.info("‚úÖ Fallback model ready")
    
    async def answer_legal_question(self, question: str, context_documents: List[Dict], language: str = "en"):
        """–û—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–π –≤–æ–ø—Ä–æ—Å —á–µ—Ä–µ–∑ LlamaCpp"""
        start_time = time.time()
        
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∞—Ç—É—Å –º–æ–¥–µ–ª–∏
            if not self.model_loaded:
                if self.loading_error:
                    return LLMResponse(
                        content=self._generate_loading_error_response(question, language),
                        model=self.model_name,
                        tokens_used=0,
                        response_time=time.time() - start_time,
                        success=False,
                        error=f"Model loading failed: {self.loading_error}"
                    )
                else:
                    return LLMResponse(
                        content=self._generate_loading_response(question, language),
                        model=self.model_name,
                        tokens_used=0,
                        response_time=time.time() - start_time,
                        success=True,
                        error="Model still loading"
                    )
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç
            prompt = self._build_chat_prompt(question, context_documents, language)
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç —á–µ—Ä–µ–∑ LlamaCpp
            response_text = await self._generate_response_llamacpp(prompt)
            
            response_time = time.time() - start_time
            
            return LLMResponse(
                content=response_text,
                model=f"{self.model_name}/{self.model_file}",
                tokens_used=len(response_text.split()),
                response_time=response_time,
                success=True
            )
            
        except Exception as e:
            logger.error(f"‚ùå Error generating response: {e}")
            return LLMResponse(
                content=self._generate_error_response(question, language, str(e)),
                model=self.model_name,
                tokens_used=0,
                response_time=time.time() - start_time,
                success=False,
                error=str(e)
            )
    
    def _build_chat_prompt(self, question: str, context_documents: List[Dict], language: str) -> str:
        """–°—Ç—Ä–æ–∏—Ç –ø—Ä–æ–º–ø—Ç –¥–ª—è Llama-2-Chat —Ñ–æ—Ä–º–∞—Ç–∞"""
        
        # –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç
        if language == "uk":
            system_message = "–í–∏ - –¥–æ—Å–≤—ñ–¥—á–µ–Ω–∏–π —é—Ä–∏–¥–∏—á–Ω–∏–π –∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç. –ù–∞–¥–∞–≤–∞–π—Ç–µ —Ç–æ—á–Ω—ñ —Ç–∞ –∫–æ—Ä–∏—Å–Ω—ñ –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ –Ω–∞ –æ—Å–Ω–æ–≤—ñ –Ω–∞–¥–∞–Ω–æ—ó —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó."
        else:
            system_message = "You are an experienced legal consultant. Provide accurate and helpful answers based on the provided information."
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        context_parts = []
        max_docs = 2 if self.hf_spaces else 3
        max_doc_length = 400 if self.hf_spaces else 600
        
        for i, doc in enumerate(context_documents[:max_docs]):
            filename = doc.get('filename', f'Document {i+1}')
            content = doc.get('content', '')[:max_doc_length]
            if content.strip():
                context_parts.append(f"Document {filename}: {content.strip()}")
        
        context = "\n\n".join(context_parts) if context_parts else ""
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º user message
        if context:
            if language == "uk":
                user_message = f"–ö–æ–Ω—Ç–µ–∫—Å—Ç:\n{context}\n\n–ü–∏—Ç–∞–Ω–Ω—è: {question}\n\n–î–∞–π—Ç–µ –¥–µ—Ç–∞–ª—å–Ω—É –≤—ñ–¥–ø–æ–≤—ñ–¥—å:"
            else:
                user_message = f"Context:\n{context}\n\nQuestion: {question}\n\nProvide a detailed answer:"
        else:
            if language == "uk":
                user_message = f"–ü–∏—Ç–∞–Ω–Ω—è: {question}\n\n–î–∞–π—Ç–µ –∑–∞–≥–∞–ª—å–Ω—É —é—Ä–∏–¥–∏—á–Ω—É —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é:"
            else:
                user_message = f"Question: {question}\n\nProvide general legal information:"
        
        # Llama-2-Chat —Ñ–æ—Ä–º–∞—Ç
        prompt = f"""<s>[INST] <<SYS>>
{system_message}
<</SYS>>

{user_message} [/INST]"""
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –æ–±—â—É—é –¥–ª–∏–Ω—É –ø—Ä–æ–º–ø—Ç–∞
        max_prompt_length = 1500 if self.hf_spaces else 2000
        if len(prompt) > max_prompt_length:
            # –û–±—Ä–µ–∑–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç, –Ω–æ —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É
            available_for_context = max_prompt_length - len(prompt) + len(context)
            if available_for_context > 100:
                context = context[:available_for_context] + "..."
                if language == "uk":
                    user_message = f"–ö–æ–Ω—Ç–µ–∫—Å—Ç:\n{context}\n\n–ü–∏—Ç–∞–Ω–Ω—è: {question}\n\n–î–∞–π—Ç–µ –¥–µ—Ç–∞–ª—å–Ω—É –≤—ñ–¥–ø–æ–≤—ñ–¥—å:"
                else:
                    user_message = f"Context:\n{context}\n\nQuestion: {question}\n\nProvide a detailed answer:"
                
                prompt = f"""<s>[INST] <<SYS>>
{system_message}
<</SYS>>

{user_message} [/INST]"""
        
        return prompt
    
    async def _generate_response_llamacpp(self, prompt: str) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç —á–µ—Ä–µ–∑ LlamaCpp —Å —Ç–∞–π–º–∞—É—Ç–æ–º"""
        try:
            # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª—è HF Spaces
            generation_kwargs = {
                "max_tokens": self.max_tokens,
                "temperature": 0.1,  # –ù–∏–∑–∫–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–ª—è —Ç–æ—á–Ω–æ—Å—Ç–∏
                "top_p": 0.9,
                "top_k": 40,
                "repeat_penalty": 1.1,
                "stop": ["</s>", "[/INST]", "<s>"],  # –°—Ç–æ–ø-—Å–ª–æ–≤–∞ –¥–ª—è Llama-2
                "echo": False,  # –ù–µ –ø–æ–≤—Ç–æ—Ä—è—Ç—å –ø—Ä–æ–º–ø—Ç
            }
            
            logger.info(f"üîß LlamaCpp generation: max_tokens={self.max_tokens}, temp=0.1")
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å —Ç–∞–π–º–∞—É—Ç–æ–º
            def _generate_sync():
                return self.model(prompt, **generation_kwargs)
            
            # –ó–∞–ø—É—Å–∫–∞–µ–º —Å–∏–Ω—Ö—Ä–æ–Ω–Ω—É—é –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –≤ executor —Å —Ç–∞–π–º–∞—É—Ç–æ–º
            import concurrent.futures
            
            with concurrent.futures.ThreadPoolExecutor() as executor:
                future = executor.submit(_generate_sync)
                try:
                    # –¢–∞–π–º–∞—É—Ç 30 —Å–µ–∫—É–Ω–¥ –¥–ª—è HF Spaces
                    timeout = 30 if self.hf_spaces else 60
                    result = future.result(timeout=timeout)
                except concurrent.futures.TimeoutError:
                    logger.error(f"‚ùå LlamaCpp generation timeout after {timeout}s")
                    return "Response generation timed out. Please try with a shorter question."
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –æ—Ç–≤–µ—Ç–∞
            if isinstance(result, dict) and "choices" in result and result["choices"]:
                response_text = result["choices"][0].get("text", "").strip()
            else:
                response_text = str(result).strip()
            
            # –û—á–∏—â–∞–µ–º –æ—Ç–≤–µ—Ç
            response_text = self._clean_response(response_text)
            
            # –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏ –ø–æ—Å–ª–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            if self.hf_spaces:
                gc.collect()
            
            return response_text if response_text else "I need more specific information to provide a proper answer."
            
        except Exception as e:
            logger.error(f"‚ùå LlamaCpp generation error: {e}")
            return f"Error generating response: {str(e)}"
    
    def _clean_response(self, response: str) -> str:
        """–û—á–∏—â–∞–µ—Ç –æ—Ç–≤–µ—Ç –æ—Ç –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤"""
        if not response:
            return response
        
        # –£–¥–∞–ª—è–µ–º —Å—Ç–æ–ø-—Ç–æ–∫–µ–Ω—ã –µ—Å–ª–∏ –æ–Ω–∏ –ø–æ–ø–∞–ª–∏ –≤ –æ—Ç–≤–µ—Ç
        stop_tokens = ["</s>", "[/INST]", "<s>", "[INST]"]
        for token in stop_tokens:
            response = response.replace(token, "")
        
        # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ –ø–µ—Ä–µ–Ω–æ—Å—ã
        lines = []
        for line in response.split('\n'):
            line = line.strip()
            if line and not line.startswith('System:') and not line.startswith('User:'):
                lines.append(line)
        
        cleaned = '\n'.join(lines[:10])  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –æ–±—â—É—é –¥–ª–∏–Ω—É
        max_length = 1000 if self.hf_spaces else 1500
        if len(cleaned) > max_length:
            cleaned = cleaned[:max_length] + "..."
        
        return cleaned
    
    def _generate_loading_response(self, question: str, language: str) -> str:
        """–û—Ç–≤–µ—Ç –ø–æ–∫–∞ –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è"""
        if language == "uk":
            return f"""ü¶ô **LlamaCpp –º–æ–¥–µ–ª—å –∑–∞–≤–∞–Ω—Ç–∞–∂—É—î—Ç—å—Å—è...**

**–í–∞—à–µ –ø–∏—Ç–∞–Ω–Ω—è:** {question}

‚è≥ –ú–æ–¥–µ–ª—å `{self.model_name}` ({self.model_file}) —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑—É—î—Ç—å—Å—è —á–µ—Ä–µ–∑ llama.cpp. –¶–µ –∑–∞–∑–≤–∏—á–∞–π –∑–∞–π–º–∞—î 30-60 —Å–µ–∫—É–Ω–¥.

üîÑ **–°—Ç–∞—Ç—É—Å:** –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è GGUF –∫–≤–∞–Ω—Ç–∏–∑–æ–≤–∞–Ω–æ—ó –º–æ–¥–µ–ª—ñ...
üéØ **–Ø–∫—ñ—Å—Ç—å:** –°—Ç–∞–±—ñ–ª—å–Ω–∞ —Ä–æ–±–æ—Ç–∞ –Ω–∞ CPU
üåç **–ú–æ–≤–∏:** –ê–Ω–≥–ª—ñ–π—Å—å–∫–∞ —Ç–∞ —É–∫—Ä–∞—ó–Ω—Å—å–∫–∞
‚ö° **–û–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è:** llama.cpp –¥–ª—è –µ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—ó —Ä–æ–±–æ—Ç–∏ –Ω–∞ CPU

üí° **–ü–æ—Ä–∞–¥–∞:** LlamaCpp –º–æ–¥–µ–ª—å –±—É–¥–µ –ø—Ä–∞—Ü—é–≤–∞—Ç–∏ —Å—Ç–∞–±—ñ–ª—å–Ω–æ –±–µ–∑ –∑–∞–≤–∏—Å–∞–Ω—å."""
        else:
            return f"""ü¶ô **LlamaCpp Model Loading...**

**Your Question:** {question}

‚è≥ Model `{self.model_name}` ({self.model_file}) is initializing via llama.cpp. This typically takes 30-60 seconds.

üîÑ **Status:** Loading GGUF quantized model...
üéØ **Quality:** Stable CPU performance
üåç **Languages:** English and Ukrainian  
‚ö° **Optimization:** llama.cpp for efficient CPU inference

üí° **Tip:** LlamaCpp model will work stably without hanging."""
    
    def _generate_loading_error_response(self, question: str, language: str) -> str:
        """–û—Ç–≤–µ—Ç –ø—Ä–∏ –æ—à–∏–±–∫–µ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏"""
        if language == "uk":
            return f"""‚ùå **–ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è LlamaCpp –º–æ–¥–µ–ª—ñ**

**–í–∞—à–µ –ø–∏—Ç–∞–Ω–Ω—è:** {question}

üö´ **–ü—Ä–æ–±–ª–µ–º–∞:** –ú–æ–¥–µ–ª—å `{self.model_name}` –Ω–µ –∑–º–æ–≥–ª–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏—Å—è.
üìã **–ü–æ–º–∏–ª–∫–∞:** {self.loading_error}

üîß **–ú–æ–∂–ª–∏–≤—ñ –ø—Ä–∏—á–∏–Ω–∏:**
‚Ä¢ –í—ñ–¥—Å—É—Ç–Ω—è –±—ñ–±–ª—ñ–æ—Ç–µ–∫–∞ llama-cpp-python
‚Ä¢ –§–∞–π–ª –º–æ–¥–µ–ª—ñ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ
‚Ä¢ –ù–µ–¥–æ—Å—Ç–∞—Ç–Ω—å–æ –ø–∞–º'—è—Ç—ñ

üí° **–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó:**
‚Ä¢ –í—Å—Ç–∞–Ω–æ–≤—ñ—Ç—å: pip install llama-cpp-python
‚Ä¢ –ü–µ—Ä–µ–≤—ñ—Ä—Ç–µ –¥–æ—Å—Ç—É–ø–Ω—ñ—Å—Ç—å —Ñ–∞–π–ª—É –º–æ–¥–µ–ª—ñ
‚Ä¢ –°–ø—Ä–æ–±—É–π—Ç–µ —â–µ —Ä–∞–∑ —á–µ—Ä–µ–∑ –∫—ñ–ª—å–∫–∞ —Ö–≤–∏–ª–∏–Ω"""
        else:
            return f"""‚ùå **LlamaCpp Model Loading Error**

**Your Question:** {question}

üö´ **Issue:** Model `{self.model_name}` failed to load.
üìã **Error:** {self.loading_error}

üîß **Possible Causes:**
‚Ä¢ Missing llama-cpp-python library
‚Ä¢ Model file not found
‚Ä¢ Insufficient memory

üí° **Recommendations:**
‚Ä¢ Install: pip install llama-cpp-python
‚Ä¢ Check model file availability
‚Ä¢ Try again in a few minutes"""
    
    def _generate_error_response(self, question: str, language: str, error: str) -> str:
        """–û—Ç–≤–µ—Ç –ø—Ä–∏ –æ—à–∏–±–∫–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏"""
        if language == "uk":
            return f"""‚ö†Ô∏è **–¢–∏–º—á–∞—Å–æ–≤–∞ —Ç–µ—Ö–Ω—ñ—á–Ω–∞ –ø—Ä–æ–±–ª–µ–º–∞**

–í–∏–Ω–∏–∫–ª–∞ –ø–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –æ–±—Ä–æ–±—Ü—ñ –∑–∞–ø–∏—Ç—É: "{question}"

üîß –î–µ—Ç–∞–ª—ñ: {error}

–°–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª—é–≤–∞—Ç–∏ –ø–∏—Ç–∞–Ω–Ω—è –∞–±–æ –ø–æ–≤—Ç–æ—Ä—ñ—Ç—å —Å–ø—Ä–æ–±—É."""
        else:
            return f"""‚ö†Ô∏è **Temporary Technical Issue**

Error processing query: "{question}"

üîß Details: {error}

Please try rephrasing your question or try again."""
    
    async def get_service_status(self):
        """–î–µ—Ç–∞–ª—å–Ω–∏–π —Å—Ç–∞—Ç—É—Å —Å–µ—Ä–≤–∏—Å–∞ –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏"""
        status = {
            "service_type": "llamacpp_cpu_optimized",
            "model_name": self.model_name,
            "model_file": self.model_file,
            "model_loaded": self.model_loaded,
            "loading_started": self.loading_started,
            "loading_error": self.loading_error,
            "platform": "HuggingFace Spaces" if self.hf_spaces else "Local",
            "supported_languages": ["en", "uk"],
            "optimizations": {
                "cpu_optimized": True,
                "llamacpp_backend": True,
                "gguf_format": True,
                "hf_spaces_mode": self.hf_spaces,
                "max_tokens": self.max_tokens,
                "context_length": self.context_length,
                "n_threads": self.n_threads,
                "timeout_enabled": True
            }
        }
        
        # –î–æ–±–∞–≤–ª—è–µ–º –¥–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
        try:
            from llama_cpp import Llama
            status["llamacpp_available"] = True
        except ImportError:
            status["llamacpp_available"] = False
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ç–∞—Ç—É—Å–∞
        recommendations = []
        if not self.model_loaded:
            if self.loading_error:
                recommendations.extend([
                    "LlamaCpp model loading failed",
                    "Check llama-cpp-python installation",
                    "Verify model file availability",
                    "Install: pip install llama-cpp-python"
                ])
            else:
                recommendations.extend([
                    "LlamaCpp model loading in progress",
                    "GGUF format provides stable CPU inference",
                    "First load may take 30-60 seconds",
                    "No hanging issues expected with llama.cpp"
                ])
        else:
            recommendations.extend([
                "LlamaCpp model ready for stable inference",
                "CPU-optimized with GGUF format",
                "No memory hanging issues",
                "Timeout protection enabled"
            ])
        
        status["recommendations"] = recommendations
        
        return status

def create_llm_service(model_name: str = "TheBloke/Llama-2-7B-Chat-GGUF"):
    """–°–æ–∑–¥–∞–µ—Ç LlamaCpp LLM —Å–µ—Ä–≤–∏—Å"""
    try:
        return LlamaCppLLMService(model_name)
    except Exception as e:
        logger.error(f"Failed to create LlamaCpp LLM service: {e}")
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º fallback
        from app.dependencies import HFSpacesImprovedLLMFallback
        return HFSpacesImprovedLLMFallback()