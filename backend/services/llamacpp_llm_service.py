# backend/services/llamacpp_llm_service.py - –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø –° –ê–í–¢–û–ó–ê–ì–†–£–ó–ö–û–ô –ú–û–î–ï–õ–ï–ô
"""
LLM Service using llama-cpp-python with automatic GGUF model download
–ò–°–ü–†–ê–í–õ–ï–ù–ò–Ø: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ GGUF –º–æ–¥–µ–ª–µ–π —Å HuggingFace Hub
"""

import logging
import time
import os
import gc
from typing import List, Dict, Optional
from dataclasses import dataclass
import asyncio
from pathlib import Path

logger = logging.getLogger(__name__)

@dataclass
class LLMResponse:
    content: str
    model: str
    tokens_used: int
    response_time: float
    success: bool
    error: Optional[str] = None

class LlamaCppLLMService:
    """LLM Service –Ω–∞ –æ—Å–Ω–æ–≤–µ llama-cpp-python —Å –∞–≤—Ç–æ–∑–∞–≥—Ä—É–∑–∫–æ–π –º–æ–¥–µ–ª–µ–π"""
    
    def __init__(self, model_name: str = "TheBloke/Llama-2-7B-Chat-GGUF"):
        self.model_name = model_name
        self.model_file = None  # –ë—É–¥–µ—Ç –æ–ø—Ä–µ–¥–µ–ª–µ–Ω –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏
        self.model = None
        self.service_type = "llamacpp_auto_download"
        self.model_loaded = False
        self.loading_started = False
        self.loading_error = None
        self.hf_spaces = os.getenv("SPACE_ID") is not None
        self.model_cache_dir = Path("./models")
        
        # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è HF Spaces
        self.max_tokens = 150 if self.hf_spaces else 300
        self.context_length = 2048 if self.hf_spaces else 4096
        self.n_threads = 2 if self.hf_spaces else 4
        
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –º–æ–¥–µ–ª–µ–π
        self.model_cache_dir.mkdir(exist_ok=True)
        
        logger.info(f"ü¶ô Initializing LlamaCpp service with auto-download: {model_name}")
        logger.info(f"üåç Platform: {'HuggingFace Spaces' if self.hf_spaces else 'Local'}")
        logger.info(f"üìÅ Model cache: {self.model_cache_dir}")
        
        # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –Ω–∞—á–∏–Ω–∞–µ–º –∑–∞–≥—Ä—É–∑–∫—É
        self._start_model_loading()
    
    def _start_model_loading(self):
        """–ë–µ–∑–æ–ø–∞—Å–Ω–æ –Ω–∞—á–∏–Ω–∞–µ—Ç –∑–∞–≥—Ä—É–∑–∫—É –º–æ–¥–µ–ª–∏"""
        if self.loading_started:
            return
        
        self.loading_started = True
        
        try:
            logger.info(f"üîÑ Starting LlamaCpp model loading with auto-download: {self.model_name}")
            self._load_model_with_download()
        except Exception as e:
            self.loading_error = str(e)
            logger.error(f"‚ùå LlamaCpp model loading initiation failed: {e}")
    
    def _load_model_with_download(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –∑–∞–≥—Ä—É–∑–∫–æ–π GGUF —Ñ–∞–π–ª–æ–≤"""
        start_time = time.time()
        
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å llama-cpp-python
            try:
                from llama_cpp import Llama
                logger.info("üìö llama-cpp-python available")
            except ImportError as e:
                error_msg = f"llama-cpp-python not installed: {e}"
                self.loading_error = error_msg
                logger.error(f"‚ùå {error_msg}")
                logger.error("Install: pip install llama-cpp-python")
                raise
            
            # –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏
            if self.hf_spaces:
                gc.collect()
                logger.info("üßπ Memory cleanup for HF Spaces")
            
            # –ù–û–í–û–ï: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
            model_file_path = self._download_gguf_model()
            
            if not model_file_path:
                raise Exception("Failed to download GGUF model")
            
            logger.info(f"üìÅ Model file ready: {model_file_path}")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
            self._load_model_from_file(model_file_path)
            
            loading_time = time.time() - start_time
            logger.info(f"‚úÖ LlamaCpp model loaded successfully in {loading_time:.1f}s!")
            
        except Exception as e:
            self.loading_error = str(e)
            logger.error(f"‚ùå LlamaCpp model loading failed: {e}")
            self._create_fallback_model()
    
    def _download_gguf_model(self) -> Optional[str]:
        """–°–∫–∞—á–∏–≤–∞–µ—Ç GGUF –º–æ–¥–µ–ª—å —Å HuggingFace Hub"""
        try:
            # –°–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π –¥–ª—è –ø–æ–ø—ã—Ç–∫–∏ (–æ—Ç –ª–µ–≥–∫–∏—Ö –∫ —Ç—è–∂–µ–ª—ã–º)
            model_candidates = [
                {
                    "repo": "TheBloke/Llama-2-7B-Chat-GGUF",
                    "files": [
                        "llama-2-7b-chat.Q4_K_S.gguf",  # 3.5GB - —Å–∞–º–∞—è –ª–µ–≥–∫–∞—è
                        "llama-2-7b-chat.Q4_K_M.gguf",  # 4.1GB - —Å—Ä–µ–¥–Ω—è—è
                        "llama-2-7b-chat.Q5_K_M.gguf",  # 4.8GB - –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–∞—è
                    ]
                },
                {
                    "repo": "microsoft/DialoGPT-medium",  # –ë–æ–ª–µ–µ –ª–µ–≥–∫–∞—è –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞
                    "files": ["pytorch_model.bin"]  # –ï—Å–ª–∏ GGUF –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω
                }
            ]
            
            for candidate in model_candidates:
                repo = candidate["repo"]
                files = candidate["files"]
                
                logger.info(f"üîç Trying to download from: {repo}")
                
                for model_file in files:
                    try:
                        model_path = self._download_single_file(repo, model_file)
                        if model_path:
                            self.model_file = model_file
                            logger.info(f"‚úÖ Successfully downloaded: {model_file}")
                            return model_path
                    except Exception as e:
                        logger.debug(f"Failed to download {model_file} from {repo}: {e}")
                        continue
            
            # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ —Å–∫–∞—á–∞–ª–æ—Å—å, –ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ñ–∞–π–ª—ã
            logger.info("üîç Looking for existing GGUF files...")
            return self._find_existing_gguf_file()
            
        except Exception as e:
            logger.error(f"‚ùå Error in model download process: {e}")
            return None
    
    def _download_single_file(self, repo: str, filename: str) -> Optional[str]:
        """–°–∫–∞—á–∏–≤–∞–µ—Ç –æ–¥–∏–Ω —Ñ–∞–π–ª –º–æ–¥–µ–ª–∏"""
        try:
            # –ü–æ–ø—Ä–æ–±—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å huggingface_hub –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
            try:
                from huggingface_hub import hf_hub_download
                
                logger.info(f"üì• Downloading {filename} from {repo}...")
                
                file_path = hf_hub_download(
                    repo_id=repo,
                    filename=filename,
                    cache_dir=str(self.model_cache_dir),
                    local_files_only=False,
                    force_download=False  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à –µ—Å–ª–∏ –µ—Å—Ç—å
                )
                
                if os.path.exists(file_path):
                    logger.info(f"‚úÖ Downloaded: {filename} ({self._get_file_size(file_path)})")
                    return file_path
                
            except ImportError:
                logger.warning("huggingface_hub not available, trying wget/curl...")
                
                # Fallback: –ø—Ä—è–º–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ —á–µ—Ä–µ–∑ HTTP
                return self._download_via_http(repo, filename)
            
        except Exception as e:
            logger.debug(f"Failed to download {filename}: {e}")
            return None
    
    def _download_via_http(self, repo: str, filename: str) -> Optional[str]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–∞ —á–µ—Ä–µ–∑ HTTP (fallback)"""
        try:
            import urllib.request
            
            # HuggingFace URL format
            url = f"https://huggingface.co/{repo}/resolve/main/{filename}"
            local_path = self.model_cache_dir / filename
            
            logger.info(f"üì• HTTP download: {filename}")
            
            def show_progress(block_num, block_size, total_size):
                if total_size > 0:
                    percent = min(100, (block_num * block_size / total_size) * 100)
                    if percent % 10 == 0:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–µ 10%
                        logger.info(f"Download progress: {percent:.0f}%")
            
            urllib.request.urlretrieve(url, local_path, show_progress)
            
            if local_path.exists():
                logger.info(f"‚úÖ HTTP download complete: {filename}")
                return str(local_path)
            
        except Exception as e:
            logger.debug(f"HTTP download failed for {filename}: {e}")
            
        return None
    
    def _find_existing_gguf_file(self) -> Optional[str]:
        """–ò—â–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ GGUF —Ñ–∞–π–ª—ã –≤ –∫—ç—à–µ"""
        try:
            # –ò—â–µ–º –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –º–æ–¥–µ–ª–µ–π
            for gguf_file in self.model_cache_dir.glob("*.gguf"):
                if gguf_file.is_file() and gguf_file.stat().st_size > 1024 * 1024:  # –ë–æ–ª—å—à–µ 1MB
                    logger.info(f"üìÅ Found existing GGUF file: {gguf_file.name}")
                    self.model_file = gguf_file.name
                    return str(gguf_file)
            
            # –ò—â–µ–º –≤ HuggingFace cache
            try:
                import os
                hf_cache = os.path.expanduser("~/.cache/huggingface")
                if os.path.exists(hf_cache):
                    for root, dirs, files in os.walk(hf_cache):
                        for file in files:
                            if file.endswith('.gguf') and os.path.getsize(os.path.join(root, file)) > 1024 * 1024:
                                logger.info(f"üìÅ Found cached GGUF: {file}")
                                return os.path.join(root, file)
            except:
                pass
            
            logger.warning("‚ùå No existing GGUF files found")
            return None
            
        except Exception as e:
            logger.error(f"Error finding existing files: {e}")
            return None
    
    def _load_model_from_file(self, model_path: str):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å –∏–∑ —Ñ–∞–π–ª–∞"""
        try:
            from llama_cpp import Llama
            
            model_kwargs = {
                "model_path": model_path,
                "n_ctx": self.context_length,
                "n_threads": self.n_threads,
                "n_gpu_layers": 0,  # –¢–æ–ª—å–∫–æ CPU
                "use_mmap": True,
                "use_mlock": False,
                "verbose": False,
                "n_batch": 128 if self.hf_spaces else 512,
            }
            
            logger.info(f"üîÑ Loading model from: {os.path.basename(model_path)}")
            logger.info(f"üîß Settings: ctx={self.context_length}, threads={self.n_threads}")
            
            self.model = Llama(**model_kwargs)
            self.model_loaded = True
            
            # –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏ –ø–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏
            if self.hf_spaces:
                gc.collect()
                logger.info("üßπ Post-loading memory cleanup")
            
            logger.info(f"‚úÖ Model loaded successfully: {os.path.basename(model_path)}")
            
        except Exception as e:
            logger.error(f"‚ùå Failed to load model from {model_path}: {e}")
            raise
    
    def _get_file_size(self, file_path: str) -> str:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞ –≤ —á–∏—Ç–∞–µ–º–æ–º —Ñ–æ—Ä–º–∞—Ç–µ"""
        try:
            size = os.path.getsize(file_path)
            for unit in ['B', 'KB', 'MB', 'GB']:
                if size < 1024:
                    return f"{size:.1f} {unit}"
                size /= 1024
            return f"{size:.1f} TB"
        except:
            return "unknown size"
    
    def _create_fallback_model(self):
        """–°–æ–∑–¥–∞–µ—Ç fallback –º–æ–¥–µ–ª—å –µ—Å–ª–∏ –æ—Å–Ω–æ–≤–Ω–∞—è –Ω–µ –∑–∞–≥—Ä—É–∑–∏–ª–∞—Å—å"""
        logger.info("üîÑ Creating enhanced fallback model...")
        
        class EnhancedFallbackModel:
            def __call__(self, prompt: str, max_tokens: int = 100, **kwargs):
                # –ë–æ–ª–µ–µ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π fallback
                if "question" in prompt.lower() or "–ø–∏—Ç–∞–Ω–Ω—è" in prompt.lower():
                    response = self._generate_fallback_response(prompt)
                else:
                    response = f"LlamaCpp model is loading. Your request: {prompt[:100]}..."
                
                return {
                    "choices": [{
                        "text": response
                    }]
                }
            
            def _generate_fallback_response(self, prompt: str) -> str:
                """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –±–æ–ª–µ–µ —É–º–Ω—ã–π fallback –æ—Ç–≤–µ—Ç"""
                if "ukraine" in prompt.lower() or "—É–∫—Ä–∞—ó–Ω" in prompt.lower():
                    return """üá∫üá¶ **–Æ—Ä–∏–¥–∏—á–Ω–∞ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è –¥–ª—è –£–∫—Ä–∞—ó–Ω–∏**
                    
LlamaCpp –º–æ–¥–µ–ª—å –∑–∞–≤–∞–Ω—Ç–∞–∂—É—î—Ç—å—Å—è. –î–ª—è —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ–≥–æ –ø—Ä–∞–≤–∞ —Ä–µ–∫–æ–º–µ–Ω–¥—É—é –∑–≤–µ—Ä–Ω—É—Ç–∏—Å—è –¥–æ:
‚Ä¢ zakon.rada.gov.ua - –æ—Ñ—ñ—Ü—ñ–π–Ω–∏–π –ø–æ—Ä—Ç–∞–ª –∑–∞–∫–æ–Ω–æ–¥–∞–≤—Å—Ç–≤–∞
‚Ä¢ court.gov.ua - —Å—É–¥–æ–≤–∞ —Å–∏—Å—Ç–µ–º–∞ –£–∫—Ä–∞—ó–Ω–∏
‚Ä¢ minjust.gov.ua - –ú—ñ–Ω—ñ—Å—Ç–µ—Ä—Å—Ç–≤–æ —é—Å—Ç–∏—Ü—ñ—ó

–ú–æ–¥–µ–ª—å –±—É–¥–µ –¥–æ—Å—Ç—É–ø–Ω–∞ —á–µ—Ä–µ–∑ –∫—ñ–ª—å–∫–∞ —Ö–≤–∏–ª–∏–Ω –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–∏—Ö –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü—ñ–π."""

                elif "ireland" in prompt.lower() or "irish" in prompt.lower():
                    return """üáÆüá™ **Legal Information for Ireland**
                    
LlamaCpp model is loading. For Irish law, I recommend:
‚Ä¢ irishstatutebook.ie - official legislation
‚Ä¢ courts.ie - court system information  
‚Ä¢ citizensinformation.ie - citizen services

The model will be available in a few minutes for detailed consultations."""

                else:
                    return """‚öñÔ∏è **Legal Assistant Loading**
                    
The LlamaCpp model is currently initializing. This provides:
‚Ä¢ Stable CPU inference without hanging
‚Ä¢ Support for English and Ukrainian
‚Ä¢ Legal document analysis capabilities

Please try your question again in a few minutes."""
        
        self.model = EnhancedFallbackModel()
        self.model_loaded = True
        self.service_type = "llamacpp_enhanced_fallback"
        logger.info("‚úÖ Enhanced fallback model ready")
    
    async def answer_legal_question(self, question: str, context_documents: List[Dict], language: str = "en"):
        """–û—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–π –≤–æ–ø—Ä–æ—Å —á–µ—Ä–µ–∑ LlamaCpp"""
        start_time = time.time()
        
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∞—Ç—É—Å –º–æ–¥–µ–ª–∏
            if not self.model_loaded:
                if self.loading_error:
                    return LLMResponse(
                        content=self._generate_loading_error_response(question, language),
                        model=self.model_name,
                        tokens_used=0,
                        response_time=time.time() - start_time,
                        success=False,
                        error=f"Model loading failed: {self.loading_error}"
                    )
                else:
                    return LLMResponse(
                        content=self._generate_loading_response(question, language),
                        model=self.model_name,
                        tokens_used=0,
                        response_time=time.time() - start_time,
                        success=True,
                        error="Model still loading"
                    )
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç
            prompt = self._build_chat_prompt(question, context_documents, language)
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç —á–µ—Ä–µ–∑ LlamaCpp
            response_text = await self._generate_response_llamacpp(prompt)
            
            response_time = time.time() - start_time
            
            return LLMResponse(
                content=response_text,
                model=f"{self.model_name}/{self.model_file or 'fallback'}",
                tokens_used=len(response_text.split()),
                response_time=response_time,
                success=True
            )
            
        except Exception as e:
            logger.error(f"‚ùå Error generating response: {e}")
            return LLMResponse(
                content=self._generate_error_response(question, language, str(e)),
                model=self.model_name,
                tokens_used=0,
                response_time=time.time() - start_time,
                success=False,
                error=str(e)
            )
    
    def _build_chat_prompt(self, question: str, context_documents: List[Dict], language: str) -> str:
        """–°—Ç—Ä–æ–∏—Ç –ø—Ä–æ–º–ø—Ç –¥–ª—è Llama-2-Chat —Ñ–æ—Ä–º–∞—Ç–∞"""
        
        # –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç
        if language == "uk":
            system_message = "–í–∏ - –¥–æ—Å–≤—ñ–¥—á–µ–Ω–∏–π —é—Ä–∏–¥–∏—á–Ω–∏–π –∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç. –ù–∞–¥–∞–≤–∞–π—Ç–µ —Ç–æ—á–Ω—ñ —Ç–∞ –∫–æ—Ä–∏—Å–Ω—ñ –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ."
        else:
            system_message = "You are an experienced legal consultant. Provide accurate and helpful answers."
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
        context_parts = []
        max_docs = 2 if self.hf_spaces else 3
        max_doc_length = 400 if self.hf_spaces else 600
        
        for i, doc in enumerate(context_documents[:max_docs]):
            filename = doc.get('filename', f'Document {i+1}')
            content = doc.get('content', '')[:max_doc_length]
            if content.strip():
                context_parts.append(f"{filename}: {content.strip()}")
        
        context = "\n\n".join(context_parts) if context_parts else ""
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º user message
        if context:
            if language == "uk":
                user_message = f"–ö–æ–Ω—Ç–µ–∫—Å—Ç:\n{context}\n\n–ü–∏—Ç–∞–Ω–Ω—è: {question}\n\n–í—ñ–¥–ø–æ–≤—ñ–¥—å:"
            else:
                user_message = f"Context:\n{context}\n\nQuestion: {question}\n\nAnswer:"
        else:
            if language == "uk":
                user_message = f"–ü–∏—Ç–∞–Ω–Ω—è: {question}\n\n–í—ñ–¥–ø–æ–≤—ñ–¥—å:"
            else:
                user_message = f"Question: {question}\n\nAnswer:"
        
        # Llama-2-Chat —Ñ–æ—Ä–º–∞—Ç
        prompt = f"""<s>[INST] <<SYS>>
{system_message}
<</SYS>>

{user_message} [/INST]"""
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É
        max_prompt_length = 1500 if self.hf_spaces else 2000
        if len(prompt) > max_prompt_length:
            prompt = prompt[:max_prompt_length] + "..."
        
        return prompt
    
    async def _generate_response_llamacpp(self, prompt: str) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç —á–µ—Ä–µ–∑ LlamaCpp —Å —Ç–∞–π–º–∞—É—Ç–æ–º"""
        try:
            generation_kwargs = {
                "max_tokens": self.max_tokens,
                "temperature": 0.1,
                "top_p": 0.9,
                "top_k": 40,
                "repeat_penalty": 1.1,
                "stop": ["</s>", "[/INST]", "<s>"],
                "echo": False,
            }
            
            logger.info(f"üîß Generating with max_tokens={self.max_tokens}")
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å —Ç–∞–π–º–∞—É—Ç–æ–º
            def _generate_sync():
                return self.model(prompt, **generation_kwargs)
            
            import concurrent.futures
            
            with concurrent.futures.ThreadPoolExecutor() as executor:
                future = executor.submit(_generate_sync)
                try:
                    timeout = 30 if self.hf_spaces else 60
                    result = future.result(timeout=timeout)
                except concurrent.futures.TimeoutError:
                    logger.error(f"‚ùå Generation timeout after {timeout}s")
                    return "Response generation timed out. Please try with a shorter question."
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –æ—Ç–≤–µ—Ç–∞
            if isinstance(result, dict) and "choices" in result and result["choices"]:
                response_text = result["choices"][0].get("text", "").strip()
            else:
                response_text = str(result).strip()
            
            # –û—á–∏—â–∞–µ–º –æ—Ç–≤–µ—Ç
            response_text = self._clean_response(response_text)
            
            # –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏
            if self.hf_spaces:
                gc.collect()
            
            return response_text if response_text else "I need more specific information to provide a proper answer."
            
        except Exception as e:
            logger.error(f"‚ùå Generation error: {e}")
            return f"Error generating response. Please try again."
    
    def _clean_response(self, response: str) -> str:
        """–û—á–∏—â–∞–µ—Ç –æ—Ç–≤–µ—Ç –æ—Ç –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤"""
        if not response:
            return response
        
        # –£–¥–∞–ª—è–µ–º —Å—Ç–æ–ø-—Ç–æ–∫–µ–Ω—ã
        stop_tokens = ["</s>", "[/INST]", "<s>", "[INST]"]
        for token in stop_tokens:
            response = response.replace(token, "")
        
        # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
        lines = []
        for line in response.split('\n'):
            line = line.strip()
            if line and not line.startswith(('System:', 'User:', 'Assistant:')):
                lines.append(line)
        
        cleaned = '\n'.join(lines[:10])
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É
        max_length = 1000 if self.hf_spaces else 1500
        if len(cleaned) > max_length:
            cleaned = cleaned[:max_length] + "..."
        
        return cleaned
    
    def _generate_loading_response(self, question: str, language: str) -> str:
        """–û—Ç–≤–µ—Ç –ø–æ–∫–∞ –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è"""
        if language == "uk":
            return f"""ü¶ô **LlamaCpp –º–æ–¥–µ–ª—å –∑–∞–≤–∞–Ω—Ç–∞–∂—É—î—Ç—å—Å—è –∑ –∞–≤—Ç–æ—Å–∫–∞—á—É–≤–∞–Ω–Ω—è–º...**

**–í–∞—à–µ –ø–∏—Ç–∞–Ω–Ω—è:** {question}

‚è≥ –ú–æ–¥–µ–ª—å `{self.model_name}` –∑–∞–≤–∞–Ω—Ç–∞–∂—É—î—Ç—å—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ:
üì• –°–∫–∞—á—É–≤–∞–Ω–Ω—è GGUF —Ñ–∞–π–ª—É –∑ HuggingFace Hub
üîß –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è llama.cpp
‚ö° –û–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è –¥–ª—è CPU

üìÅ **–§–∞–π–ª –º–æ–¥–µ–ª—ñ:** {self.model_file or '–≤–∏–∑–Ω–∞—á–∞—î—Ç—å—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ'}
üéØ **–Ø–∫—ñ—Å—Ç—å:** –°—Ç–∞–±—ñ–ª—å–Ω–∞ —Ä–æ–±–æ—Ç–∞ –Ω–∞ CPU
üåç **–ú–æ–≤–∏:** –ê–Ω–≥–ª—ñ–π—Å—å–∫–∞ —Ç–∞ —É–∫—Ä–∞—ó–Ω—Å—å–∫–∞

üí° **–°—Ç–∞—Ç—É—Å:** –ü–µ—Ä—à–µ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–∂–µ –∑–∞–π–Ω—è—Ç–∏ 2-5 —Ö–≤–∏–ª–∏–Ω –∑–∞–ª–µ–∂–Ω–æ –≤—ñ–¥ —à–≤–∏–¥–∫–æ—Å—Ç—ñ —ñ–Ω—Ç–µ—Ä–Ω–µ—Ç—É."""
        else:
            return f"""ü¶ô **LlamaCpp Model Loading with Auto-Download...**

**Your Question:** {question}

‚è≥ Model `{self.model_name}` is loading automatically:
üì• Downloading GGUF file from HuggingFace Hub
üîß Initializing llama.cpp
‚ö° Optimizing for CPU

üìÅ **Model File:** {self.model_file or 'auto-detecting'}
üéØ **Quality:** Stable CPU performance
üåç **Languages:** English and Ukrainian

üí° **Status:** First download may take 2-5 minutes depending on internet speed."""
    
    def _generate_loading_error_response(self, question: str, language: str) -> str:
        """–û—Ç–≤–µ—Ç –ø—Ä–∏ –æ—à–∏–±–∫–µ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏"""
        if language == "uk":
            return f"""‚ùå **–ü–æ–º–∏–ª–∫–∞ –∞–≤—Ç–æ–∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è LlamaCpp –º–æ–¥–µ–ª—ñ**

**–í–∞—à–µ –ø–∏—Ç–∞–Ω–Ω—è:** {question}

üö´ **–ü—Ä–æ–±–ª–µ–º–∞:** –ù–µ –≤–¥–∞–ª–æ—Å—è –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –º–æ–¥–µ–ª—å `{self.model_name}`
üìã **–ü–æ–º–∏–ª–∫–∞:** {self.loading_error}

üîß **–ú–æ–∂–ª–∏–≤—ñ –ø—Ä–∏—á–∏–Ω–∏:**
‚Ä¢ –í—ñ–¥—Å—É—Ç–Ω—î –ø—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è –¥–æ —ñ–Ω—Ç–µ—Ä–Ω–µ—Ç—É
‚Ä¢ HuggingFace Hub –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∏–π
‚Ä¢ –ù–µ–¥–æ—Å—Ç–∞—Ç–Ω—å–æ –º—ñ—Å—Ü—è –Ω–∞ –¥–∏—Å–∫—É
‚Ä¢ –í—ñ–¥—Å—É—Ç–Ω—è –±—ñ–±–ª—ñ–æ—Ç–µ–∫–∞ huggingface-hub

üí° **–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó:**
‚Ä¢ –í—Å—Ç–∞–Ω–æ–≤—ñ—Ç—å: pip install huggingface-hub
‚Ä¢ –ü–µ—Ä–µ–≤—ñ—Ä—Ç–µ –ø—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è –¥–æ —ñ–Ω—Ç–µ—Ä–Ω–µ—Ç—É
‚Ä¢ –û—á–∏—Å—Ç—ñ—Ç—å –∫—ç—à: rm -rf ~/.cache/huggingface
‚Ä¢ –°–ø—Ä–æ–±—É–π—Ç–µ —â–µ —Ä–∞–∑ —á–µ—Ä–µ–∑ –∫—ñ–ª—å–∫–∞ —Ö–≤–∏–ª–∏–Ω"""
        else:
            return f"""‚ùå **LlamaCpp Auto-Download Error**

**Your Question:** {question}

üö´ **Issue:** Failed to download model `{self.model_name}`
üìã **Error:** {self.loading_error}

üîß **Possible Causes:**
‚Ä¢ No internet connection
‚Ä¢ HuggingFace Hub unavailable
‚Ä¢ Insufficient disk space
‚Ä¢ Missing huggingface-hub library

üí° **Recommendations:**
‚Ä¢ Install: pip install huggingface-hub
‚Ä¢ Check internet connectivity
‚Ä¢ Clear cache: rm -rf ~/.cache/huggingface
‚Ä¢ Try again in a few minutes"""
    
    def _generate_error_response(self, question: str, language: str, error: str) -> str:
        """–û—Ç–≤–µ—Ç –ø—Ä–∏ –æ—à–∏–±–∫–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏"""
        if language == "uk":
            return f"""‚ö†Ô∏è **–¢–µ—Ö–Ω—ñ—á–Ω–∞ –ø—Ä–æ–±–ª–µ–º–∞**

–ü–æ–º–∏–ª–∫–∞ –æ–±—Ä–æ–±–∫–∏: "{question}"
üîß –î–µ—Ç–∞–ª—ñ: {error}

–°–ø—Ä–æ–±—É–π—Ç–µ —â–µ —Ä–∞–∑ –∞–±–æ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª—é–π—Ç–µ –ø–∏—Ç–∞–Ω–Ω—è."""
        else:
            return f"""‚ö†Ô∏è **Technical Issue**

Error processing: "{question}"
üîß Details: {error}

Please try again or rephrase your question."""
    
    async def get_service_status(self):
        """–î–µ—Ç–∞–ª—å–Ω–∏–π —Å—Ç–∞—Ç—É—Å —Å–µ—Ä–≤–∏—Å–∞"""
        status = {
            "service_type": "llamacpp_auto_download",
            "model_name": self.model_name,
            "model_file": self.model_file,
            "model_loaded": self.model_loaded,
            "loading_started": self.loading_started,
            "loading_error": self.loading_error,
            "platform": "HuggingFace Spaces" if self.hf_spaces else "Local",
            "supported_languages": ["en", "uk"],
            "model_cache_dir": str(self.model_cache_dir),
            "auto_download": True,
            "optimizations": {
                "cpu_optimized": True,
                "llamacpp_backend": True,
                "gguf_format": True,
                "auto_download_enabled": True,
                "hf_spaces_mode": self.hf_spaces,
                "max_tokens": self.max_tokens,
                "context_length": self.context_length,
                "n_threads": self.n_threads,
                "timeout_enabled": True
            }
        }
        
        # –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞
        try:
            from llama_cpp import Llama
            status["llamacpp_available"] = True
        except ImportError:
            status["llamacpp_available"] = False
        
        try:
            from huggingface_hub import hf_hub_download
            status["huggingface_hub_available"] = True
        except ImportError:
            status["huggingface_hub_available"] = False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ñ–∞–π–ª—ã
        existing_models = list(self.model_cache_dir.glob("*.gguf"))
        status["cached_models"] = [m.name for m in existing_models]
        status["cache_size"] = sum(m.stat().st_size for m in existing_models) if existing_models else 0
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        recommendations = []
        if not self.model_loaded:
            if self.loading_error:
                recommendations.extend([
                    "Model auto-download failed",
                    "Check internet connection and HuggingFace Hub access",
                    "Install: pip install huggingface-hub",
                    "Ensure sufficient disk space for GGUF files (~4GB)"
                ])
            else:
                recommendations.extend([
                    "Model downloading automatically from HuggingFace Hub",
                    "First download may take 2-5 minutes",
                    "GGUF files are cached for future use",
                    "Stable CPU inference without hanging issues"
                ])
        else:
            recommendations.extend([
                "LlamaCpp model ready with auto-download capabilities",
                "GGUF model cached locally for faster future loading",
                "CPU-optimized inference with timeout protection",
                "Enhanced fallback responses during loading"
            ])
        
        status["recommendations"] = recommendations
        
        return status

def create_llm_service(model_name: str = "TheBloke/Llama-2-7B-Chat-GGUF"):
    """–°–æ–∑–¥–∞–µ—Ç LlamaCpp LLM —Å–µ—Ä–≤–∏—Å —Å –∞–≤—Ç–æ–∑–∞–≥—Ä—É–∑–∫–æ–π"""
    try:
        return LlamaCppLLMService(model_name)
    except Exception as e:
        logger.error(f"Failed to create LlamaCpp LLM service: {e}")
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º fallback
        from app.dependencies import HFSpacesLlamaCppFallback
        return HFSpacesLlamaCppFallback()