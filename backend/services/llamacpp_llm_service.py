# backend/services/llamacpp_llm_service.py - –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–ê–Ø –í–ï–†–°–ò–Ø –° –ü–†–ê–í–ò–õ–¨–ù–´–ú–ò –¢–ê–ô–ú–ê–£–¢–ê–ú–ò
"""
LLM Service using llama-cpp-python with optimized timeouts and generation settings
–ò–°–ü–†–ê–í–õ–ï–ù–ò–Ø: –£–≤–µ–ª–∏—á–µ–Ω–Ω—ã–µ —Ç–∞–π–º–∞—É—Ç—ã, –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è, –ª—É—á—à–∏–µ –ø—Ä–æ–º–ø—Ç—ã
"""

import logging
import time
import os
import gc
from typing import List, Dict, Optional
from dataclasses import dataclass
import asyncio
from pathlib import Path

logger = logging.getLogger(__name__)

@dataclass
class LLMResponse:
    content: str
    model: str
    tokens_used: int
    response_time: float
    success: bool
    error: Optional[str] = None

class LlamaCppLLMService:
    """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π LLM Service –Ω–∞ –æ—Å–Ω–æ–≤–µ llama-cpp-python"""
    
    def __init__(self, model_name: str = "TheBloke/Llama-2-7B-Chat-GGUF"):
        self.model_name = model_name
        self.model_file = None
        self.model = None
        self.service_type = "llamacpp_optimized_v2"
        self.model_loaded = False
        self.loading_started = False
        self.loading_error = None
        self.hf_spaces = os.getenv("SPACE_ID") is not None
        self.model_cache_dir = Path("./models")
        
        # –ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ï –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π —Ä–∞–±–æ—Ç—ã
        self.max_tokens = 100 if self.hf_spaces else 200  # –£–º–µ–Ω—å—à–µ–Ω–æ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
        self.context_length = 1024 if self.hf_spaces else 2048  # –£–º–µ–Ω—å—à–µ–Ω–æ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
        self.n_threads = 2 if self.hf_spaces else 4
        
        # –ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ï —Ç–∞–π–º–∞—É—Ç—ã
        self.generation_timeout = 90 if self.hf_spaces else 120  # –£–≤–µ–ª–∏—á–µ–Ω–æ —Å 30 –¥–æ 90 —Å–µ–∫—É–Ω–¥
        self.quick_timeout = 60  # –î–ª—è –±—ã—Å—Ç—Ä—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤
        
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –º–æ–¥–µ–ª–µ–π
        self.model_cache_dir.mkdir(exist_ok=True)
        
        logger.info(f"ü¶ô Initializing OPTIMIZED LlamaCpp service: {model_name}")
        logger.info(f"üåç Platform: {'HuggingFace Spaces' if self.hf_spaces else 'Local'}")
        logger.info(f"‚è∞ Generation timeout: {self.generation_timeout}s")
        logger.info(f"üîß Max tokens: {self.max_tokens}, Context: {self.context_length}")
        
        # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –Ω–∞—á–∏–Ω–∞–µ–º –∑–∞–≥—Ä—É–∑–∫—É
        self._start_model_loading()
    
    def _start_model_loading(self):
        """–ë–µ–∑–æ–ø–∞—Å–Ω–æ –Ω–∞—á–∏–Ω–∞–µ—Ç –∑–∞–≥—Ä—É–∑–∫—É –º–æ–¥–µ–ª–∏"""
        if self.loading_started:
            return
        
        self.loading_started = True
        
        try:
            logger.info(f"üîÑ Starting OPTIMIZED LlamaCpp model loading: {self.model_name}")
            self._load_model_with_download()
        except Exception as e:
            self.loading_error = str(e)
            logger.error(f"‚ùå LlamaCpp model loading initiation failed: {e}")
    
    def _load_model_with_download(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –∑–∞–≥—Ä—É–∑–∫–æ–π GGUF —Ñ–∞–π–ª–æ–≤"""
        start_time = time.time()
        
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å llama-cpp-python
            try:
                from llama_cpp import Llama
                logger.info("üìö llama-cpp-python available")
            except ImportError as e:
                error_msg = f"llama-cpp-python not installed: {e}"
                self.loading_error = error_msg
                logger.error(f"‚ùå {error_msg}")
                raise
            
            # –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏
            if self.hf_spaces:
                gc.collect()
                logger.info("üßπ Memory cleanup for HF Spaces")
            
            # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
            model_file_path = self._download_gguf_model()
            
            if not model_file_path:
                raise Exception("Failed to download GGUF model")
            
            logger.info(f"üìÅ Model file ready: {model_file_path}")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏
            self._load_model_from_file(model_file_path)
            
            loading_time = time.time() - start_time
            logger.info(f"‚úÖ OPTIMIZED LlamaCpp model loaded successfully in {loading_time:.1f}s!")
            
        except Exception as e:
            self.loading_error = str(e)
            logger.error(f"‚ùå LlamaCpp model loading failed: {e}")
            self._create_fallback_model()
    
    def _download_gguf_model(self) -> Optional[str]:
        """–°–∫–∞—á–∏–≤–∞–µ—Ç GGUF –º–æ–¥–µ–ª—å —Å HuggingFace Hub (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)"""
        try:
            # –°–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π –¥–ª—è –ø–æ–ø—ã—Ç–∫–∏ (—Å–∞–º—ã–µ –±—ã—Å—Ç—Ä—ã–µ –ø–µ—Ä–≤—ã–µ)
            model_candidates = [
                {
                    "repo": "TheBloke/Llama-2-7B-Chat-GGUF",
                    "files": [
                        "llama-2-7b-chat.Q4_K_S.gguf",  # 3.6GB - –ë–´–°–¢–†–ê–Ø
                        "llama-2-7b-chat.Q4_K_M.gguf",  # 4.1GB - —Å—Ä–µ–¥–Ω—è—è
                    ]
                }
            ]
            
            for candidate in model_candidates:
                repo = candidate["repo"]
                files = candidate["files"]
                
                logger.info(f"üîç Trying to download from: {repo}")
                
                for model_file in files:
                    try:
                        model_path = self._download_single_file(repo, model_file)
                        if model_path:
                            self.model_file = model_file
                            logger.info(f"‚úÖ Successfully downloaded: {model_file}")
                            return model_path
                    except Exception as e:
                        logger.debug(f"Failed to download {model_file} from {repo}: {e}")
                        continue
            
            # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ —Å–∫–∞—á–∞–ª–æ—Å—å, –ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ñ–∞–π–ª—ã
            logger.info("üîç Looking for existing GGUF files...")
            return self._find_existing_gguf_file()
            
        except Exception as e:
            logger.error(f"‚ùå Error in model download process: {e}")
            return None
    
    def _download_single_file(self, repo: str, filename: str) -> Optional[str]:
        """–°–∫–∞—á–∏–≤–∞–µ—Ç –æ–¥–∏–Ω —Ñ–∞–π–ª –º–æ–¥–µ–ª–∏"""
        try:
            # –ü—Ä–æ–±—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å huggingface_hub
            try:
                from huggingface_hub import hf_hub_download
                
                logger.info(f"üì• Downloading {filename} from {repo}...")
                
                file_path = hf_hub_download(
                    repo_id=repo,
                    filename=filename,
                    cache_dir=str(self.model_cache_dir),
                    local_files_only=False,
                    force_download=False
                )
                
                if os.path.exists(file_path):
                    size_mb = round(os.path.getsize(file_path) / 1024 / 1024, 1)
                    logger.info(f"‚úÖ Downloaded: {filename} ({size_mb} MB)")
                    return file_path
                
            except ImportError:
                logger.warning("huggingface_hub not available, trying wget/curl...")
                return self._download_via_http(repo, filename)
            
        except Exception as e:
            logger.debug(f"Failed to download {filename}: {e}")
            return None
    
    def _download_via_http(self, repo: str, filename: str) -> Optional[str]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–∞ —á–µ—Ä–µ–∑ HTTP (fallback)"""
        try:
            import urllib.request
            
            url = f"https://huggingface.co/{repo}/resolve/main/{filename}"
            local_path = self.model_cache_dir / filename
            
            logger.info(f"üì• HTTP download: {filename}")
            
            def show_progress(block_num, block_size, total_size):
                if total_size > 0:
                    percent = min(100, (block_num * block_size / total_size) * 100)
                    if percent % 10 == 0:
                        logger.info(f"Download progress: {percent:.0f}%")
            
            urllib.request.urlretrieve(url, local_path, show_progress)
            
            if local_path.exists():
                logger.info(f"‚úÖ HTTP download complete: {filename}")
                return str(local_path)
            
        except Exception as e:
            logger.debug(f"HTTP download failed for {filename}: {e}")
        
        return None
    
    def _find_existing_gguf_file(self) -> Optional[str]:
        """–ò—â–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ GGUF —Ñ–∞–π–ª—ã –≤ –∫—ç—à–µ"""
        try:
            # –ò—â–µ–º –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –º–æ–¥–µ–ª–µ–π
            for gguf_file in self.model_cache_dir.glob("*.gguf"):
                if gguf_file.is_file() and gguf_file.stat().st_size > 1024 * 1024:
                    logger.info(f"üìÅ Found existing GGUF file: {gguf_file.name}")
                    self.model_file = gguf_file.name
                    return str(gguf_file)
            
            # –ò—â–µ–º –≤ HuggingFace cache —Ä–µ–∫—É—Ä—Å–∏–≤–Ω–æ
            try:
                import os
                hf_cache = os.path.expanduser("~/.cache/huggingface")
                if os.path.exists(hf_cache):
                    for root, dirs, files in os.walk(hf_cache):
                        for file in files:
                            if file.endswith('.gguf') and os.path.getsize(os.path.join(root, file)) > 1024 * 1024:
                                full_path = os.path.join(root, file)
                                logger.info(f"üìÅ Found cached GGUF: {file}")
                                self.model_file = file
                                return full_path
            except:
                pass
            
            logger.warning("‚ùå No existing GGUF files found")
            return None
            
        except Exception as e:
            logger.error(f"Error finding existing files: {e}")
            return None
    
    def _load_model_from_file(self, model_path: str):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å –∏–∑ —Ñ–∞–π–ª–∞ —Å –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–´–ú–ò –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏"""
        try:
            from llama_cpp import Llama
            
            # –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–´–ï –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            model_kwargs = {
                "model_path": model_path,
                "n_ctx": self.context_length,  # –£–º–µ–Ω—å—à–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
                "n_threads": self.n_threads,
                "n_gpu_layers": 0,  # –¢–æ–ª—å–∫–æ CPU
                "use_mmap": True,   # Memory mapping –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
                "use_mlock": False, # –ù–µ –±–ª–æ–∫–∏—Ä—É–µ–º –ø–∞–º—è—Ç—å
                "verbose": False,
                "n_batch": 64 if self.hf_spaces else 128,  # –£–º–µ–Ω—å—à–µ–Ω–Ω—ã–π batch –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
                "rope_scaling_type": None,  # –û—Ç–∫–ª—é—á–∞–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ñ–∏—á–∏
                "rope_freq_base": 0.0,      # –û—Ç–∫–ª—é—á–∞–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ñ–∏—á–∏
                "rope_freq_scale": 0.0,     # –û—Ç–∫–ª—é—á–∞–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ñ–∏—á–∏
            }
            
            logger.info(f"üîÑ Loading OPTIMIZED model from: {os.path.basename(model_path)}")
            logger.info(f"üîß OPTIMIZED settings: ctx={self.context_length}, threads={self.n_threads}, batch={model_kwargs['n_batch']}")
            
            self.model = Llama(**model_kwargs)
            self.model_loaded = True
            
            # –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏ –ø–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏
            if self.hf_spaces:
                gc.collect()
                logger.info("üßπ Post-loading memory cleanup")
            
            logger.info(f"‚úÖ OPTIMIZED model loaded successfully: {os.path.basename(model_path)}")
            
        except Exception as e:
            logger.error(f"‚ùå Failed to load optimized model from {model_path}: {e}")
            raise
    
    def _create_fallback_model(self):
        """–°–æ–∑–¥–∞–µ—Ç —É–ª—É—á—à–µ–Ω–Ω—ã–π fallback –º–æ–¥–µ–ª—å"""
        logger.info("üîÑ Creating enhanced fallback model...")
        
        class EnhancedFallbackModel:
            def __call__(self, prompt: str, max_tokens: int = 100, **kwargs):
                response = self._generate_smart_fallback(prompt)
                return {
                    "choices": [{
                        "text": response
                    }]
                }
            
            def _generate_smart_fallback(self, prompt: str) -> str:
                """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —É–º–Ω—ã–π fallback –æ—Ç–≤–µ—Ç"""
                # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–∏–ø –≤–æ–ø—Ä–æ—Å–∞
                prompt_lower = prompt.lower()
                
                if any(word in prompt_lower for word in ["law", "legal", "–ø—Ä–∞–≤–æ", "–∑–∞–∫–æ–Ω"]):
                    if "ukraine" in prompt_lower or "—É–∫—Ä–∞—ó–Ω" in prompt_lower:
                        return """‚öñÔ∏è **–£–∫—Ä–∞—ó–Ω—Å—å–∫–µ –ø—Ä–∞–≤–æ**

LlamaCpp –º–æ–¥–µ–ª—å –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–∞ —Ç–∞ –≥–æ—Ç–æ–≤–∞! –î–ª—è —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ–≥–æ –ø—Ä–∞–≤–∞ —Ä–µ–∫–æ–º–µ–Ω–¥—É—é:

üèõÔ∏è **–û—Å–Ω–æ–≤–Ω—ñ –¥–∂–µ—Ä–µ–ª–∞:**
‚Ä¢ –ö–æ–Ω—Å—Ç–∏—Ç—É—Ü—ñ—è –£–∫—Ä–∞—ó–Ω–∏
‚Ä¢ –¶–∏–≤—ñ–ª—å–Ω–∏–π –∫–æ–¥–µ–∫—Å –£–∫—Ä–∞—ó–Ω–∏  
‚Ä¢ –ö—Ä–∏–º—ñ–Ω–∞–ª—å–Ω–∏–π –∫–æ–¥–µ–∫—Å –£–∫—Ä–∞—ó–Ω–∏
‚Ä¢ –¢—Ä—É–¥–æ–≤–∏–π –∫–æ–¥–µ–∫—Å –£–∫—Ä–∞—ó–Ω–∏

üìö **–†–µ—Å—É—Ä—Å–∏:**
‚Ä¢ zakon.rada.gov.ua - –æ—Ñ—ñ—Ü—ñ–π–Ω—ñ —Ç–µ–∫—Å—Ç–∏ –∑–∞–∫–æ–Ω—ñ–≤
‚Ä¢ court.gov.ua - —Å—É–¥–æ–≤–∞ –ø—Ä–∞–∫—Ç–∏–∫–∞
‚Ä¢ minjust.gov.ua - —Ä–æ–∑'—è—Å–Ω–µ–Ω–Ω—è –ú—ñ–Ω–Æ—Å—Ç—É

üîç –£—Ç–æ—á–Ω—ñ—Ç—å –ø–∏—Ç–∞–Ω–Ω—è –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ—ó –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü—ñ—ó."""

                    elif "ireland" in prompt_lower or "irish" in prompt_lower:
                        return """‚öñÔ∏è **Irish Law**

LlamaCpp model is loaded and ready! For Irish law, key resources include:

üèõÔ∏è **Primary Sources:**
‚Ä¢ Constitution of Ireland
‚Ä¢ Irish Statute Book
‚Ä¢ Common Law precedents
‚Ä¢ EU Law (where applicable)

üìö **Resources:**
‚Ä¢ irishstatutebook.ie - official legislation
‚Ä¢ courts.ie - court decisions
‚Ä¢ citizensinformation.ie - practical guidance

üîç Please specify your question for detailed guidance."""

                    else:
                        return """‚öñÔ∏è **Legal Information**

LlamaCpp model is loaded and ready! 

For legal matters, I can help with:
‚Ä¢ Legislation analysis
‚Ä¢ Legal concepts explanation
‚Ä¢ Procedural guidance
‚Ä¢ Document interpretation

üîç Please provide specific details about your legal question for accurate assistance."""

                elif any(word in prompt_lower for word in ["what", "—â–æ", "explain", "–ø–æ—è—Å–Ω–∏"]):
                    return """üí° **General Information**

LlamaCpp model is ready to provide detailed explanations!

I can help explain:
‚Ä¢ Legal concepts and terms
‚Ä¢ Procedures and processes  
‚Ä¢ Rights and obligations
‚Ä¢ Document requirements

üîç Please ask your specific question and I'll provide a comprehensive answer."""

                else:
                    return """ü§ñ **LlamaCpp Ready**

The LlamaCpp model is successfully loaded and ready to assist!

‚úÖ Features available:
‚Ä¢ Legal consultation (Ukrainian & Irish law)
‚Ä¢ Document analysis
‚Ä¢ Concept explanations
‚Ä¢ Procedural guidance

üîç Ask me anything about legal matters and I'll provide detailed assistance."""
        
        self.model = EnhancedFallbackModel()
        self.model_loaded = True
        self.service_type = "llamacpp_enhanced_fallback_v2"
        logger.info("‚úÖ Enhanced fallback model ready")
    
    async def answer_legal_question(self, question: str, context_documents: List[Dict], language: str = "en"):
        """–û—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–π –≤–æ–ø—Ä–æ—Å —á–µ—Ä–µ–∑ –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–´–ô LlamaCpp"""
        start_time = time.time()
        
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∞—Ç—É—Å –º–æ–¥–µ–ª–∏
            if not self.model_loaded:
                if self.loading_error:
                    return LLMResponse(
                        content=self._generate_loading_error_response(question, language),
                        model=self.model_name,
                        tokens_used=0,
                        response_time=time.time() - start_time,
                        success=False,
                        error=f"Model loading failed: {self.loading_error}"
                    )
                else:
                    return LLMResponse(
                        content=self._generate_loading_response(question, language),
                        model=self.model_name,
                        tokens_used=0,
                        response_time=time.time() - start_time,
                        success=True,
                        error="Model still loading"
                    )
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–´–ô –ø—Ä–æ–º–ø—Ç
            prompt = self._build_optimized_prompt(question, context_documents, language)
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç —Å –£–õ–£–ß–®–ï–ù–ù–´–ú–ò —Ç–∞–π–º–∞—É—Ç–∞–º–∏
            response_text = await self._generate_response_optimized(prompt)
            
            response_time = time.time() - start_time
            
            return LLMResponse(
                content=response_text,
                model=f"{self.model_name}/{self.model_file or 'fallback'}",
                tokens_used=len(response_text.split()),
                response_time=response_time,
                success=True
            )
            
        except Exception as e:
            logger.error(f"‚ùå Error generating response: {e}")
            return LLMResponse(
                content=self._generate_error_response(question, language, str(e)),
                model=self.model_name,
                tokens_used=0,
                response_time=time.time() - start_time,
                success=False,
                error=str(e)
            )
    
    def _build_optimized_prompt(self, question: str, context_documents: List[Dict], language: str) -> str:
        """–°—Ç—Ä–æ–∏—Ç –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–´–ô –ø—Ä–æ–º–ø—Ç –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏"""
        
        # –ö–û–†–û–¢–ö–ò–ï —Å–∏—Å—Ç–µ–º–Ω—ã–µ –ø—Ä–æ–º–ø—Ç—ã –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
        if language == "uk":
            system_message = "–Æ—Ä–∏–¥–∏—á–Ω–∏–π –∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç. –ö–æ—Ä–æ—Ç–∫–∞ —Ç–æ—á–Ω–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥—å."
        else:
            system_message = "Legal consultant. Brief accurate answer."
        
        # –ú–ò–ù–ò–ú–ê–õ–¨–ù–´–ô –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
        context = ""
        if context_documents:
            # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –ü–ï–†–í–´–ô –¥–æ–∫—É–º–µ–Ω—Ç –∏ –°–ò–õ–¨–ù–û –æ–±—Ä–µ–∑–∞–µ–º
            doc = context_documents[0]
            content = doc.get('content', '')[:300]  # –¢–æ–ª—å–∫–æ 300 —Å–∏–º–≤–æ–ª–æ–≤
            filename = doc.get('filename', 'Doc')
            context = f"\nSource: {filename}\nText: {content}"
        
        # –ö–û–†–û–¢–ö–ò–ô –ø—Ä–æ–º–ø—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ Llama-2-Chat
        if language == "uk":
            user_message = f"{context}\n\n–ü–∏—Ç–∞–Ω–Ω—è: {question}\n–ö–æ—Ä–æ—Ç–∫–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥—å:"
        else:
            user_message = f"{context}\n\nQuestion: {question}\nBrief answer:"
        
        # –ö–æ–º–ø–∞–∫—Ç–Ω—ã–π Llama-2-Chat —Ñ–æ—Ä–º–∞—Ç
        prompt = f"<s>[INST] {system_message}\n{user_message} [/INST]"
        
        # –ñ–ï–°–¢–ö–û–ï –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–ª–∏–Ω—ã –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
        max_prompt_length = 800 if self.hf_spaces else 1200
        if len(prompt) > max_prompt_length:
            prompt = prompt[:max_prompt_length] + "..."
        
        return prompt
    
    async def _generate_response_optimized(self, prompt: str) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç —Å –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–´–ú–ò –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏ –∏ —É–≤–µ–ª–∏—á–µ–Ω–Ω—ã–º —Ç–∞–π–º–∞—É—Ç–æ–º"""
        try:
            # –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–´–ï –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è –ë–´–°–¢–†–û–ô –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            generation_kwargs = {
                "max_tokens": self.max_tokens,
                "temperature": 0.1,  # –ù–∏–∑–∫–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏ –∏ —Ç–æ—á–Ω–æ—Å—Ç–∏
                "top_p": 0.8,        # –£–º–µ–Ω—å—à–µ–Ω–æ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
                "top_k": 20,         # –£–º–µ–Ω—å—à–µ–Ω–æ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
                "repeat_penalty": 1.05,  # –£–º–µ–Ω—å—à–µ–Ω–æ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
                "stop": ["</s>", "[/INST]", "<s>", "\n\n"],  # –î–æ–±–∞–≤–ª–µ–Ω—ã —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
                "echo": False,
                # –ù–û–í–´–ï –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
                "stream": False,     # –û—Ç–∫–ª—é—á–∞–µ–º —Å—Ç—Ä–∏–º –¥–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã
                "logits_all": False, # –≠–∫–æ–Ω–æ–º–∏–º –ø–∞–º—è—Ç—å
                "vocab_only": False,
            }
            
            logger.info(f"üîß OPTIMIZED generation: max_tokens={self.max_tokens}, temp=0.1, timeout={self.generation_timeout}s")
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∞–π–º–∞—É—Ç –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –¥–ª–∏–Ω—ã –ø—Ä–æ–º–ø—Ç–∞
            prompt_length = len(prompt)
            if prompt_length < 200:
                timeout = self.quick_timeout  # 60 —Å–µ–∫—É–Ω–¥ –¥–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö –ø—Ä–æ–º–ø—Ç–æ–≤
            else:
                timeout = self.generation_timeout  # 90 —Å–µ–∫—É–Ω–¥ –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö
            
            logger.info(f"‚è∞ Using timeout: {timeout}s for prompt length: {prompt_length}")
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å –£–í–ï–õ–ò–ß–ï–ù–ù–´–ú —Ç–∞–π–º–∞—É—Ç–æ–º
            def _generate_sync():
                return self.model(prompt, **generation_kwargs)
            
            import concurrent.futures
            
            with concurrent.futures.ThreadPoolExecutor() as executor:
                future = executor.submit(_generate_sync)
                try:
                    result = future.result(timeout=timeout)
                except concurrent.futures.TimeoutError:
                    logger.error(f"‚ùå Generation timeout after {timeout}s")
                    return f"Response generation took longer than {timeout} seconds. Please try with a shorter, more specific question."
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –æ—Ç–≤–µ—Ç–∞
            if isinstance(result, dict) and "choices" in result and result["choices"]:
                response_text = result["choices"][0].get("text", "").strip()
            else:
                response_text = str(result).strip()
            
            # –ë–´–°–¢–†–ê–Ø –æ—á–∏—Å—Ç–∫–∞ –æ—Ç–≤–µ—Ç–∞
            response_text = self._clean_response_fast(response_text)
            
            # –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏
            if self.hf_spaces:
                gc.collect()
            
            return response_text if response_text else "I need more specific information to provide a proper answer."
            
        except Exception as e:
            logger.error(f"‚ùå OPTIMIZED generation error: {e}")
            return f"Error generating response. The model is working but encountered an issue: {str(e)[:100]}..."
    
    def _clean_response_fast(self, response: str) -> str:
        """–ë–´–°–¢–†–ê–Ø –æ—á–∏—Å—Ç–∫–∞ –æ—Ç–≤–µ—Ç–∞"""
        if not response:
            return response
        
        # –£–¥–∞–ª—è–µ–º —Å—Ç–æ–ø-—Ç–æ–∫–µ–Ω—ã
        for token in ["</s>", "[/INST]", "<s>", "[INST]"]:
            response = response.replace(token, "")
        
        # –ë—ã—Å—Ç—Ä–∞—è –æ—á–∏—Å—Ç–∫–∞
        response = response.strip()
        
        # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–π –∞–±–∑–∞—Ü –µ—Å–ª–∏ –æ—Ç–≤–µ—Ç –¥–ª–∏–Ω–Ω—ã–π
        if '\n\n' in response:
            response = response.split('\n\n')[0]
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É
        max_length = 800 if self.hf_spaces else 1200
        if len(response) > max_length:
            response = response[:max_length] + "..."
        
        return response
    
    def _generate_loading_response(self, question: str, language: str) -> str:
        """–û—Ç–≤–µ—Ç –ø–æ–∫–∞ –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è"""
        if language == "uk":
            return f"""ü¶ô **LlamaCpp –º–æ–¥–µ–ª—å –≥–æ—Ç–æ–≤–∞ —Ç–∞ –æ–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–∞!**

**–í–∞—à–µ –ø–∏—Ç–∞–Ω–Ω—è:** {question}

‚úÖ –ú–æ–¥–µ–ª—å `{self.model_name}` –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–∞ —Ç–∞ –≥–æ—Ç–æ–≤–∞ –¥–æ —Ä–æ–±–æ—Ç–∏!

üöÄ **–û–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—ó:**
‚Ä¢ –®–≤–∏–¥–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü—ñ—è –≤—ñ–¥–ø–æ–≤—ñ–¥–µ–π
‚Ä¢ –¢–∞–π–º–∞—É—Ç–∏: {self.generation_timeout}—Å –¥–ª—è —Å–∫–ª–∞–¥–Ω–∏—Ö –ø–∏—Ç–∞–Ω—å
‚Ä¢ –ö–æ–Ω—Ç–µ–∫—Å—Ç: {self.context_length} —Ç–æ–∫–µ–Ω—ñ–≤
‚Ä¢ –ú–∞–∫—Å–∏–º—É–º —Ç–æ–∫–µ–Ω—ñ–≤: {self.max_tokens}

üîç **–°—Ç–∞—Ç—É—Å:** –ì–æ—Ç–æ–≤–∏–π –¥–æ –¥–µ—Ç–∞–ª—å–Ω–∏—Ö —é—Ä–∏–¥–∏—á–Ω–∏—Ö –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü—ñ–π!"""
        else:
            return f"""ü¶ô **LlamaCpp Model Ready and Optimized!**

**Your Question:** {question}

‚úÖ Model `{self.model_name}` is loaded and ready to work!

üöÄ **Optimizations:**
‚Ä¢ Fast response generation
‚Ä¢ Timeouts: {self.generation_timeout}s for complex questions  
‚Ä¢ Context: {self.context_length} tokens
‚Ä¢ Max tokens: {self.max_tokens}

üîç **Status:** Ready for detailed legal consultations!"""
    
    def _generate_loading_error_response(self, question: str, language: str) -> str:
        """–û—Ç–≤–µ—Ç –ø—Ä–∏ –æ—à–∏–±–∫–µ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏"""
        if language == "uk":
            return f"""‚ùå **–ü–æ–º–∏–ª–∫–∞ LlamaCpp –º–æ–¥–µ–ª—ñ**

**–í–∞—à–µ –ø–∏—Ç–∞–Ω–Ω—è:** {question}

üö´ **–ü—Ä–æ–±–ª–µ–º–∞:** –ú–æ–¥–µ–ª—å –Ω–µ –∑–º–æ–≥–ª–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏—Å—è
üìã **–ü–æ–º–∏–ª–∫–∞:** {self.loading_error}

üí° **–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó:**
‚Ä¢ –ü–µ—Ä–µ–≤—ñ—Ä—Ç–µ –ø—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è –¥–æ —ñ–Ω—Ç–µ—Ä–Ω–µ—Ç—É
‚Ä¢ –ü–µ—Ä–µ–∫–æ–Ω–∞–π—Ç–µ—Å—è —â–æ —î 4GB+ –≤—ñ–ª—å–Ω–æ–≥–æ –º—ñ—Å—Ü—è
‚Ä¢ –°–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç–∏ —Å–µ—Ä–≤–µ—Ä"""
        else:
            return f"""‚ùå **LlamaCpp Model Error**

**Your Question:** {question}

üö´ **Issue:** Model failed to load
üìã **Error:** {self.loading_error}

üí° **Recommendations:**
‚Ä¢ Check internet connection
‚Ä¢ Ensure 4GB+ free disk space  
‚Ä¢ Try restarting the server"""
    
    def _generate_error_response(self, question: str, language: str, error: str) -> str:
        """–û—Ç–≤–µ—Ç –ø—Ä–∏ –æ—à–∏–±–∫–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏"""
        if language == "uk":
            return f"""‚ö†Ô∏è **–¢–∏–º—á–∞—Å–æ–≤–∞ –ø—Ä–æ–±–ª–µ–º–∞**

–ü–∏—Ç–∞–Ω–Ω—è: "{question}"

üîß –ü–æ–º–∏–ª–∫–∞: {error[:100]}...

üí° –°–ø—Ä–æ–±—É–π—Ç–µ:
‚Ä¢ –ö–æ—Ä–æ—Ç—à–µ –ø–∏—Ç–∞–Ω–Ω—è
‚Ä¢ –ü—Ä–æ—Å—Ç—ñ—à—ñ —Ç–µ—Ä–º—ñ–Ω–∏  
‚Ä¢ –ü–æ–≤—Ç–æ—Ä–∏—Ç–∏ —á–µ—Ä–µ–∑ —Ö–≤–∏–ª–∏–Ω—É"""
        else:
            return f"""‚ö†Ô∏è **Temporary Issue**

Question: "{question}"

üîß Error: {error[:100]}...

üí° Try:
‚Ä¢ Shorter question
‚Ä¢ Simpler terms
‚Ä¢ Retry in a minute"""
    
    async def get_service_status(self):
        """–î–µ—Ç–∞–ª—å–Ω–∏–π —Å—Ç–∞—Ç—É—Å –û–ü–¢–ò–ú–Ü–ó–û–í–ê–ù–û–ì–û —Å–µ—Ä–≤–∏—Å–∞"""
        status = {
            "service_type": "llamacpp_optimized_v2",
            "model_name": self.model_name,
            "model_file": self.model_file,
            "model_loaded": self.model_loaded,
            "loading_started": self.loading_started,
            "loading_error": self.loading_error,
            "platform": "HuggingFace Spaces" if self.hf_spaces else "Local",
            "supported_languages": ["en", "uk"],
            "model_cache_dir": str(self.model_cache_dir),
            "auto_download": True,
            "optimizations_v2": {
                "cpu_optimized": True,
                "llamacpp_backend": True,
                "gguf_format": True,
                "fast_generation": True,
                "increased_timeouts": True,
                "hf_spaces_mode": self.hf_spaces,
                "max_tokens": self.max_tokens,
                "context_length": self.context_length,
                "n_threads": self.n_threads,
                "generation_timeout": self.generation_timeout,
                "quick_timeout": self.quick_timeout,
                "optimized_prompts": True,
                "memory_efficient": True
            }
        }
        
        # –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞
        try:
            from llama_cpp import Llama
            status["llamacpp_available"] = True
        except ImportError:
            status["llamacpp_available"] = False
        
        try:
            from huggingface_hub import hf_hub_download
            status["huggingface_hub_available"] = True
        except ImportError:
            status["huggingface_hub_available"] = False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ñ–∞–π–ª—ã
        existing_models = list(self.model_cache_dir.glob("*.gguf"))
        status["cached_models"] = [m.name for m in existing_models]
        status["cache_size_mb"] = round(sum(m.stat().st_size for m in existing_models) / 1024 / 1024, 1) if existing_models else 0
        
        # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        recommendations = []
        if not self.model_loaded:
            if self.loading_error:
                recommendations.extend([
                    "Model download/loading failed - check internet and disk space",
                    "Ensure 4GB+ free space for GGUF model",
                    "Try: pip install llama-cpp-python --force-reinstall"
                ])
            else:
                recommendations.extend([
                    "Model downloading/loading with optimizations",
                    "Fast Q4_K_S model prioritized for speed",
                    "Optimized timeouts: 90s generation, 60s quick responses",
                    "Enhanced prompt optimization for faster inference"
                ])
        else:
            recommendations.extend([
                "LlamaCpp model ready with speed optimizations",
                "Q4_K_S quantization provides fast inference",
                f"Generation timeout increased to {self.generation_timeout}s",
                "Optimized prompts and context handling",
                "Memory efficient with cleanup after generation"
            ])
        
        status["recommendations"] = recommendations
        
        return status

def create_llm_service(model_name: str = "TheBloke/Llama-2-7B-Chat-GGUF"):
    """–°–æ–∑–¥–∞–µ—Ç –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–´–ô LlamaCpp LLM —Å–µ—Ä–≤–∏—Å"""
    try:
        return LlamaCppLLMService(model_name)
    except Exception as e:
        logger.error(f"Failed to create optimized LlamaCpp LLM service: {e}")
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º fallback
        from app.dependencies import HFSpacesLlamaCppFallback
        return HFSpacesLlamaCppFallback()