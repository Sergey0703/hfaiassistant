# ====================================
# –§–ê–ô–õ: backend/services/llm_service.py (–ù–û–í–´–ô –§–ê–ô–õ)
# –°–æ–∑–¥–∞—Ç—å –Ω–æ–≤—ã–π —Ñ–∞–π–ª –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å Ollama LLM
# ====================================

"""
LLM Service - –°–µ—Ä–≤–∏—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ —á–µ—Ä–µ–∑ Ollama
"""

import aiohttp
import asyncio
import logging
import json
import time
from typing import Dict, List, Optional, Any
from dataclasses import dataclass

logger = logging.getLogger(__name__)

@dataclass
class LLMResponse:
    """–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –æ—Ç–≤–µ—Ç–∞ –æ—Ç LLM"""
    content: str
    model: str
    tokens_used: int
    response_time: float
    success: bool
    error: Optional[str] = None

class OllamaService:
    """–°–µ—Ä–≤–∏—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å Ollama"""
    
    def __init__(self, base_url: str = "http://localhost:11434", default_model: str = "llama3:latest"):
        self.base_url = base_url.rstrip('/')
        self.default_model = default_model
        self.session = None
        self.available_models = []
        self.service_available = False
        
        logger.info(f"ü§ñ Initializing Ollama service: {self.base_url}")
        
        # –ù–ï —Å–æ–∑–¥–∞–µ–º —Å–µ—Å—Å–∏—é –∑–¥–µ—Å—å - –±—É–¥–µ–º —Å–æ–∑–¥–∞–≤–∞—Ç—å –ø–æ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—é
    
    def _create_session(self):
        """–°–æ–∑–¥–∞–µ—Ç –Ω–æ–≤—É—é HTTP —Å–µ—Å—Å–∏—é"""
        timeout = aiohttp.ClientTimeout(total=120, connect=10)
        return aiohttp.ClientSession(timeout=timeout)
    
    async def check_service_health(self) -> Dict[str, Any]:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å Ollama —Å–µ—Ä–≤–∏—Å–∞"""
        session = None
        try:
            session = aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=10))
            
            async with session.get(f"{self.base_url}/api/tags") as response:
                if response.status == 200:
                    data = await response.json()
                    self.available_models = [model["name"] for model in data.get("models", [])]
                    self.service_available = True
                    
                    logger.info(f"‚úÖ Ollama service is available with {len(self.available_models)} models")
                    
                    return {
                        "available": True,
                        "models": self.available_models,
                        "default_model": self.default_model,
                        "base_url": self.base_url
                    }
                else:
                    self.service_available = False
                    return {
                        "available": False,
                        "error": f"HTTP {response.status}",
                        "base_url": self.base_url
                    }
                    
        except aiohttp.ClientConnectorError:
            self.service_available = False
            logger.warning("‚ùå Ollama service not available - connection refused")
            return {
                "available": False,
                "error": "Connection refused - is Ollama running?",
                "base_url": self.base_url
            }
        except Exception as e:
            self.service_available = False
            logger.error(f"‚ùå Error checking Ollama service: {e}")
            return {
                "available": False,
                "error": str(e),
                "base_url": self.base_url
            }
        finally:
            # –í–ê–ñ–ù–û: –í—Å–µ–≥–¥–∞ –∑–∞–∫—Ä—ã–≤–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é —Å–µ—Å—Å–∏—é
            if session and not session.closed:
                await session.close()
    
    async def pull_model(self, model_name: str) -> Dict[str, Any]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å –≤ Ollama"""
        session = None
        try:
            session = aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=300))  # 5 –º–∏–Ω—É—Ç –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏
            
            payload = {"name": model_name}
            
            async with session.post(f"{self.base_url}/api/pull", json=payload) as response:
                if response.status == 200:
                    logger.info(f"‚úÖ Model {model_name} pulled successfully")
                    await self.check_service_health()  # –û–±–Ω–æ–≤–ª—è–µ–º —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π
                    return {"success": True, "model": model_name}
                else:
                    error_text = await response.text()
                    logger.error(f"‚ùå Failed to pull model {model_name}: {error_text}")
                    return {"success": False, "error": error_text}
                    
        except Exception as e:
            logger.error(f"‚ùå Error pulling model {model_name}: {e}")
            return {"success": False, "error": str(e)}
        finally:
            # –í–ê–ñ–ù–û: –í—Å–µ–≥–¥–∞ –∑–∞–∫—Ä—ã–≤–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é —Å–µ—Å—Å–∏—é
            if session and not session.closed:
                await session.close()
    
    async def generate_response(self, 
                              prompt: str, 
                              model: str = None,
                              system_prompt: str = None,
                              temperature: float = 0.7,
                              max_tokens: int = 1000) -> LLMResponse:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –æ—Ç LLM"""
        
        model = model or self.default_model
        start_time = time.time()
        
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å —Å–µ—Ä–≤–∏—Å–∞
            if not self.service_available:
                health = await self.check_service_health()
                if not health["available"]:
                    return LLMResponse(
                        content="",
                        model=model,
                        tokens_used=0,
                        response_time=0,
                        success=False,
                        error="Ollama service not available"
                    )
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –º–æ–¥–µ–ª–∏
            if model not in self.available_models:
                logger.warning(f"Model {model} not found, attempting to pull...")
                pull_result = await self.pull_model(model)
                if not pull_result["success"]:
                    return LLMResponse(
                        content="",
                        model=model,
                        tokens_used=0,
                        response_time=time.time() - start_time,
                        success=False,
                        error=f"Model {model} not available and pull failed"
                    )
            
            session = self._create_session()  # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é —Å–µ—Å—Å–∏—é –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
            
            # –£–ü–†–û–©–ï–ù–ù–´–ô payload –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
            payload = {
                "model": model,
                "prompt": prompt,
                "stream": False
            }
            
            # –î–æ–±–∞–≤–ª—è–µ–º options —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            if temperature != 0.7 or max_tokens != 1000:
                payload["options"] = {
                    "temperature": temperature,
                    "num_predict": max_tokens
                }
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –µ—Å–ª–∏ –µ—Å—Ç—å
            if system_prompt:
                payload["system"] = system_prompt
            
            logger.debug(f"ü§ñ Sending request to Ollama: model={model}, prompt_length={len(prompt)}")
            
            try:
                async with session.post(f"{self.base_url}/api/generate", json=payload) as response:
                    if response.status == 200:
                        data = await response.json()
                        
                        content = data.get("response", "")
                        tokens_used = data.get("eval_count", 0)
                        response_time = time.time() - start_time
                        
                        logger.info(f"‚úÖ LLM response generated: {len(content)} chars, {tokens_used} tokens, {response_time:.2f}s")
                        
                        return LLMResponse(
                            content=content,
                            model=model,
                            tokens_used=tokens_used,
                            response_time=response_time,
                            success=True
                        )
                    else:
                        error_text = await response.text()
                        logger.error(f"‚ùå Ollama API error: {response.status} - {error_text}")
                        
                        return LLMResponse(
                            content="",
                            model=model,
                            tokens_used=0,
                            response_time=time.time() - start_time,
                            success=False,
                            error=f"API error: {response.status}"
                        )
            finally:
                # –í–ê–ñ–ù–û: –í—Å–µ–≥–¥–∞ –∑–∞–∫—Ä—ã–≤–∞–µ–º —Å–µ—Å—Å–∏—é –ø–æ—Å–ª–µ –∑–∞–ø—Ä–æ—Å–∞
                if session and not session.closed:
                    await session.close()
                    
        except asyncio.TimeoutError:
            logger.error("‚ùå Ollama request timeout")
            return LLMResponse(
                content="",
                model=model,
                tokens_used=0,
                response_time=time.time() - start_time,
                success=False,
                error="Request timeout"
            )
        except Exception as e:
            logger.error(f"‚ùå Error generating LLM response: {e}")
            return LLMResponse(
                content="",
                model=model,
                tokens_used=0,
                response_time=time.time() - start_time,
                success=False,
                error=str(e)
            )
    
    async def close(self):
        """–ó–∞–∫—Ä—ã–≤–∞–µ—Ç HTTP —Å–µ—Å—Å–∏—é"""
        # –ù–µ –Ω—É–∂–Ω–æ –∑–∞–∫—Ä—ã–≤–∞—Ç—å —Å–µ—Å—Å–∏—é, —Ç–∞–∫ –∫–∞–∫ —Å–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
        logger.debug("üîí Ollama service cleanup completed")

class LegalAssistantLLM:
    """–û—Å–Ω–æ–≤–Ω–æ–π —Å–µ—Ä–≤–∏—Å Legal Assistant —Å –ø—Ä–æ–º–ø—Ç–∞–º–∏ –¥–ª—è —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤"""
    
    def __init__(self, ollama_service: OllamaService):
        self.ollama = ollama_service
        self.system_prompts = {
            "en": """You are a helpful legal assistant specializing in Irish and Ukrainian law. 
Your task is to provide accurate, helpful answers based on the provided legal documents.

Guidelines:
- Answer only based on the provided context
- If information is not in the context, say so clearly
- Provide specific references to laws, acts, or regulations when mentioned
- Use clear, professional language
- If asked about something outside your expertise, acknowledge limitations
- Always prioritize accuracy over completeness""",
            
            "uk": """–í–∏ - –∫–æ—Ä–∏—Å–Ω–∏–π —é—Ä–∏–¥–∏—á–Ω–∏–π –ø–æ–º—ñ—á–Ω–∏–∫, —â–æ —Å–ø–µ—Ü—ñ–∞–ª—ñ–∑—É—î—Ç—å—Å—è –Ω–∞ —ñ—Ä–ª–∞–Ω–¥—Å—å–∫–æ–º—É —Ç–∞ —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ–º—É –ø—Ä–∞–≤—ñ.
–í–∞—à–µ –∑–∞–≤–¥–∞–Ω–Ω—è - –Ω–∞–¥–∞–≤–∞—Ç–∏ —Ç–æ—á–Ω—ñ, –∫–æ—Ä–∏—Å–Ω—ñ –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ –Ω–∞ –æ—Å–Ω–æ–≤—ñ –Ω–∞–¥–∞–Ω–∏—Ö —é—Ä–∏–¥–∏—á–Ω–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤.

–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó:
- –í—ñ–¥–ø–æ–≤—ñ–¥–∞–π—Ç–µ –ª–∏—à–µ –Ω–∞ –æ—Å–Ω–æ–≤—ñ –Ω–∞–¥–∞–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É
- –Ø–∫—â–æ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó –Ω–µ–º–∞—î –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç—ñ, —á—ñ—Ç–∫–æ –ø—Ä–æ —Ü–µ —Å–∫–∞–∂—ñ—Ç—å
- –ù–∞–¥–∞–≤–∞–π—Ç–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ñ –ø–æ—Å–∏–ª–∞–Ω–Ω—è –Ω–∞ –∑–∞–∫–æ–Ω–∏, –∞–∫—Ç–∏ —á–∏ –ø–æ—Å—Ç–∞–Ω–æ–≤–∏, –∫–æ–ª–∏ –≤–æ–Ω–∏ –∑–≥–∞–¥—É—é—Ç—å—Å—è
- –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π—Ç–µ –∑—Ä–æ–∑—É–º—ñ–ª—É, –ø—Ä–æ—Ñ–µ—Å—ñ–π–Ω—É –º–æ–≤—É
- –Ø–∫—â–æ –∑–∞–ø–∏—Ç—É—é—Ç—å –ø—Ä–æ —â–æ—Å—å –ø–æ–∑–∞ –≤–∞—à–æ—é –µ–∫—Å–ø–µ—Ä—Ç–∏–∑–æ—é, –≤–∏–∑–Ω–∞–π—Ç–µ –æ–±–º–µ–∂–µ–Ω–Ω—è
- –ó–∞–≤–∂–¥–∏ –Ω–∞–¥–∞–≤–∞–π—Ç–µ –ø–µ—Ä–µ–≤–∞–≥—É —Ç–æ—á–Ω–æ—Å—Ç—ñ –Ω–∞–¥ –ø–æ–≤–Ω–æ—Ç–æ—é"""
        }
    
    async def answer_legal_question(self, 
                                  question: str, 
                                  context_documents: List[Dict[str, Any]], 
                                  language: str = "en") -> LLMResponse:
        """–û—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–π –≤–æ–ø—Ä–æ—Å –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
        
        # –£–ü–†–û–©–ï–ù–ù–´–ô –∫–æ–Ω—Ç–µ–∫—Å—Ç - –±–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–π –¥–æ–∫—É–º–µ–Ω—Ç –∏ –æ–±—Ä–µ–∑–∞–µ–º
        if context_documents:
            first_doc = context_documents[0]
            content = first_doc.get("content", "")
            
            # –°–ò–õ–¨–ù–û –æ–±—Ä–µ–∑–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç - –º–∞–∫—Å–∏–º—É–º 500 —Å–∏–º–≤–æ–ª–æ–≤
            if len(content) > 500:
                content = content[:500] + "..."
            
            filename = first_doc.get("filename", "Document")
            
            # –ö–†–ê–¢–ö–ò–ô –ø—Ä–æ–º–ø—Ç –±–µ–∑ –ª–∏—à–Ω–∏—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π
            if language == "uk":
                prompt = f"""–î–æ–∫—É–º–µ–Ω—Ç: {filename}
–ö–æ–Ω—Ç–µ–∫—Å—Ç: {content}

–ü–∏—Ç–∞–Ω–Ω—è: {question}

–ö–æ—Ä–æ—Ç–∫–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥—å:"""
            else:
                prompt = f"""Document: {filename}
Context: {content}

Question: {question}

Brief answer:"""
        else:
            # –ï—Å–ª–∏ –Ω–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ - —Å–æ–≤—Å–µ–º –ø—Ä–æ—Å—Ç–æ–π –ø—Ä–æ–º–ø—Ç
            if language == "uk":
                prompt = f"–ü–∏—Ç–∞–Ω–Ω—è: {question}\n–ö–æ—Ä–æ—Ç–∫–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥—å:"
            else:
                prompt = f"Question: {question}\nBrief answer:"
        
        # –£–±–∏—Ä–∞–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è —É–ø—Ä–æ—â–µ–Ω–∏—è
        response = await self.ollama.generate_response(
            prompt=prompt,
            system_prompt=None,  # –£–±–∏—Ä–∞–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç
            temperature=0.1,  # –û—á–µ–Ω—å –Ω–∏–∑–∫–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞
            max_tokens=200    # –°–∏–ª—å–Ω–æ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É –æ—Ç–≤–µ—Ç–∞
        )
        
        return response
    
    async def get_service_status(self) -> Dict[str, Any]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç—É—Å LLM —Å–µ—Ä–≤–∏—Å–∞"""
        health = await self.ollama.check_service_health()
        
        return {
            "ollama_available": health["available"],
            "models_available": health.get("models", []),
            "default_model": self.ollama.default_model,
            "base_url": self.ollama.base_url,
            "system_prompts_loaded": len(self.system_prompts),
            "supported_languages": list(self.system_prompts.keys()),
            "error": health.get("error")
        }
    
    async def close(self):
        """–ó–∞–∫—Ä—ã–≤–∞–µ—Ç —Å–µ—Ä–≤–∏—Å"""
        await self.ollama.close()

# –§–∞–±—Ä–∏—á–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Å–µ—Ä–≤–∏—Å–∞
def create_llm_service(ollama_url: str = "http://localhost:11434", 
                      model: str = "llama3:latest") -> LegalAssistantLLM:
    """–°–æ–∑–¥–∞–µ—Ç –∏ –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç LLM —Å–µ—Ä–≤–∏—Å"""
    ollama_service = OllamaService(base_url=ollama_url, default_model=model)
    return LegalAssistantLLM(ollama_service)