# backend/services/llama_service.py - –ü–†–ê–í–ò–õ–¨–ù–´–ô LLM –°–ï–†–í–ò–°
"""
Llama LLM Service using HuggingFace Inference API
"""

import logging
import time
import os
import asyncio
from typing import List, Dict, Optional
from dataclasses import dataclass

logger = logging.getLogger(__name__)

@dataclass
class LlamaResponse:
    content: str
    model: str
    tokens_used: int
    response_time: float
    success: bool
    error: Optional[str] = None

class LlamaService:
    """LLM —Å–µ—Ä–≤–∏—Å —Å HuggingFace Inference API"""
    
    def __init__(self):
        self.service_type = "huggingface_inference"
        self.model_name = "meta-llama/Llama-3.1-8B-Instruct"
        self.hf_token = os.getenv("HF_TOKEN")
        self.ready = True
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ç–æ–∫–µ–Ω–∞
        if not self.hf_token:
            logger.warning("‚ö†Ô∏è HF_TOKEN not set - using public inference (rate limited)")
        else:
            logger.info("‚úÖ HF_TOKEN configured for Llama service")
        
        logger.info(f"ü¶ô Llama service initialized: {self.model_name}")
    
    async def answer_legal_question(self, question: str, context_documents: List[Dict], language: str = "en"):
        """–û—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–π –≤–æ–ø—Ä–æ—Å"""
        start_time = time.time()
        
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º demo —Ä–µ–∂–∏–º
            if os.getenv("LLM_DEMO_MODE", "false").lower() == "true":
                return self._generate_demo_response(question, context_documents, language, start_time)
            
            # –°—Ç—Ä–æ–∏–º –ø—Ä–æ–º–ø—Ç
            prompt = self._build_prompt(question, context_documents, language)
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç —á–µ—Ä–µ–∑ HuggingFace API
            try:
                from huggingface_hub import InferenceClient
                
                client = InferenceClient(
                    model=self.model_name,
                    token=self.hf_token
                )
                
                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
                response = client.text_generation(
                    prompt,
                    max_new_tokens=300,
                    temperature=0.3,
                    do_sample=True,
                    return_full_text=False
                )
                
                # –û—á–∏—â–∞–µ–º –æ—Ç–≤–µ—Ç
                if isinstance(response, str):
                    content = response.strip()
                else:
                    content = str(response).strip()
                
                response_time = time.time() - start_time
                
                return LlamaResponse(
                    content=content,
                    model=self.model_name,
                    tokens_used=len(content.split()),
                    response_time=response_time,
                    success=True
                )
                
            except ImportError:
                logger.error("‚ùå huggingface_hub not installed")
                return self._generate_error_response(question, language, "Missing huggingface_hub", start_time)
            
            except Exception as e:
                logger.error(f"‚ùå HuggingFace API error: {e}")
                return self._generate_error_response(question, language, str(e), start_time)
        
        except Exception as e:
            logger.error(f"‚ùå General error in answer_legal_question: {e}")
            return self._generate_error_response(question, language, str(e), start_time)
    
    def _build_prompt(self, question: str, context_documents: List[Dict], language: str) -> str:
        """–°—Ç—Ä–æ–∏—Ç –ø—Ä–æ–º–ø—Ç –¥–ª—è Llama"""
        
        if language == "uk":
            system_prompt = "–¢–∏ —é—Ä–∏–¥–∏—á–Ω–∏–π –∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç. –í—ñ–¥–ø–æ–≤—ñ–¥–∞–π –∫–æ—Ä–æ—Ç–∫–æ —ñ –ø–æ —Å—É—Ç—ñ –Ω–∞ –æ—Å–Ω–æ–≤—ñ –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤."
            context_intro = "–î–æ–∫—É–º–µ–Ω—Ç–∏:"
            answer_intro = f"–ü–∏—Ç–∞–Ω–Ω—è: {question}\n–í—ñ–¥–ø–æ–≤—ñ–¥—å:"
        else:
            system_prompt = "You are a legal consultant. Answer concisely based on the provided documents."
            context_intro = "Documents:"
            answer_intro = f"Question: {question}\nAnswer:"
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
        context_parts = []
        for i, doc in enumerate(context_documents[:2]):  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º 2 –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏
            filename = doc.get('filename', f'Doc {i+1}')
            content = doc.get('content', '')[:500]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É
            context_parts.append(f"{filename}: {content}")
        
        context = f"\n{context_intro}\n" + "\n".join(context_parts) if context_parts else ""
        
        # –°–æ–±–∏—Ä–∞–µ–º –ø—Ä–æ–º–ø—Ç
        prompt = f"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n{system_prompt}<|eot_id|>\n"
        
        if context:
            prompt += f"<|start_header_id|>user<|end_header_id|>\n{context}\n\n{answer_intro}<|eot_id|>\n"
        else:
            prompt += f"<|start_header_id|>user<|end_header_id|>\n{answer_intro}<|eot_id|>\n"
        
        prompt += "<|start_header_id|>assistant<|end_header_id|>\n"
        
        return prompt
    
    def _generate_demo_response(self, question: str, context_documents: List[Dict], language: str, start_time: float):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç demo –æ—Ç–≤–µ—Ç"""
        if language == "uk":
            content = f"""ü§ñ **Demo —Ä–µ–∂–∏–º –∞–∫—Ç–∏–≤–Ω–∏–π**

**–í–∞—à–µ –ø–∏—Ç–∞–Ω–Ω—è:** {question}

üìö –ó–Ω–∞–π–¥–µ–Ω–æ {len(context_documents)} –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤ —É –±–∞–∑—ñ –∑–Ω–∞–Ω—å.

üí° **Demo –≤—ñ–¥–ø–æ–≤—ñ–¥—å:** –¶–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü—ñ–π–Ω–∏–π —Ä–µ–∂–∏–º Legal Assistant. –£ —Ä–æ–±–æ—á–æ–º—É —Ä–µ–∂–∏–º—ñ —Ç—É—Ç –±—É–¥–µ –¥–µ—Ç–∞–ª—å–Ω–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥—å –≤—ñ–¥ Llama-3.1-8B-Instruct –Ω–∞ –æ—Å–Ω–æ–≤—ñ –≤–∞—à–∏—Ö —é—Ä–∏–¥–∏—á–Ω–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤.

üîß **–î–ª—è –∞–∫—Ç–∏–≤–∞—Ü—ñ—ó –ø–æ–≤–Ω–æ—ó –≤–µ—Ä—Å—ñ—ó:**
1. –û—Ç—Ä–∏–º–∞–π—Ç–µ —Ç–æ–∫–µ–Ω –Ω–∞ https://huggingface.co/settings/tokens
2. –í—Å—Ç–∞–Ω–æ–≤—ñ—Ç—å –∑–º—ñ–Ω–Ω—É HF_TOKEN
3. –í—Å—Ç–∞–Ω–æ–≤—ñ—Ç—å LLM_DEMO_MODE=false"""
        else:
            content = f"""ü§ñ **Demo Mode Active**

**Your Question:** {question}

üìö Found {len(context_documents)} documents in knowledge base.

üí° **Demo Response:** This is Legal Assistant demo mode. In production mode, you would get detailed answers from Llama-3.1-8B-Instruct based on your legal documents.

üîß **To activate full version:**
1. Get token at https://huggingface.co/settings/tokens
2. Set HF_TOKEN environment variable
3. Set LLM_DEMO_MODE=false"""
        
        return LlamaResponse(
            content=content,
            model="demo_mode",
            tokens_used=len(content.split()),
            response_time=time.time() - start_time,
            success=True
        )
    
    def _generate_error_response(self, question: str, language: str, error: str, start_time: float):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –ø—Ä–∏ –æ—à–∏–±–∫–µ"""
        if language == "uk":
            content = f"""‚ùå **–ü–æ–º–∏–ª–∫–∞ LLM —Å–µ—Ä–≤—ñ—Å—É**

**–í–∞—à–µ –ø–∏—Ç–∞–Ω–Ω—è:** {question}

üîß **–ü–æ–º–∏–ª–∫–∞:** {error}

üí° **–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó:**
‚Ä¢ –ü–µ—Ä–µ–≤—ñ—Ä—Ç–µ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è HF_TOKEN
‚Ä¢ –°–ø—Ä–æ–±—É–π—Ç–µ demo —Ä–µ–∂–∏–º: LLM_DEMO_MODE=true
‚Ä¢ –ü–µ—Ä–µ–≤—ñ—Ä—Ç–µ –ø—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è –¥–æ —ñ–Ω—Ç–µ—Ä–Ω–µ—Ç—É"""
        else:
            content = f"""‚ùå **LLM Service Error**

**Your Question:** {question}

üîß **Error:** {error}

üí° **Recommendations:**
‚Ä¢ Check HF_TOKEN configuration
‚Ä¢ Try demo mode: LLM_DEMO_MODE=true
‚Ä¢ Check internet connection"""
        
        return LlamaResponse(
            content=content,
            model="error_fallback",
            tokens_used=len(content.split()),
            response_time=time.time() - start_time,
            success=False,
            error=error
        )
    
    async def get_service_status(self):
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç—É—Å —Å–µ—Ä–≤–∏—Å–∞"""
        return {
            "service_type": self.service_type,
            "model_name": self.model_name,
            "ready": self.ready,
            "hf_token_configured": bool(self.hf_token),
            "demo_mode": os.getenv("LLM_DEMO_MODE", "false").lower() == "true",
            "recommendations": [
                "Set HF_TOKEN for better rate limits",
                "Use LLM_DEMO_MODE=true for testing",
                "Check https://huggingface.co/settings/tokens for token"
            ]
        }

def create_llama_service():
    """–°–æ–∑–¥–∞–µ—Ç Llama —Å–µ—Ä–≤–∏—Å"""
    try:
        return LlamaService()
    except Exception as e:
        logger.error(f"‚ùå Failed to create Llama service: {e}")
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º fallback
        return create_fallback_service()

def create_fallback_service():
    """–°–æ–∑–¥–∞–µ—Ç fallback —Å–µ—Ä–≤–∏—Å"""
    class FallbackService:
        def __init__(self):
            self.service_type = "fallback"
            self.ready = True
        
        async def answer_legal_question(self, question: str, context_documents: list, language: str = "en"):
            if language == "uk":
                content = f"""üîÑ **–°–∏—Å—Ç–µ–º–∞ —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑—É—î—Ç—å—Å—è**

**–í–∞—à–µ –ø–∏—Ç–∞–Ω–Ω—è:** {question}

‚ö†Ô∏è LLM —Å–µ—Ä–≤—ñ—Å –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∏–π. –í—Å—Ç–∞–Ω–æ–≤—ñ—Ç—å:
1. HF_TOKEN - —Ç–æ–∫–µ–Ω –≤—ñ–¥ HuggingFace
2. LLM_DEMO_MODE=true –¥–ª—è –¥–µ–º–æ —Ä–µ–∂–∏–º—É

üìö –ó–Ω–∞–π–¥–µ–Ω–æ {len(context_documents)} –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤ —É –±–∞–∑—ñ."""
            else:
                content = f"""üîÑ **System Initializing**

**Your Question:** {question}

‚ö†Ô∏è LLM service unavailable. Please set:
1. HF_TOKEN - HuggingFace token
2. LLM_DEMO_MODE=true for demo mode

üìö Found {len(context_documents)} documents in database."""
            
            return LlamaResponse(
                content=content,
                model="fallback",
                tokens_used=len(content.split()),
                response_time=0.1,
                success=True
            )
        
        async def get_service_status(self):
            return {"service_type": "fallback", "ready": True}
    
    return FallbackService()