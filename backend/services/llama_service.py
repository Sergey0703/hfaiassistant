# backend/services/llama_service.py - –ò–°–ü–†–ê–í–õ–ï–ù –ü–û–î –ù–û–í–´–ô HUGGINGFACE API
"""
Llama LLM Service using NEW HuggingFace Inference Providers API
–ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –°—Ç–∞—Ä—ã–π API –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç, –ø–µ—Ä–µ—Ö–æ–¥ –Ω–∞ –Ω–æ–≤—ã–π Inference Providers
"""

import logging
import time
import os
import asyncio
from typing import List, Dict, Optional
from dataclasses import dataclass

logger = logging.getLogger(__name__)

@dataclass
class LlamaResponse:
    content: str
    model: str
    tokens_used: int
    response_time: float
    success: bool
    error: Optional[str] = None

class LlamaService:
    """LLM —Å–µ—Ä–≤–∏—Å —Å –ù–û–í–´–ú HuggingFace Inference Providers API"""
    
    def __init__(self):
        self.service_type = "huggingface_inference_providers"
        self.model_name = os.getenv("LLM_MODEL", "meta-llama/Llama-3.1-8B-Instruct")
        self.hf_token = os.getenv("HF_TOKEN")
        self.ready = True
        
        # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –Ω–æ–≤—ã—Ö API endpoints
        self.api_endpoints = {
            # –ù–æ–≤—ã–π Inference Providers API (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)
            "providers": "https://api-inference.huggingface.co/models/",
            # Fallback endpoints –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –º–æ–¥–µ–ª–µ–π
            "text_generation": "https://api-inference.huggingface.co/models/",
            "chat": "https://api-inference.huggingface.co/models/"
        }
        
        # –ü—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã–µ —Ä–∞–±–æ—á–∏–µ –º–æ–¥–µ–ª–∏ (–∏—é–Ω—å 2025)
        self.working_models = [
            "microsoft/DialoGPT-small",    # –ß–∞—Ç –º–æ–¥–µ–ª—å (–º–µ–Ω—å—à–µ medium)
            "gpt2",                        # –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å OpenAI
            "distilgpt2",                  # –ö–æ–º–ø–∞–∫—Ç–Ω–∞—è –≤–µ—Ä—Å–∏—è
            "EleutherAI/gpt-neo-125M",    # EleutherAI –º–æ–¥–µ–ª—å
            "bigscience/bloom-560m"        # BLOOM –º–æ–¥–µ–ª—å
        ]
        
        if not self.hf_token:
            logger.warning("‚ö†Ô∏è HF_TOKEN not set - using public inference (rate limited)")
        else:
            logger.info("‚úÖ HF_TOKEN configured for Llama service")
        
        logger.info(f"ü¶ô Llama service initialized: {self.model_name}")
        logger.info(f"üîÑ API endpoint: {self.api_endpoints['providers']}")
    
    async def answer_legal_question(self, question: str, context_documents: List[Dict], language: str = "en"):
        """–û—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–π –≤–æ–ø—Ä–æ—Å —Å –ù–û–í–´–ú API –∏ fallback –º–æ–¥–µ–ª—è–º–∏"""
        start_time = time.time()
        
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º demo —Ä–µ–∂–∏–º
            if os.getenv("LLM_DEMO_MODE", "false").lower() == "true":
                return self._generate_demo_response(question, context_documents, language, start_time)
            
            # –°—Ç—Ä–æ–∏–º –ø—Ä–æ–º–ø—Ç
            prompt = self._build_prompt(question, context_documents, language)
            
            # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ü—Ä–æ–±—É–µ–º –º–æ–¥–µ–ª–∏ –ø–æ –æ—á–µ—Ä–µ–¥–∏ –¥–æ —É—Å–ø–µ—Ö–∞
            models_to_try = [self.model_name] + self.working_models
            
            for model_attempt, model in enumerate(models_to_try):
                try:
                    logger.info(f"üîÑ Trying model {model_attempt + 1}/{len(models_to_try)}: {model}")
                    
                    response = await self._generate_with_model(prompt, model)
                    
                    if response.success:
                        logger.info(f"‚úÖ Success with model: {model}")
                        return response
                    else:
                        logger.warning(f"‚ùå Model {model} failed: {response.error}")
                        
                except Exception as e:
                    logger.warning(f"‚ùå Model {model} exception: {e}")
                    continue
            
            # –ï—Å–ª–∏ –≤—Å–µ –º–æ–¥–µ–ª–∏ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–∏ - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º fallback
            logger.error("‚ùå All models failed, returning fallback response")
            return self._generate_error_response(question, language, "All models unavailable", start_time)
            
        except Exception as e:
            logger.error(f"‚ùå General error in answer_legal_question: {e}")
            return self._generate_error_response(question, language, str(e), start_time)
    
    async def _generate_with_model(self, prompt: str, model: str) -> LlamaResponse:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç —Å –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –º–æ–¥–µ–ª—å—é –∏—Å–ø–æ–ª—å–∑—É—è –ù–û–í–´–ô API"""
        start_time = time.time()
        
        try:
            # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ü–æ–ø—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ API endpoints
            endpoints_to_try = [
                f"https://api-inference.huggingface.co/models/{model}",
                f"https://api-inference.huggingface.co/pipeline/text-generation/{model}",
            ]
            
            for endpoint in endpoints_to_try:
                try:
                    # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º requests –∑–¥–µ—Å—å –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –ø—Ä–æ–±–ª–µ–º
                    import requests
                    
                    headers = {
                        "Authorization": f"Bearer {self.hf_token}" if self.hf_token else "",
                        "Content-Type": "application/json"
                    }
                    
                    # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –£–ø—Ä–æ—â–µ–Ω–Ω—ã–π payload –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
                    if "chat" in model.lower() or "dialog" in model.lower():
                        # –î–ª—è chat –º–æ–¥–µ–ª–µ–π
                        payload = {
                            "inputs": prompt,
                            "parameters": {
                                "max_new_tokens": 150,
                                "temperature": 0.3,
                                "do_sample": True,
                                "return_full_text": False
                            }
                        }
                    else:
                        # –î–ª—è –æ–±—ã—á–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
                        payload = {
                            "inputs": prompt,
                            "parameters": {
                                "max_new_tokens": 150,
                                "temperature": 0.3,
                                "return_full_text": False
                            }
                        }
                    
                    # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –°–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å —Å —Ç–∞–π–º–∞—É—Ç–æ–º
                    response = requests.post(
                        endpoint,
                        headers=headers,
                        json=payload,
                        timeout=30
                    )
                    
                    if response.status_code == 200:
                        data = response.json()
                        
                        # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤ –æ—Ç–≤–µ—Ç–∞
                        if isinstance(data, list) and len(data) > 0:
                            # –§–æ—Ä–º–∞—Ç: [{"generated_text": "..."}]
                            content = data[0].get("generated_text", "")
                        elif isinstance(data, dict):
                            # –§–æ—Ä–º–∞—Ç: {"generated_text": "..."}
                            content = data.get("generated_text", data.get("text", ""))
                        else:
                            content = str(data)
                        
                        # –û—á–∏—â–∞–µ–º –æ—Ç–≤–µ—Ç –æ—Ç –ø—Ä–æ–º–ø—Ç–∞
                        if content.startswith(prompt):
                            content = content[len(prompt):].strip()
                        
                        content = self._clean_response(content)
                        
                        if content and len(content.strip()) > 5:
                            response_time = time.time() - start_time
                            
                            return LlamaResponse(
                                content=content,
                                model=model,
                                tokens_used=len(content.split()),
                                response_time=response_time,
                                success=True
                            )
                    
                    else:
                        logger.debug(f"HTTP {response.status_code} for {endpoint}")
                        
                except requests.exceptions.RequestException as e:
                    logger.debug(f"Request error for {endpoint}: {e}")
                    continue
                except Exception as e:
                    logger.debug(f"Error with {endpoint}: {e}")
                    continue
            
            # –ï—Å–ª–∏ –≤—Å–µ endpoints –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–∏
            return LlamaResponse(
                content="",
                model=model,
                tokens_used=0,
                response_time=time.time() - start_time,
                success=False,
                error="All endpoints failed"
            )
            
        except ImportError:
            logger.error("‚ùå requests library not available")
            return LlamaResponse(
                content="",
                model=model,
                tokens_used=0,
                response_time=time.time() - start_time,
                success=False,
                error="Missing requests library"
            )
        except Exception as e:
            logger.error(f"‚ùå Model generation error: {e}")
            return LlamaResponse(
                content="",
                model=model,
                tokens_used=0,
                response_time=time.time() - start_time,
                success=False,
                error=str(e)
            )
    
    def _build_prompt(self, question: str, context_documents: List[Dict], language: str) -> str:
        """–°—Ç—Ä–æ–∏—Ç –ø—Ä–æ–º–ø—Ç –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –º–æ–¥–µ–ª–µ–π"""
        
        # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –û—á–µ–Ω—å –ø—Ä–æ—Å—Ç–æ–π –ø—Ä–æ–º–ø—Ç –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
        if context_documents:
            # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–π –¥–æ–∫—É–º–µ–Ω—Ç –∏ —Å–∏–ª—å–Ω–æ —Å–æ–∫—Ä–∞—â–∞–µ–º
            doc = context_documents[0]
            context = doc.get('content', '')[:200]  # –ú–∞–∫—Å–∏–º—É–º 200 —Å–∏–º–≤–æ–ª–æ–≤
            
            if language == "uk":
                prompt = f"–ö–æ–Ω—Ç–µ–∫—Å—Ç: {context}\n\n–ü–∏—Ç–∞–Ω–Ω—è: {question}\n–í—ñ–¥–ø–æ–≤—ñ–¥—å:"
            else:
                prompt = f"Context: {context}\n\nQuestion: {question}\nAnswer:"
        else:
            if language == "uk":
                prompt = f"–ü–∏—Ç–∞–Ω–Ω—è: {question}\n–í—ñ–¥–ø–æ–≤—ñ–¥—å:"
            else:
                prompt = f"Question: {question}\nAnswer:"
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –æ–±—â—É—é –¥–ª–∏–Ω—É –ø—Ä–æ–º–ø—Ç–∞
        if len(prompt) > 300:
            prompt = prompt[:300] + "..."
        
        return prompt
    
    def _clean_response(self, response: str) -> str:
        """–û—á–∏—â–∞–µ—Ç –æ—Ç–≤–µ—Ç –æ—Ç –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤"""
        if not response:
            return "I need more information to provide a proper legal analysis."
        
        # –ë–∞–∑–æ–≤–∞—è –æ—á–∏—Å—Ç–∫–∞
        response = response.strip()
        
        # –£–¥–∞–ª—è–µ–º –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è –ø—Ä–æ–º–ø—Ç–∞
        lines = response.split('\n')
        cleaned_lines = []
        for line in lines:
            line = line.strip()
            if line and not line.endswith(':') and len(line) > 3:
                cleaned_lines.append(line)
                if len(cleaned_lines) >= 3:  # –ú–∞–∫—Å–∏–º—É–º 3 —Å—Ç—Ä–æ–∫–∏
                    break
        
        cleaned = ' '.join(cleaned_lines)
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É –æ—Ç–≤–µ—Ç–∞
        if len(cleaned) > 400:
            cleaned = cleaned[:400] + "..."
        
        return cleaned or "Unable to generate response."
    
    def _generate_demo_response(self, question: str, context_documents: List[Dict], language: str, start_time: float):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç demo –æ—Ç–≤–µ—Ç"""
        if language == "uk":
            content = f"""ü§ñ **Demo —Ä–µ–∂–∏–º –∞–∫—Ç–∏–≤–Ω–∏–π** (LLM_DEMO_MODE=true)

**–í–∞—à–µ –ø–∏—Ç–∞–Ω–Ω—è:** {question}

üìö –ó–Ω–∞–π–¥–µ–Ω–æ {len(context_documents)} –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤ —É –±–∞–∑—ñ –∑–Ω–∞–Ω—å.

üí° **Demo –≤—ñ–¥–ø–æ–≤—ñ–¥—å:** –¶–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü—ñ–π–Ω–∏–π —Ä–µ–∂–∏–º. –£ —Ä–æ–±–æ—á–æ–º—É —Ä–µ–∂–∏–º—ñ —Ç—É—Ç –±—É–¥–µ –¥–µ—Ç–∞–ª—å–Ω–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥—å –≤—ñ–¥ —Ä–µ–∞–ª—å–Ω–æ–≥–æ AI –Ω–∞ –æ—Å–Ω–æ–≤—ñ –≤–∞—à–∏—Ö —é—Ä–∏–¥–∏—á–Ω–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤.

üîß **–î–ª—è –∞–∫—Ç–∏–≤–∞—Ü—ñ—ó –ø–æ–≤–Ω–æ—ó –≤–µ—Ä—Å—ñ—ó:**
‚Ä¢ –í—Å—Ç–∞–Ω–æ–≤—ñ—Ç—å LLM_DEMO_MODE=false
‚Ä¢ –ü–µ—Ä–µ–∫–æ–Ω–∞–π—Ç–µ—Å—å —â–æ HF_TOKEN –Ω–∞–ª–∞—à—Ç–æ–≤–∞–Ω–∏–π

üìÑ **–ó–Ω–∞–π–¥–µ–Ω—ñ –¥–æ–∫—É–º–µ–Ω—Ç–∏:** {', '.join([doc.get('filename', 'Unknown') for doc in context_documents[:3]])}"""
        else:
            content = f"""ü§ñ **Demo Mode Active** (LLM_DEMO_MODE=true)

**Your Question:** {question}

üìö Found {len(context_documents)} documents in knowledge base.

üí° **Demo Response:** This is demonstration mode. In production mode, you would get detailed answers from real AI based on your legal documents.

üîß **To activate full version:**
‚Ä¢ Set LLM_DEMO_MODE=false  
‚Ä¢ Ensure HF_TOKEN is configured

üìÑ **Found documents:** {', '.join([doc.get('filename', 'Unknown') for doc in context_documents[:3]])}"""
        
        return LlamaResponse(
            content=content,
            model="demo_mode",
            tokens_used=len(content.split()),
            response_time=time.time() - start_time,
            success=True
        )
    
    def _generate_error_response(self, question: str, language: str, error: str, start_time: float):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –ø—Ä–∏ –æ—à–∏–±–∫–µ"""
        
        if language == "uk":
            content = f"""‚ùå **–ü–æ–º–∏–ª–∫–∞ LLM —Å–µ—Ä–≤—ñ—Å—É**

**–í–∞—à–µ –ø–∏—Ç–∞–Ω–Ω—è:** {question}

üîß **–ü—Ä–æ–±–ª–µ–º–∞:** {error}

üí° **–ú–æ–∂–ª–∏–≤—ñ —Ä—ñ—à–µ–Ω–Ω—è:**
‚Ä¢ HuggingFace API –º–æ–∂–µ –±—É—Ç–∏ –ø–µ—Ä–µ–≤–∞–Ω—Ç–∞–∂–µ–Ω–∏–π
‚Ä¢ –°–ø—Ä–æ–±—É–π—Ç–µ —â–µ —Ä–∞–∑ —á–µ—Ä–µ–∑ –∫—ñ–ª—å–∫–∞ —Ö–≤–∏–ª–∏–Ω  
‚Ä¢ –ü–µ—Ä–µ–≤—ñ—Ä—Ç–µ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è HF_TOKEN
‚Ä¢ –ê–±–æ –≤–∫–ª—é—á—ñ—Ç—å demo —Ä–µ–∂–∏–º: LLM_DEMO_MODE=true

üîÑ **–°—Ç–∞—Ç—É—Å:** –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ —Å–ø—Ä–æ–±—É–≤–∞–ª–∏ –∫—ñ–ª—å–∫–∞ –º–æ–¥–µ–ª–µ–π"""
        else:
            content = f"""‚ùå **LLM Service Error**

**Your Question:** {question}

üîß **Issue:** {error}

üí° **Possible Solutions:**
‚Ä¢ HuggingFace API may be overloaded
‚Ä¢ Try again in a few minutes
‚Ä¢ Check HF_TOKEN configuration  
‚Ä¢ Or enable demo mode: LLM_DEMO_MODE=true

üîÑ **Status:** Automatically tried multiple models"""
        
        return LlamaResponse(
            content=content,
            model="error_fallback",
            tokens_used=len(content.split()),
            response_time=time.time() - start_time,
            success=False,
            error=error
        )
    
    async def get_service_status(self):
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç—É—Å —Å–µ—Ä–≤–∏—Å–∞"""
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å requests
        try:
            import requests
            requests_available = True
        except ImportError:
            requests_available = False
        
        return {
            "service_type": self.service_type,
            "model_name": self.model_name,
            "ready": self.ready,
            "hf_token_configured": bool(self.hf_token),
            "demo_mode": os.getenv("LLM_DEMO_MODE", "false").lower() == "true",
            "requests_available": requests_available,
            "api_endpoints": self.api_endpoints,
            "working_models": self.working_models,
            "fallback_enabled": True,
            "recommendations": [
                "New HuggingFace Inference Providers API implemented",
                "Multiple fallback models configured",
                "Set HF_TOKEN for better rate limits",
                "Use LLM_DEMO_MODE=true for testing without API calls",
                "If all models fail, check HuggingFace status"
            ]
        }

def create_llama_service():
    """–°–æ–∑–¥–∞–µ—Ç –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π Llama —Å–µ—Ä–≤–∏—Å"""
    try:
        return LlamaService()
    except Exception as e:
        logger.error(f"‚ùå Failed to create Llama service: {e}")
        return create_fallback_service()

def create_fallback_service():
    """–°–æ–∑–¥–∞–µ—Ç fallback —Å–µ—Ä–≤–∏—Å"""
    class FallbackService:
        def __init__(self):
            self.service_type = "fallback"
            self.ready = True
        
        async def answer_legal_question(self, question: str, context_documents: list, language: str = "en"):
            if language == "uk":
                content = f"""üîÑ **–°–∏—Å—Ç–µ–º–∞ —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑—É—î—Ç—å—Å—è**

**–í–∞—à–µ –ø–∏—Ç–∞–Ω–Ω—è:** {question}

‚ö†Ô∏è LLM —Å–µ—Ä–≤—ñ—Å –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∏–π. 

üìö –ó–Ω–∞–π–¥–µ–Ω–æ {len(context_documents)} –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤ —É –±–∞–∑—ñ –∑–Ω–∞–Ω—å.

üîß **–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó:**
‚Ä¢ –ü–µ—Ä–µ–≤—ñ—Ä—Ç–µ HF_TOKEN –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è
‚Ä¢ –°–ø—Ä–æ–±—É–π—Ç–µ demo —Ä–µ–∂–∏–º: LLM_DEMO_MODE=true
‚Ä¢ HuggingFace API –º–æ–∂–µ –±—É—Ç–∏ —Ç–∏–º—á–∞—Å–æ–≤–æ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∏–π"""
            else:
                content = f"""üîÑ **System Initializing**

**Your Question:** {question}

‚ö†Ô∏è LLM service unavailable.

üìö Found {len(context_documents)} documents in database.

üîß **Recommendations:**
‚Ä¢ Check HF_TOKEN configuration
‚Ä¢ Try demo mode: LLM_DEMO_MODE=true  
‚Ä¢ HuggingFace API may be temporarily unavailable"""
            
            return LlamaResponse(
                content=content,
                model="fallback",
                tokens_used=len(content.split()),
                response_time=0.1,
                success=True
            )
        
        async def get_service_status(self):
            return {
                "service_type": "fallback",
                "ready": True,
                "recommendations": [
                    "Install requests: pip install requests",
                    "Configure HF_TOKEN in environment variables",
                    "Check HuggingFace API status"
                ]
            }
    
    return FallbackService()