# backend/services/llama_service.py - –ü–û–õ–ù–´–ô –ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ô LLM –°–ï–†–í–ò–°
"""
Llama LLM Service using HuggingFace Inference API —Å retry –ª–æ–≥–∏–∫–æ–π
"""

import logging
import time
import os
import asyncio
from typing import List, Dict, Optional
from dataclasses import dataclass

logger = logging.getLogger(__name__)

@dataclass
class LlamaResponse:
    content: str
    model: str
    tokens_used: int
    response_time: float
    success: bool
    error: Optional[str] = None

class LlamaService:
    """LLM —Å–µ—Ä–≤–∏—Å —Å HuggingFace Inference API –∏ retry –ª–æ–≥–∏–∫–æ–π"""
    
    def __init__(self):
        self.service_type = "huggingface_inference"
        self.model_name = os.getenv("LLM_MODEL", "meta-llama/Llama-3.1-8B-Instruct")
        self.hf_token = os.getenv("HF_TOKEN")
        self.ready = True
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ç–æ–∫–µ–Ω–∞
        if not self.hf_token:
            logger.warning("‚ö†Ô∏è HF_TOKEN not set - using public inference (rate limited)")
        else:
            logger.info("‚úÖ HF_TOKEN configured for Llama service")
        
        logger.info(f"ü¶ô Llama service initialized: {self.model_name}")
    
    async def answer_legal_question(self, question: str, context_documents: List[Dict], language: str = "en"):
        """–û—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–π –≤–æ–ø—Ä–æ—Å —Å retry –ª–æ–≥–∏–∫–æ–π"""
        start_time = time.time()
        
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º demo —Ä–µ–∂–∏–º
            if os.getenv("LLM_DEMO_MODE", "false").lower() == "true":
                return self._generate_demo_response(question, context_documents, language, start_time)
            
            # –°—Ç—Ä–æ–∏–º –ø—Ä–æ–º–ø—Ç
            prompt = self._build_prompt(question, context_documents, language)
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç —á–µ—Ä–µ–∑ HuggingFace API —Å retry
            try:
                from huggingface_hub import InferenceClient
                
                client = InferenceClient(
                    model=self.model_name,
                    token=self.hf_token
                )
                
                # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç —Å retry –ª–æ–≥–∏–∫–æ–π
                max_retries = 2
                response = None
                
                for attempt in range(max_retries):
                    try:
                        logger.info(f"üîÑ Attempt {attempt + 1}/{max_retries} for LLM generation...")
                        
                        response = client.text_generation(
                            prompt,
                            max_new_tokens=200,  # –£–º–µ–Ω—å—à–µ–Ω–æ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
                            temperature=0.3,
                            do_sample=True,
                            return_full_text=False,
                            timeout=60  # 60 —Å–µ–∫—É–Ω–¥ —Ç–∞–π–º–∞—É—Ç
                        )
                        
                        logger.info(f"‚úÖ LLM generation successful on attempt {attempt + 1}")
                        break  # –£—Å–ø–µ—à–Ω–æ - –≤—ã—Ö–æ–¥–∏–º –∏–∑ —Ü–∏–∫–ª–∞
                        
                    except Exception as retry_error:
                        if attempt < max_retries - 1:
                            logger.warning(f"‚ùå Attempt {attempt + 1} failed: {retry_error}, retrying in 5s...")
                            await asyncio.sleep(5)  # –ü–∞—É–∑–∞ –ø–µ—Ä–µ–¥ –ø–æ–≤—Ç–æ—Ä–æ–º
                            continue
                        else:
                            logger.error(f"‚ùå All {max_retries} attempts failed: {retry_error}")
                            raise retry_error  # –ü–æ—Å–ª–µ–¥–Ω—è—è –ø–æ–ø—ã—Ç–∫–∞ - –ø–æ–¥–Ω–∏–º–∞–µ–º –æ—à–∏–±–∫—É
                
                # –û—á–∏—â–∞–µ–º –æ—Ç–≤–µ—Ç
                if isinstance(response, str):
                    content = response.strip()
                else:
                    content = str(response).strip()
                
                # –ë–∞–∑–æ–≤–∞—è –æ—á–∏—Å—Ç–∫–∞ –æ—Ç–≤–µ—Ç–∞
                content = self._clean_response(content)
                
                response_time = time.time() - start_time
                
                return LlamaResponse(
                    content=content,
                    model=self.model_name,
                    tokens_used=len(content.split()),
                    response_time=response_time,
                    success=True
                )
                
            except ImportError:
                logger.error("‚ùå huggingface_hub not installed")
                return self._generate_error_response(question, language, "Missing huggingface_hub", start_time)
            
            except Exception as e:
                logger.error(f"‚ùå HuggingFace API error: {e}")
                return self._generate_error_response(question, language, str(e), start_time)
        
        except Exception as e:
            logger.error(f"‚ùå General error in answer_legal_question: {e}")
            return self._generate_error_response(question, language, str(e), start_time)
    
    def _build_prompt(self, question: str, context_documents: List[Dict], language: str) -> str:
        """–°—Ç—Ä–æ–∏—Ç –ø—Ä–æ–º–ø—Ç –¥–ª—è Llama"""
        
        if language == "uk":
            system_prompt = "–¢–∏ —é—Ä–∏–¥–∏—á–Ω–∏–π –∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç. –í—ñ–¥–ø–æ–≤—ñ–¥–∞–π –∫–æ—Ä–æ—Ç–∫–æ —ñ –ø–æ —Å—É—Ç—ñ –Ω–∞ –æ—Å–Ω–æ–≤—ñ –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤."
            context_intro = "–î–æ–∫—É–º–µ–Ω—Ç–∏:"
            answer_intro = f"–ü–∏—Ç–∞–Ω–Ω—è: {question}\n–í—ñ–¥–ø–æ–≤—ñ–¥—å:"
        else:
            system_prompt = "You are a legal consultant. Answer concisely based on the provided documents."
            context_intro = "Documents:"
            answer_intro = f"Question: {question}\nAnswer:"
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç (–æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏)
        context_parts = []
        for i, doc in enumerate(context_documents[:2]):  # –ú–∞–∫—Å–∏–º—É–º 2 –¥–æ–∫—É–º–µ–Ω—Ç–∞
            filename = doc.get('filename', f'Doc {i+1}')
            content = doc.get('content', '')[:400]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ 400 —Å–∏–º–≤–æ–ª–æ–≤
            context_parts.append(f"{filename}: {content}")
        
        context = f"\n{context_intro}\n" + "\n".join(context_parts) if context_parts else ""
        
        # –°–æ–±–∏—Ä–∞–µ–º –ø—Ä–æ–º–ø—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ Llama-3.1
        prompt = f"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n{system_prompt}<|eot_id|>\n"
        
        if context:
            prompt += f"<|start_header_id|>user<|end_header_id|>\n{context}\n\n{answer_intro}<|eot_id|>\n"
        else:
            prompt += f"<|start_header_id|>user<|end_header_id|>\n{answer_intro}<|eot_id|>\n"
        
        prompt += "<|start_header_id|>assistant<|end_header_id|>\n"
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –æ–±—â—É—é –¥–ª–∏–Ω—É –ø—Ä–æ–º–ø—Ç–∞
        max_prompt_length = 1200
        if len(prompt) > max_prompt_length:
            prompt = prompt[:max_prompt_length] + "..."
        
        return prompt
    
    def _clean_response(self, response: str) -> str:
        """–û—á–∏—â–∞–µ—Ç –æ—Ç–≤–µ—Ç –æ—Ç –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤"""
        if not response:
            return "I need more information to provide a proper legal analysis."
        
        # –£–¥–∞–ª—è–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã –µ—Å–ª–∏ –ø–æ–ø–∞–ª–∏ –≤ –æ—Ç–≤–µ—Ç
        response = response.replace("<|eot_id|>", "")
        response = response.replace("<|end_header_id|>", "")
        response = response.replace("<|start_header_id|>", "")
        
        # –ë–∞–∑–æ–≤–∞—è –æ—á–∏—Å—Ç–∫–∞
        response = response.strip()
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É –æ—Ç–≤–µ—Ç–∞
        if len(response) > 800:
            response = response[:800] + "..."
        
        return response
    
    def _generate_demo_response(self, question: str, context_documents: List[Dict], language: str, start_time: float):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç demo –æ—Ç–≤–µ—Ç"""
        if language == "uk":
            content = f"""ü§ñ **Demo —Ä–µ–∂–∏–º –∞–∫—Ç–∏–≤–Ω–∏–π**

**–í–∞—à–µ –ø–∏—Ç–∞–Ω–Ω—è:** {question}

üìö –ó–Ω–∞–π–¥–µ–Ω–æ {len(context_documents)} –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤ —É –±–∞–∑—ñ –∑–Ω–∞–Ω—å.

üí° **Demo –≤—ñ–¥–ø–æ–≤—ñ–¥—å:** –¶–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü—ñ–π–Ω–∏–π —Ä–µ–∂–∏–º Legal Assistant. –£ —Ä–æ–±–æ—á–æ–º—É —Ä–µ–∂–∏–º—ñ —Ç—É—Ç –±—É–¥–µ –¥–µ—Ç–∞–ª—å–Ω–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥—å –≤—ñ–¥ Llama-3.1-8B-Instruct –Ω–∞ –æ—Å–Ω–æ–≤—ñ –≤–∞—à–∏—Ö —é—Ä–∏–¥–∏—á–Ω–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤.

üîß **–î–ª—è –∞–∫—Ç–∏–≤–∞—Ü—ñ—ó –ø–æ–≤–Ω–æ—ó –≤–µ—Ä—Å—ñ—ó:**
1. –û—Ç—Ä–∏–º–∞–π—Ç–µ —Ç–æ–∫–µ–Ω –Ω–∞ https://huggingface.co/settings/tokens
2. –í—Å—Ç–∞–Ω–æ–≤—ñ—Ç—å –∑–º—ñ–Ω–Ω—É HF_TOKEN —É –Ω–∞—Å—Ç—Ä–æ–π–∫–∞—Ö Space
3. –í—Å—Ç–∞–Ω–æ–≤—ñ—Ç—å LLM_DEMO_MODE=false

üìÑ **–ó–Ω–∞–π–¥–µ–Ω—ñ –¥–æ–∫—É–º–µ–Ω—Ç–∏:** {', '.join([doc.get('filename', 'Unknown') for doc in context_documents[:3]])}"""
        else:
            content = f"""ü§ñ **Demo Mode Active**

**Your Question:** {question}

üìö Found {len(context_documents)} documents in knowledge base.

üí° **Demo Response:** This is Legal Assistant demo mode. In production mode, you would get detailed answers from Llama-3.1-8B-Instruct based on your legal documents.

üîß **To activate full version:**
1. Get token at https://huggingface.co/settings/tokens
2. Set HF_TOKEN variable in Space settings
3. Set LLM_DEMO_MODE=false

üìÑ **Found documents:** {', '.join([doc.get('filename', 'Unknown') for doc in context_documents[:3]])}"""
        
        return LlamaResponse(
            content=content,
            model="demo_mode",
            tokens_used=len(content.split()),
            response_time=time.time() - start_time,
            success=True
        )
    
    def _generate_error_response(self, question: str, language: str, error: str, start_time: float):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –ø—Ä–∏ –æ—à–∏–±–∫–µ"""
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –æ—à–∏–±–∫–∏ –¥–ª—è –ª—É—á—à–∏—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
        if "504" in error or "timeout" in error.lower():
            error_type = "timeout"
        elif "503" in error or "overloaded" in error.lower():
            error_type = "overloaded"
        elif "401" in error or "token" in error.lower():
            error_type = "auth"
        else:
            error_type = "general"
        
        if language == "uk":
            if error_type == "timeout":
                content = f"""‚è∞ **–¢–∞–π–º–∞—É—Ç HuggingFace API**

**–í–∞—à–µ –ø–∏—Ç–∞–Ω–Ω—è:** {question}

üîß **–ü—Ä–æ–±–ª–µ–º–∞:** –°–µ—Ä–≤–µ—Ä HuggingFace –ø–µ—Ä–µ–≤–∞–Ω—Ç–∞–∂–µ–Ω–∏–π (504 Gateway Timeout)

üí° **–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó:**
‚Ä¢ –°–ø—Ä–æ–±—É–π—Ç–µ —â–µ —Ä–∞–∑ —á–µ—Ä–µ–∑ 1-2 —Ö–≤–∏–ª–∏–Ω–∏
‚Ä¢ –£–≤—ñ–º–∫–Ω—ñ—Ç—å demo —Ä–µ–∂–∏–º: LLM_DEMO_MODE=true
‚Ä¢ HuggingFace API –º–æ–∂–µ –±—É—Ç–∏ –ø–µ—Ä–µ–≤–∞–Ω—Ç–∞–∂–µ–Ω–∏–π —É –≥–æ–¥–∏–Ω–∏ –ø—ñ–∫

üîÑ **–°—Ç–∞—Ç—É—Å:** –°–∏—Å—Ç–µ–º–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –ø–æ–≤—Ç–æ—Ä–∏–ª–∞ –∑–∞–ø–∏—Ç 2 —Ä–∞–∑–∏"""
            elif error_type == "auth":
                content = f"""üîë **–ü–æ–º–∏–ª–∫–∞ –∞–≤—Ç–µ–Ω—Ç–∏—Ñ—ñ–∫–∞—Ü—ñ—ó**

**–í–∞—à–µ –ø–∏—Ç–∞–Ω–Ω—è:** {question}

üîß **–ü—Ä–æ–±–ª–µ–º–∞:** –ü–µ—Ä–µ–≤—ñ—Ä—Ç–µ HF_TOKEN

üí° **–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó:**
‚Ä¢ –û—Ç—Ä–∏–º–∞–π—Ç–µ –Ω–æ–≤–∏–π —Ç–æ–∫–µ–Ω: https://huggingface.co/settings/tokens
‚Ä¢ –í—Å—Ç–∞–Ω–æ–≤—ñ—Ç—å HF_TOKEN —É –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è—Ö Space
‚Ä¢ –ê–±–æ —É–≤—ñ–º–∫–Ω—ñ—Ç—å demo —Ä–µ–∂–∏–º: LLM_DEMO_MODE=true"""
            else:
                content = f"""‚ùå **–ü–æ–º–∏–ª–∫–∞ LLM —Å–µ—Ä–≤—ñ—Å—É**

**–í–∞—à–µ –ø–∏—Ç–∞–Ω–Ω—è:** {question}

üîß **–ü–æ–º–∏–ª–∫–∞:** {error}

üí° **–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó:**
‚Ä¢ –°–ø—Ä–æ–±—É–π—Ç–µ demo —Ä–µ–∂–∏–º: LLM_DEMO_MODE=true
‚Ä¢ –ü–µ—Ä–µ–≤—ñ—Ä—Ç–µ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è HF_TOKEN
‚Ä¢ –°–ø—Ä–æ–±—É–π—Ç–µ —â–µ —Ä–∞–∑ —á–µ—Ä–µ–∑ –∫—ñ–ª—å–∫–∞ —Ö–≤–∏–ª–∏–Ω"""
        else:
            if error_type == "timeout":
                content = f"""‚è∞ **HuggingFace API Timeout**

**Your Question:** {question}

üîß **Issue:** HuggingFace server overloaded (504 Gateway Timeout)

üí° **Recommendations:**
‚Ä¢ Try again in 1-2 minutes
‚Ä¢ Enable demo mode: LLM_DEMO_MODE=true
‚Ä¢ HuggingFace API may be overloaded during peak hours

üîÑ **Status:** System automatically retried request 2 times"""
            elif error_type == "auth":
                content = f"""üîë **Authentication Error**

**Your Question:** {question}

üîß **Issue:** Check HF_TOKEN configuration

üí° **Recommendations:**
‚Ä¢ Get new token: https://huggingface.co/settings/tokens
‚Ä¢ Set HF_TOKEN in Space settings
‚Ä¢ Or enable demo mode: LLM_DEMO_MODE=true"""
            else:
                content = f"""‚ùå **LLM Service Error**

**Your Question:** {question}

üîß **Error:** {error}

üí° **Recommendations:**
‚Ä¢ Try demo mode: LLM_DEMO_MODE=true
‚Ä¢ Check HF_TOKEN configuration
‚Ä¢ Try again in a few minutes"""
        
        return LlamaResponse(
            content=content,
            model="error_fallback",
            tokens_used=len(content.split()),
            response_time=time.time() - start_time,
            success=False,
            error=error
        )
    
    async def get_service_status(self):
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç—É—Å —Å–µ—Ä–≤–∏—Å–∞"""
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å HuggingFace
        hf_available = False
        try:
            from huggingface_hub import InferenceClient
            hf_available = True
        except ImportError:
            pass
        
        return {
            "service_type": self.service_type,
            "model_name": self.model_name,
            "ready": self.ready,
            "hf_token_configured": bool(self.hf_token),
            "demo_mode": os.getenv("LLM_DEMO_MODE", "false").lower() == "true",
            "huggingface_hub_available": hf_available,
            "retry_enabled": True,
            "max_retries": 2,
            "timeout": 60,
            "recommendations": [
                "Set HF_TOKEN for better rate limits and priority access",
                "Use LLM_DEMO_MODE=true for testing without API calls",
                "Check https://status.huggingface.co/ for service status",
                "Visit https://huggingface.co/settings/tokens for token management"
            ]
        }

def create_llama_service():
    """–°–æ–∑–¥–∞–µ—Ç Llama —Å–µ—Ä–≤–∏—Å"""
    try:
        return LlamaService()
    except Exception as e:
        logger.error(f"‚ùå Failed to create Llama service: {e}")
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º fallback
        return create_fallback_service()

def create_fallback_service():
    """–°–æ–∑–¥–∞–µ—Ç fallback —Å–µ—Ä–≤–∏—Å"""
    class FallbackService:
        def __init__(self):
            self.service_type = "fallback"
            self.ready = True
        
        async def answer_legal_question(self, question: str, context_documents: list, language: str = "en"):
            if language == "uk":
                content = f"""üîÑ **–°–∏—Å—Ç–µ–º–∞ —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑—É—î—Ç—å—Å—è**

**–í–∞—à–µ –ø–∏—Ç–∞–Ω–Ω—è:** {question}

‚ö†Ô∏è LLM —Å–µ—Ä–≤—ñ—Å –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∏–π. –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó:
1. –í—Å—Ç–∞–Ω–æ–≤—ñ—Ç—å HF_TOKEN - —Ç–æ–∫–µ–Ω –≤—ñ–¥ HuggingFace
2. –ê–±–æ —É–≤—ñ–º–∫–Ω—ñ—Ç—å LLM_DEMO_MODE=true –¥–ª—è –¥–µ–º–æ —Ä–µ–∂–∏–º—É

üìö –ó–Ω–∞–π–¥–µ–Ω–æ {len(context_documents)} –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤ —É –±–∞–∑—ñ –∑–Ω–∞–Ω—å.

üîß **–ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è:**
‚Ä¢ HF_TOKEN: –æ—Ç—Ä–∏–º–∞–π—Ç–µ –Ω–∞ https://huggingface.co/settings/tokens
‚Ä¢ LLM_DEMO_MODE: –≤—Å—Ç–∞–Ω–æ–≤—ñ—Ç—å —É –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è—Ö HuggingFace Space"""
            else:
                content = f"""üîÑ **System Initializing**

**Your Question:** {question}

‚ö†Ô∏è LLM service unavailable. Recommendations:
1. Set HF_TOKEN - HuggingFace token
2. Or enable LLM_DEMO_MODE=true for demo mode

üìö Found {len(context_documents)} documents in database.

üîß **Configuration:**
‚Ä¢ HF_TOKEN: get at https://huggingface.co/settings/tokens
‚Ä¢ LLM_DEMO_MODE: set in HuggingFace Space settings"""
            
            return LlamaResponse(
                content=content,
                model="fallback",
                tokens_used=len(content.split()),
                response_time=0.1,
                success=True
            )
        
        async def get_service_status(self):
            return {
                "service_type": "fallback", 
                "ready": True,
                "recommendations": [
                    "Install huggingface_hub: pip install huggingface_hub",
                    "Configure HF_TOKEN in environment variables",
                    "Enable LLM_DEMO_MODE for testing"
                ]
            }
    
    return FallbackService()