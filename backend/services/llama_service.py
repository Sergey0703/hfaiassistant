# backend/services/llama_service.py - –ò–°–ü–†–ê–í–õ–ï–ù –ü–û–î –ù–û–í–´–ô HUGGINGFACE API
"""
Llama LLM Service using NEW HuggingFace Inference Providers API
–ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –°—Ç–∞—Ä—ã–π API –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç, –ø–µ—Ä–µ—Ö–æ–¥ –Ω–∞ –Ω–æ–≤—ã–π Inference Providers
–ü–æ–¥–¥–µ—Ä–∂–∫–∞ flan-t5 –º–æ–¥–µ–ª–µ–π (encoder-decoder)
"""

import logging
import time
import os
import asyncio
from typing import List, Dict, Optional
from dataclasses import dataclass

logger = logging.getLogger(__name__)

@dataclass
class LlamaResponse:
    content: str
    model: str
    tokens_used: int
    response_time: float
    success: bool
    error: Optional[str] = None

class LlamaService:
    def __init__(self):
        self.service_type = "huggingface_inference_providers"
        self.model_name = os.getenv("LLM_MODEL", "google/flan-t5-base")
        self.hf_token = os.getenv("HF_TOKEN")
        self.ready = True

        self.api_endpoints = {
            "providers": "https://api-inference.huggingface.co/models/",
            "text_generation": "https://api-inference.huggingface.co/models/",
            "chat": "https://api-inference.huggingface.co/models/"
        }

        self.working_models = [
            "google/flan-t5-base",
            "microsoft/DialoGPT-small",
            "gpt2",
            "distilgpt2",
            "EleutherAI/gpt-neo-125M",
            "bigscience/bloom-560m"
        ]

        if not self.hf_token:
            logger.warning("‚ö†Ô∏è HF_TOKEN not set - using public inference (rate limited)")
        else:
            logger.info("‚úÖ HF_TOKEN configured for Llama service")

        logger.info(f"ü¶ô Llama service initialized: {self.model_name}")
        logger.info(f"üîÑ API endpoint: {self.api_endpoints['providers']}")

    async def answer_legal_question(self, question: str, context_documents: List[Dict], language: str = "en"):
        start_time = time.time()

        try:
            if os.getenv("LLM_DEMO_MODE", "false").lower() == "true":
                return self._generate_demo_response(question, context_documents, language, start_time)

            models_to_try = [self.model_name] + self.working_models

            for model_attempt, model in enumerate(models_to_try):
                try:
                    logger.info(f"üîÑ Trying model {model_attempt + 1}/{len(models_to_try)}: {model}")
                    prompt = self._build_prompt(question, context_documents, language, model)
                    response = await self._generate_with_model(prompt, model)

                    if response.success:
                        logger.info(f"‚úÖ Success with model: {model}")
                        return response
                    else:
                        logger.warning(f"‚ùå Model {model} failed: {response.error}")

                except Exception as e:
                    logger.warning(f"‚ùå Model {model} exception: {e}")
                    continue

            logger.error("‚ùå All models failed, returning fallback response")
            return self._generate_error_response(question, language, "All models unavailable", start_time)

        except Exception as e:
            logger.error(f"‚ùå General error in answer_legal_question: {e}")
            return self._generate_error_response(question, language, str(e), start_time)

    async def _generate_with_model(self, prompt: str, model: str) -> LlamaResponse:
        start_time = time.time()
        is_flan = "flan" in model.lower()

        try:
            import requests

            headers = {
                "Authorization": f"Bearer {self.hf_token}" if self.hf_token else "",
                "Content-Type": "application/json"
            }

            payload = {
                "inputs": prompt,
                "parameters": {
                    "max_new_tokens": 150,
                    "temperature": 0.3,
                    "do_sample": True
                }
            }

            task_type = "text2text-generation" if is_flan else "text-generation"
            endpoint = f"https://api-inference.huggingface.co/pipeline/{task_type}/{model}"
            logger.info(f"üì° Using endpoint: {endpoint}")

            response = requests.post(endpoint, headers=headers, json=payload, timeout=30)

            if response.status_code == 200:
                data = response.json()

                if isinstance(data, list) and len(data) > 0:
                    content = data[0].get("generated_text") or data[0].get("text") or ""
                elif isinstance(data, dict):
                    content = data.get("generated_text") or data.get("text") or ""
                else:
                    content = str(data)

                content = self._clean_response(content)

                if content and len(content.strip()) > 5:
                    response_time = time.time() - start_time
                    return LlamaResponse(
                        content=content,
                        model=model,
                        tokens_used=len(content.split()),
                        response_time=response_time,
                        success=True
                    )

            return LlamaResponse(
                content="",
                model=model,
                tokens_used=0,
                response_time=time.time() - start_time,
                success=False,
                error=f"HTTP {response.status_code}"
            )

        except requests.exceptions.RequestException as e:
            return LlamaResponse(
                content="",
                model=model,
                tokens_used=0,
                response_time=time.time() - start_time,
                success=False,
                error=str(e)
            )
        except ImportError:
            return LlamaResponse(
                content="",
                model=model,
                tokens_used=0,
                response_time=time.time() - start_time,
                success=False,
                error="Missing requests library"
            )
        except Exception as e:
            return LlamaResponse(
                content="",
                model=model,
                tokens_used=0,
                response_time=time.time() - start_time,
                success=False,
                error=str(e)
            )

    def _build_prompt(self, question: str, context_documents: List[Dict], language: str, model: Optional[str] = None) -> str:
        is_flan = model and "flan" in model.lower()

        context = ""
        if context_documents:
            doc = context_documents[0]
            context = doc.get('content', '')[:200]

        if language == "uk":
            return f"–ó–∞–ø–∏—Ç–∞–Ω–Ω—è: {question} –ö–æ–Ω—Ç–µ–∫—Å—Ç: {context}" if is_flan else f"–ö–æ–Ω—Ç–µ–∫—Å—Ç: {context}\n\n–ü–∏—Ç–∞–Ω–Ω—è: {question}\n–í—ñ–¥–ø–æ–≤—ñ–¥—å:"
        else:
            return f"question: {question} context: {context}" if is_flan else f"Context: {context}\n\nQuestion: {question}\nAnswer:"

    def _clean_response(self, response: str) -> str:
        if not response:
            return "I need more information to provide a proper legal analysis."

        response = response.strip()
        lines = response.split('\n')
        cleaned_lines = []
        for line in lines:
            line = line.strip()
            if line and not line.endswith(':') and len(line) > 3:
                cleaned_lines.append(line)
                if len(cleaned_lines) >= 3:
                    break

        cleaned = ' '.join(cleaned_lines)
        if len(cleaned) > 400:
            cleaned = cleaned[:400] + "..."

        return cleaned or "Unable to generate response."

    def _generate_demo_response(self, question: str, context_documents: List[Dict], language: str, start_time: float):
        if language == "uk":
            content = f"""ü§ñ **Demo —Ä–µ–∂–∏–º –∞–∫—Ç–∏–≤–Ω–∏–π** (LLM_DEMO_MODE=true)

**–í–∞—à–µ –ø–∏—Ç–∞–Ω–Ω—è:** {question}

üìö –ó–Ω–∞–π–¥–µ–Ω–æ {len(context_documents)} –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤ —É –±–∞–∑—ñ –∑–Ω–∞–Ω—å.

üí° **Demo –≤—ñ–¥–ø–æ–≤—ñ–¥—å:** –¶–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü—ñ–π–Ω–∏–π —Ä–µ–∂–∏–º. –£ —Ä–æ–±–æ—á–æ–º—É —Ä–µ–∂–∏–º—ñ —Ç—É—Ç –±—É–¥–µ –¥–µ—Ç–∞–ª—å–Ω–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥—å –≤—ñ–¥ —Ä–µ–∞–ª—å–Ω–æ–≥–æ AI –Ω–∞ –æ—Å–Ω–æ–≤—ñ –≤–∞—à–∏—Ö —é—Ä–∏–¥–∏—á–Ω–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤.

üîß **–î–ª—è –∞–∫—Ç–∏–≤–∞—Ü—ñ—ó –ø–æ–≤–Ω–æ—ó –≤–µ—Ä—Å—ñ—ó:**
‚Ä¢ –í—Å—Ç–∞–Ω–æ–≤—ñ—Ç—å LLM_DEMO_MODE=false
‚Ä¢ –ü–µ—Ä–µ–∫–æ–Ω–∞–π—Ç–µ—Å—å —â–æ HF_TOKEN –Ω–∞–ª–∞—à—Ç–æ–≤–∞–Ω–∏–π

üìÑ **–ó–Ω–∞–π–¥–µ–Ω—ñ –¥–æ–∫—É–º–µ–Ω—Ç–∏:** {', '.join([doc.get('filename', 'Unknown') for doc in context_documents[:3]])}"""
        else:
            content = f"""ü§ñ **Demo Mode Active** (LLM_DEMO_MODE=true)

**Your Question:** {question}

üìö Found {len(context_documents)} documents in knowledge base.

üí° **Demo Response:** This is demonstration mode. In production mode, you would get detailed answers from real AI based on your legal documents.

üîß **To activate full version:**
‚Ä¢ Set LLM_DEMO_MODE=false
‚Ä¢ Ensure HF_TOKEN is configured

üìÑ **Found documents:** {', '.join([doc.get('filename', 'Unknown') for doc in context_documents[:3]])}"""

        return LlamaResponse(
            content=content,
            model="demo_mode",
            tokens_used=len(content.split()),
            response_time=time.time() - start_time,
            success=True
        )

    def _generate_error_response(self, question: str, language: str, error: str, start_time: float):
        if language == "uk":
            content = f"""‚ùå **–ü–æ–º–∏–ª–∫–∞ LLM —Å–µ—Ä–≤—ñ—Å—É**

**–í–∞—à–µ –ø–∏—Ç–∞–Ω–Ω—è:** {question}

üîß **–ü—Ä–æ–±–ª–µ–º–∞:** {error}

üí° **–ú–æ–∂–ª–∏–≤—ñ —Ä—ñ—à–µ–Ω–Ω—è:**
‚Ä¢ HuggingFace API –º–æ–∂–µ –±—É—Ç–∏ –ø–µ—Ä–µ–≤–∞–Ω—Ç–∞–∂–µ–Ω–∏–π
‚Ä¢ –°–ø—Ä–æ–±—É–π—Ç–µ —â–µ —Ä–∞–∑ —á–µ—Ä–µ–∑ –∫—ñ–ª—å–∫–∞ —Ö–≤–∏–ª–∏–Ω
‚Ä¢ –ü–µ—Ä–µ–≤—ñ—Ä—Ç–µ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è HF_TOKEN
‚Ä¢ –ê–±–æ –≤–∫–ª—é—á—ñ—Ç—å demo —Ä–µ–∂–∏–º: LLM_DEMO_MODE=true

üîÑ **–°—Ç–∞—Ç—É—Å:** –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ —Å–ø—Ä–æ–±—É–≤–∞–ª–∏ –∫—ñ–ª—å–∫–∞ –º–æ–¥–µ–ª–µ–π"""
        else:
            content = f"""‚ùå **LLM Service Error**

**Your Question:** {question}

üîß **Issue:** {error}

üí° **Possible Solutions:**
‚Ä¢ HuggingFace API may be overloaded
‚Ä¢ Try again in a few minutes
‚Ä¢ Check HF_TOKEN configuration
‚Ä¢ Or enable demo mode: LLM_DEMO_MODE=true

üîÑ **Status:** Automatically tried multiple models"""

        return LlamaResponse(
            content=content,
            model="error_fallback",
            tokens_used=len(content.split()),
            response_time=time.time() - start_time,
            success=False,
            error=error
        )

def create_llama_service():
    try:
        return LlamaService()
    except Exception as e:
        logger.error(f"‚ùå Failed to create Llama service: {e}")
        return create_fallback_service()

def create_fallback_service():
    class FallbackService:
        def __init__(self):
            self.service_type = "fallback"
            self.ready = True

        async def answer_legal_question(self, question: str, context_documents: list, language: str = "en"):
            content = f"LLM service unavailable. Question: {question}. Found {len(context_documents)} documents."
            return LlamaResponse(
                content=content,
                model="fallback",
                tokens_used=len(content.split()),
                response_time=0.1,
                success=True
            )

        async def get_service_status(self):
            return {
                "service_type": "fallback",
                "ready": True,
                "recommendations": [
                    "Install requests",
                    "Set HF_TOKEN",
                    "Check HuggingFace status"
                ]
            }

    return FallbackService()
