# ====================================
# –§–ê–ô–õ: backend/services/chroma_service.py (–ü–û–õ–ù–´–ô –ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ô)
# –ó–∞–º–µ–Ω–∏—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π —Ñ–∞–π–ª –ø–æ–ª–Ω–æ—Å—Ç—å—é
# ====================================

import chromadb
from chromadb.config import Settings
from chromadb.utils import embedding_functions
import logging
import time
import hashlib
from typing import List, Dict, Optional, Any
from dataclasses import dataclass
import json
import os

logger = logging.getLogger(__name__)

@dataclass
class ProcessedDocument:
    id: str
    filename: str
    content: str
    metadata: Dict
    category: str
    chunks: List[str]

class ChromaDBService:
    """–°–µ—Ä–≤–∏—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å ChromaDB –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑–æ–π –¥–∞–Ω–Ω—ã—Ö"""
    
    def __init__(self, persist_directory: str = "./chromadb_data"):
        self.persist_directory = persist_directory
        
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        os.makedirs(persist_directory, exist_ok=True)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º ChromaDB –∫–ª–∏–µ–Ω—Ç
        self.client = chromadb.PersistentClient(path=persist_directory)
        
        # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ —Ñ—É–Ω–∫—Ü–∏—é
        self.embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(
            model_name="all-MiniLM-L6-v2"
        )
        
        # –°–æ–∑–¥–∞–µ–º –∏–ª–∏ –ø–æ–ª—É—á–∞–µ–º –∫–æ–ª–ª–µ–∫—Ü–∏—é –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        self.collection = self.client.get_or_create_collection(
            name="legal_documents",
            embedding_function=self.embedding_function,
            metadata={"description": "Legal Assistant Documents Collection"}
        )
        
        logger.info(f"ChromaDB initialized with {self.collection.count()} documents")
    
    async def add_document(self, document: ProcessedDocument) -> bool:
        """–î–æ–±–∞–≤–ª—è–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç –≤ ChromaDB"""
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            existing_docs = self.collection.get(
                ids=[document.id],
                include=["metadatas"]
            )
            
            if existing_docs["ids"]:
                logger.warning(f"Document {document.id} already exists, skipping addition")
                return True  # –°—á–∏—Ç–∞–µ–º —É—Å–ø–µ—à–Ω—ã–º, —Ç–∞–∫ –∫–∞–∫ –¥–æ–∫—É–º–µ–Ω—Ç —É–∂–µ –µ—Å—Ç—å
            
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è ChromaDB
            chroma_metadata = {
                "filename": document.filename,
                "category": document.category,
                "content_length": len(document.content),
                "word_count": len(document.content.split()),
                "chunks_count": len(document.chunks),
                "added_at": time.time(),
                **document.metadata
            }
            
            # –î–æ–±–∞–≤–ª—è–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –¥–æ–∫—É–º–µ–Ω—Ç
            main_metadata = chroma_metadata.copy()
            main_metadata.update({
                "is_chunk": False,
                "chunk_index": -1,
                "parent_document_id": document.id
            })
            
            self.collection.add(
                ids=[document.id],
                documents=[document.content],
                metadatas=[main_metadata]
            )
            
            logger.info(f"‚úÖ Added main document {document.filename}")
            
            # –ï—Å–ª–∏ –¥–æ–∫—É–º–µ–Ω—Ç –±–æ–ª—å—à–æ–π, –¥–æ–±–∞–≤–ª—è–µ–º —á–∞–Ω–∫–∏
            if len(document.chunks) > 1:
                chunk_ids = []
                chunk_documents = []
                chunk_metadatas = []
                
                for i, chunk in enumerate(document.chunks):
                    chunk_id = f"{document.id}_chunk_{i}"
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —á–∞–Ω–∫–∞
                    existing_chunk = self.collection.get(
                        ids=[chunk_id],
                        include=["metadatas"]
                    )
                    
                    if existing_chunk["ids"]:
                        logger.debug(f"Chunk {chunk_id} already exists, skipping")
                        continue
                    
                    chunk_ids.append(chunk_id)
                    chunk_documents.append(chunk)
                    
                    chunk_metadata = chroma_metadata.copy()
                    chunk_metadata.update({
                        "chunk_index": i,
                        "parent_document_id": document.id,
                        "is_chunk": True
                    })
                    chunk_metadatas.append(chunk_metadata)
                
                # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã–µ —á–∞–Ω–∫–∏
                if chunk_ids:
                    self.collection.add(
                        ids=chunk_ids,
                        documents=chunk_documents,
                        metadatas=chunk_metadatas
                    )
                    
                    logger.info(f"‚úÖ Added {len(chunk_ids)} new chunks for {document.filename}")
                else:
                    logger.info(f"‚úÖ All chunks for {document.filename} already exist")
            else:
                logger.info(f"‚úÖ Added single document {document.filename}")
            
            return True
            
        except Exception as e:
            logger.error(f"Error adding document to ChromaDB: {str(e)}")
            return False
    
    async def search_documents(self, query: str, n_results: int = 5, 
                             category: str = None, min_relevance: float = 0.3, **filters) -> List[Dict]:
        """
        –ü–æ–∏—Å–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–º—É —Å—Ö–æ–¥—Å—Ç–≤—É —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π
        """
        try:
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º —Ñ–∏–ª—å—Ç—Ä—ã
            where_filter = {}
            
            if category:
                where_filter["category"] = category
            
            # –î–æ–±–∞–≤–ª—è–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ñ–∏–ª—å—Ç—Ä—ã (–Ω–æ –ù–ï is_chunk!)
            for key, value in filters.items():
                if key != "is_chunk":  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º —Ñ–∏–ª—å—Ç—Ä –ø–æ —á–∞–Ω–∫–∞–º
                    where_filter[key] = value
            
            # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –ª—É—á—à–µ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
            search_limit = min(n_results * 3, 20)
            
            # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ò—â–µ–º –≤–æ –í–°–ï–• –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö –∏ —á–∞–Ω–∫–∞—Ö
            results = self.collection.query(
                query_texts=[query],
                n_results=search_limit,
                where=where_filter if where_filter else None,  # –£–±—Ä–∞–ª–∏ —Ñ–∏–ª—å—Ç—Ä is_chunk
                include=["documents", "metadatas", "distances"]
            )
            
            # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –∏ —Ñ–∏–ª—å—Ç—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            formatted_results = []
            query_lower = query.lower()
            seen_parent_ids = set()  # –î–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
            
            if results["documents"] and results["documents"][0]:
                for i in range(len(results["documents"][0])):
                    distance = results["distances"][0][i]
                    
                    # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ù–æ–≤–∞—è —Ñ–æ—Ä–º—É–ª–∞ –¥–ª—è relevance_score
                    # ChromaDB –º–æ–∂–µ—Ç –≤–æ–∑–≤—Ä–∞—â–∞—Ç—å distance > 1.0, —á—Ç–æ –¥–∞–µ—Ç –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ scores
                    if distance <= 0:
                        relevance_score = 1.0  # –ò–¥–µ–∞–ª—å–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
                    elif distance >= 2.0:
                        relevance_score = 0.0  # –û—á–µ–Ω—å –ø–ª–æ—Ö–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
                    else:
                        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º distance –æ—Ç 0-2 –∫ relevance_score –æ—Ç 1-0
                        relevance_score = max(0.0, (2.0 - distance) / 2.0)
                    
                    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
                    if relevance_score < min_relevance:
                        logger.debug(f"Skipping result with low relevance: {relevance_score:.3f} (distance: {distance:.3f})")
                        continue
                    
                    document_content = results["documents"][0][i]
                    metadata = results["metadatas"][0][i]
                    
                    # –ü–æ–ª—É—á–∞–µ–º parent_document_id
                    parent_doc_id = metadata.get("parent_document_id")
                    current_doc_id = results["ids"][0][i]
                    
                    # –ò–∑–±–µ–≥–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ - –µ—Å–ª–∏ —É–∂–µ –µ—Å—Ç—å –æ—Å–Ω–æ–≤–Ω–æ–π –¥–æ–∫—É–º–µ–Ω—Ç, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —á–∞–Ω–∫–∏
                    unique_id = parent_doc_id or current_doc_id
                    if unique_id in seen_parent_ids:
                        logger.debug(f"Skipping duplicate parent document: {unique_id}")
                        continue
                    seen_parent_ids.add(unique_id)
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ç–æ—á–Ω–æ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –≤ —Ç–µ–∫—Å—Ç–µ
                    content_lower = document_content.lower()
                    filename_lower = metadata.get("filename", "").lower()
                    
                    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
                    exact_match = query_lower in content_lower or query_lower in filename_lower
                    semantic_match = relevance_score > 0.7
                    
                    # –õ—É—á—à–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
                    best_context = self._find_best_context(document_content, query, max_length=400)
                    
                    result = {
                        "content": best_context,
                        "full_content": document_content,
                        "metadata": metadata,
                        "distance": distance,
                        "relevance_score": relevance_score,
                        "document_id": parent_doc_id or current_doc_id,
                        "filename": metadata.get("filename", "Unknown"),
                        "exact_match": exact_match,
                        "semantic_match": semantic_match,
                        "is_chunk": metadata.get("is_chunk", False),
                        "search_info": {
                            "query": query,
                            "match_type": "exact" if exact_match else ("semantic" if semantic_match else "weak"),
                            "confidence": "high" if relevance_score > 0.7 else ("medium" if relevance_score > 0.5 else "low"),
                            "source_type": "chunk" if metadata.get("is_chunk", False) else "document"
                        }
                    }
                    formatted_results.append(result)
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º: —Å–Ω–∞—á–∞–ª–∞ —Ç–æ—á–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è, –ø–æ—Ç–æ–º –æ—Å–Ω–æ–≤–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã, –ø–æ—Ç–æ–º –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
            formatted_results.sort(key=lambda x: (
                x["exact_match"],                    # 1. –¢–æ—á–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –ø–µ—Ä–≤—ã–º–∏
                not x["is_chunk"],                   # 2. –û—Å–Ω–æ–≤–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –ø–µ—Ä–µ–¥ —á–∞–Ω–∫–∞–º–∏  
                x["relevance_score"]                 # 3. –ü–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
            ), reverse=True)
            
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ –∑–∞–ø—Ä–æ—à–µ–Ω–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞
            formatted_results = formatted_results[:n_results]
            
            # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            if formatted_results:
                logger.info(f"Found {len(formatted_results)} relevant results for '{query}' (min_relevance={min_relevance})")
                for result in formatted_results:
                    source_type = result['search_info']['source_type']
                    logger.debug(f"  - {result['filename']} ({source_type}): {result['search_info']['match_type']} match, "
                               f"relevance={result['relevance_score']:.3f}")
            else:
                logger.info(f"No relevant results found for '{query}' with min_relevance={min_relevance}")
            
            return formatted_results
            
        except Exception as e:
            logger.error(f"Error searching documents: {str(e)}")
            return []
    
    def _find_best_context(self, content: str, query: str, max_length: int = 400) -> str:
        """
        –ù–∞—Ö–æ–¥–∏—Ç –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—É—é —á–∞—Å—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç–∞ –¥–ª—è –ø–æ–∫–∞–∑–∞ –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö
        """
        if len(content) <= max_length:
            return content
        
        query_words = query.lower().split()
        content_lower = content.lower()
        
        # –ò—â–µ–º –ª—É—á—à–µ–µ –º–µ—Å—Ç–æ –¥–ª—è –Ω–∞—á–∞–ª–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        best_score = 0
        best_start = 0
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã–µ –ø–æ–∑–∏—Ü–∏–∏ –≤ —Ç–µ–∫—Å—Ç–µ
        for start in range(0, len(content) - max_length + 1, max_length // 4):
            end = start + max_length
            segment = content_lower[start:end]
            
            # –°—á–∏—Ç–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Å–ª–æ–≤ –∑–∞–ø—Ä–æ—Å–∞ –≤ —Å–µ–≥–º–µ–Ω—Ç–µ
            score = sum(1 for word in query_words if word in segment)
            
            # –ë–æ–Ω—É—Å –∑–∞ –Ω–∞—Ö–æ–∂–¥–µ–Ω–∏–µ –≤ –Ω–∞—á–∞–ª–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            if start == 0:
                score += 0.5
            
            if score > best_score:
                best_score = score
                best_start = start
        
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ —Ö–æ—Ä–æ—à–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –Ω–∞—á–∞–ª–æ
        if best_score == 0:
            return content[:max_length] + "..."
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ª—É—á—à–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
        best_end = best_start + max_length
        context = content[best_start:best_end]
        
        # –î–æ–±–∞–≤–ª—è–µ–º –º–Ω–æ–≥–æ—Ç–æ—á–∏–µ –µ—Å–ª–∏ –æ–±—Ä–µ–∑–∞–ª–∏
        if best_start > 0:
            context = "..." + context
        if best_end < len(content):
            context = context + "..."
        
        return context.strip()
    
    async def get_document_count(self) -> int:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –∫–æ–ª–ª–µ–∫—Ü–∏–∏"""
        try:
            return self.collection.count()
        except Exception as e:
            logger.error(f"Error getting document count: {str(e)}")
            return 0
    
    async def delete_document(self, document_id: str) -> bool:
        """–£–¥–∞–ª—è–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç –∏ –≤—Å–µ –µ–≥–æ —á–∞–Ω–∫–∏"""
        try:
            # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ —Å–≤—è–∑–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏ —á–∞–Ω–∫–∏
            all_related_docs = self.collection.get(
                where={"parent_document_id": document_id},
                include=["metadatas"]
            )
            
            ids_to_delete = set()  # –ò—Å–ø–æ–ª—å–∑—É–µ–º set –¥–ª—è —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏
            
            # –î–æ–±–∞–≤–ª—è–µ–º ID —á–∞–Ω–∫–æ–≤
            if all_related_docs["ids"]:
                ids_to_delete.update(all_related_docs["ids"])
            
            # –î–æ–±–∞–≤–ª—è–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –¥–æ–∫—É–º–µ–Ω—Ç
            ids_to_delete.add(document_id)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∫–∏–µ ID —Ä–µ–∞–ª—å–Ω–æ —Å—É—â–µ—Å—Ç–≤—É—é—Ç
            existing_docs = self.collection.get(
                ids=list(ids_to_delete),
                include=["metadatas"]
            )
            
            actual_ids_to_delete = existing_docs["ids"]
            
            if actual_ids_to_delete:
                # –£–¥–∞–ª—è–µ–º –ø–æ –æ–¥–Ω–æ–º—É ID –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
                deleted_count = 0
                for doc_id in actual_ids_to_delete:
                    try:
                        self.collection.delete(ids=[doc_id])
                        deleted_count += 1
                        logger.debug(f"Deleted document/chunk: {doc_id}")
                    except Exception as e:
                        logger.warning(f"Failed to delete {doc_id}: {e}")
                        continue
                
                logger.info(f"Successfully deleted {deleted_count} documents/chunks for {document_id}")
                return deleted_count > 0
            else:
                logger.warning(f"Document {document_id} not found for deletion")
                return False
                
        except Exception as e:
            logger.error(f"Error deleting document: {str(e)}")
            return False
    
    async def get_all_documents(self) -> List[Dict]:
        """–ü–æ–ª—É—á–∞–µ—Ç –≤—Å–µ –æ—Å–Ω–æ–≤–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã (–Ω–µ —á–∞–Ω–∫–∏) –¥–ª—è –∞–¥–º–∏–Ω –ø–∞–Ω–µ–ª–∏"""
        try:
            # –ë–æ–ª–µ–µ –Ω–∞–¥–µ–∂–Ω—ã–π –∑–∞–ø—Ä–æ—Å –æ—Å–Ω–æ–≤–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            results = self.collection.get(
                where={"is_chunk": False},
                include=["documents", "metadatas"]
            )
            
            documents = []
            seen_ids = set()  # –î–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
            
            if results["ids"]:
                for i, doc_id in enumerate(results["ids"]):
                    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
                    if doc_id in seen_ids:
                        logger.debug(f"Skipping duplicate document: {doc_id}")
                        continue
                    
                    seen_ids.add(doc_id)
                    metadata = results["metadatas"][i]
                    
                    doc = {
                        "id": doc_id,
                        "filename": metadata.get("filename", "Unknown"),
                        "category": metadata.get("category", "general"),
                        "content": results["documents"][i],
                        "size": metadata.get("content_length", 0),
                        "word_count": metadata.get("word_count", 0),
                        "chunks_count": metadata.get("chunks_count", 1),
                        "added_at": metadata.get("added_at", time.time()),
                        "metadata": metadata
                    }
                    documents.append(doc)
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –¥–∞—Ç–µ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è (–Ω–æ–≤—ã–µ –ø–µ—Ä–≤—ã–µ)
            documents.sort(key=lambda x: x["added_at"], reverse=True)
            
            logger.info(f"Retrieved {len(documents)} unique documents")
            return documents
            
        except Exception as e:
            logger.error(f"Error getting all documents: {str(e)}")
            return []
    
    async def update_document(self, document_id: str, new_content: str = None, new_metadata: Dict = None) -> bool:
        """–û–±–Ω–æ–≤–ª—è–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç"""
        try:
            # ChromaDB –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –ø—Ä—è–º–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ, –ø–æ—ç—Ç–æ–º—É —É–¥–∞–ª—è–µ–º –∏ –¥–æ–±–∞–≤–ª—è–µ–º –∑–∞–Ω–æ–≤–æ
            if new_content or new_metadata:
                # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—É—â–∏–π –¥–æ–∫—É–º–µ–Ω—Ç
                current = self.collection.get(
                    ids=[document_id],
                    include=["documents", "metadatas"]
                )
                
                if not current["ids"]:
                    return False
                
                # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
                content = new_content if new_content else current["documents"][0]
                metadata = current["metadatas"][0].copy()
                
                if new_metadata:
                    metadata.update(new_metadata)
                
                # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–π –¥–æ–∫—É–º–µ–Ω—Ç –∏ –≤—Å–µ –µ–≥–æ —á–∞–Ω–∫–∏
                await self.delete_document(document_id)
                
                # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–π
                self.collection.add(
                    ids=[document_id],
                    documents=[content],
                    metadatas=[metadata]
                )
                
                logger.info(f"Updated document {document_id}")
                return True
            
            return False
            
        except Exception as e:
            logger.error(f"Error updating document: {str(e)}")
            return False
    
    async def get_stats(self) -> Dict:
        """–ü–æ–ª—É—á–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"""
        try:
            total_count = await self.get_document_count()
            
            # –ü–æ–ª—É—á–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∏–∑ –æ—Å–Ω–æ–≤–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            all_results = self.collection.get(
                where={"is_chunk": False},
                include=["metadatas"]
            )
            
            categories = set()
            unique_docs = 0
            
            if all_results["metadatas"]:
                seen_ids = set()
                for i, doc_id in enumerate(all_results["ids"]):
                    if doc_id not in seen_ids:
                        seen_ids.add(doc_id)
                        unique_docs += 1
                        category = all_results["metadatas"][i].get("category", "general")
                        categories.add(category)
            
            return {
                "total_documents": unique_docs,
                "categories": list(categories),
                "database_type": "ChromaDB",
                "persist_directory": self.persist_directory,
                "embedding_model": "all-MiniLM-L6-v2",
                "total_chunks": total_count,
                "unique_documents": unique_docs
            }
            
        except Exception as e:
            logger.error(f"Error getting stats: {str(e)}")
            return {
                "total_documents": 0,
                "categories": [],
                "database_type": "ChromaDB",
                "error": str(e)
            }
    
    async def cleanup_duplicates(self) -> Dict:
        """–û—á–∏—â–∞–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö"""
        try:
            logger.info("üßπ Starting duplicate cleanup...")
            
            # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
            all_docs = self.collection.get(include=["metadatas"])
            
            if not all_docs["ids"]:
                return {"removed": 0, "message": "No documents found"}
            
            # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ parent_document_id
            docs_by_parent = {}
            duplicates_to_remove = []
            
            for i, doc_id in enumerate(all_docs["ids"]):
                metadata = all_docs["metadatas"][i]
                parent_id = metadata.get("parent_document_id", doc_id)
                is_chunk = metadata.get("is_chunk", False)
                
                if parent_id not in docs_by_parent:
                    docs_by_parent[parent_id] = []
                
                docs_by_parent[parent_id].append({
                    "id": doc_id,
                    "is_chunk": is_chunk,
                    "metadata": metadata
                })
            
            # –ù–∞—Ö–æ–¥–∏–º –¥—É–±–ª–∏–∫–∞—Ç—ã –æ—Å–Ω–æ–≤–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            for parent_id, docs in docs_by_parent.items():
                main_docs = [d for d in docs if not d["is_chunk"]]
                
                if len(main_docs) > 1:
                    # –û—Å—Ç–∞–≤–ª—è–µ–º —Å–∞–º—ã–π –Ω–æ–≤—ã–π, —É–¥–∞–ª—è–µ–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ
                    main_docs.sort(key=lambda x: x["metadata"].get("added_at", 0), reverse=True)
                    for duplicate in main_docs[1:]:
                        duplicates_to_remove.append(duplicate["id"])
                        logger.debug(f"Marking duplicate main document for removal: {duplicate['id']}")
            
            # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
            removed_count = 0
            for doc_id in duplicates_to_remove:
                try:
                    self.collection.delete(ids=[doc_id])
                    removed_count += 1
                    logger.debug(f"Removed duplicate: {doc_id}")
                except Exception as e:
                    logger.warning(f"Failed to remove duplicate {doc_id}: {e}")
            
            logger.info(f"üßπ Cleanup completed: removed {removed_count} duplicates")
            
            return {
                "removed": removed_count,
                "message": f"Successfully removed {removed_count} duplicate documents"
            }
            
        except Exception as e:
            logger.error(f"Error during cleanup: {e}")
            return {
                "removed": 0,
                "error": str(e),
                "message": "Cleanup failed"
            }

# –î–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
class DocumentProcessor:
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
    
    def __init__(self):
        self.supported_formats = {
            '.txt': self._process_txt,
            '.md': self._process_txt,
        }
    
    async def process_file(self, file_path: str, category: str = "general") -> Optional[ProcessedDocument]:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Ñ–∞–π–ª –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç"""
        try:
            from pathlib import Path
            
            file_path = Path(file_path)
            extension = file_path.suffix.lower()
            
            if extension not in self.supported_formats:
                content = await self._process_txt(file_path)
            else:
                content = await self.supported_formats[extension](file_path)
            
            if not content or len(content.strip()) < 10:
                logger.warning(f"No meaningful content extracted from {file_path.name}")
                return None
            
            # –°–æ–∑–¥–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            metadata = await self._extract_metadata(file_path, content)
            
            # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞–Ω–∫–∏
            chunks = self._chunk_text(content)
            
            # –°–æ–∑–¥–∞–µ–º ID –¥–æ–∫—É–º–µ–Ω—Ç–∞
            doc_id = self._generate_doc_id(file_path.name, content)
            
            return ProcessedDocument(
                id=doc_id,
                filename=file_path.name,
                content=content,
                metadata=metadata,
                category=category,
                chunks=chunks
            )
            
        except Exception as e:
            logger.error(f"Error processing file {file_path}: {str(e)}")
            return None
    
    async def _process_txt(self, file_path) -> str:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Ñ–∞–π–ª—ã"""
        try:
            with open(file_path, 'r', encoding='utf-8') as file:
                return file.read()
        except UnicodeDecodeError:
            for encoding in ['cp1251', 'iso-8859-1', 'cp1252']:
                try:
                    with open(file_path, 'r', encoding=encoding) as file:
                        return file.read()
                except:
                    continue
            logger.error(f"Could not decode text file {file_path}")
            return ""
    
    async def _extract_metadata(self, file_path, content: str) -> Dict:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        return {
            "filename": file_path.name,
            "file_size": file_path.stat().st_size,
            "file_extension": file_path.suffix,
            "content_length": len(content),
            "word_count": len(content.split()),
            "created_at": file_path.stat().st_ctime,
            "modified_at": file_path.stat().st_mtime,
            "processed_at": time.time()
        }
    
    def _chunk_text(self, text: str, chunk_size: int = 1000, overlap: int = 200) -> List[str]:
        """–†–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏"""
        if len(text) <= chunk_size:
            return [text]
        
        chunks = []
        start = 0
        
        while start < len(text):
            end = start + chunk_size
            
            if end < len(text):
                sentence_end = max(
                    text.rfind('.', start, end),
                    text.rfind('!', start, end),
                    text.rfind('?', start, end)
                )
                
                if sentence_end > start:
                    end = sentence_end + 1
            
            chunk = text[start:end].strip()
            if chunk:
                chunks.append(chunk)
            
            start = end - overlap
        
        return chunks
    
    def _generate_doc_id(self, filename: str, content: str) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã–π ID –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        content_hash = hashlib.md5(content.encode()).hexdigest()[:8]
        return f"{filename}_{content_hash}"

class DocumentService:
    """–û—Å–Ω–æ–≤–Ω–æ–π —Å–µ—Ä–≤–∏—Å –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å ChromaDB"""
    
    def __init__(self, db_path: str = "./chromadb_data"):
        self.processor = DocumentProcessor()
        self.vector_db = ChromaDBService(db_path)
    
    async def process_and_store_file(self, file_path: str, category: str = "general") -> bool:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Ñ–∞–π–ª –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤ ChromaDB"""
        document = await self.processor.process_file(file_path, category)
        
        if not document:
            return False
        
        return await self.vector_db.add_document(document)
    
    async def search(self, query: str, category: str = None, limit: int = 5, min_relevance: float = 0.3) -> List[Dict]:
        """
        –ü–æ–∏—Å–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π
        """
        return await self.vector_db.search_documents(
            query=query, 
            n_results=limit, 
            category=category,
            min_relevance=min_relevance
        )
    
    async def get_stats(self) -> Dict:
        """–ü–æ–ª—É—á–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É"""
        return await self.vector_db.get_stats()
    
    async def get_all_documents(self) -> List[Dict]:
        """–ü–æ–ª—É—á–∞–µ—Ç –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã"""
        return await self.vector_db.get_all_documents()
    
    async def delete_document(self, document_id: str) -> bool:
        """–£–¥–∞–ª—è–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç"""
        return await self.vector_db.delete_document(document_id)
    
    async def cleanup_duplicates(self) -> Dict:
        """–û—á–∏—â–∞–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã"""
        return await self.vector_db.cleanup_duplicates()