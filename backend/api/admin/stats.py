# ====================================
# –§–ê–ô–õ: backend/api/admin/stats.py (–ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø)
# –ó–∞–º–µ–Ω–∏—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π —Ñ–∞–π–ª –ø–æ–ª–Ω–æ—Å—Ç—å—é
# ====================================

"""
Admin Stats Endpoints - –ê–¥–º–∏–Ω—Å–∫–∏–µ endpoints –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
"""

from fastapi import APIRouter, HTTPException, Depends
import logging
import time
from datetime import datetime, timedelta

from models.responses import AdminStats
from app.dependencies import get_document_service, get_services_status, CHROMADB_ENABLED
from api.user.chat import chat_history

router = APIRouter()
logger = logging.getLogger(__name__)

@router.get("/stats", response_model=AdminStats)
async def get_admin_stats(
    document_service = Depends(get_document_service),
    services_status = Depends(get_services_status)
):
    """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª—è –∞–¥–º–∏–Ω –ø–∞–Ω–µ–ª–∏ - –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø"""
    try:
        # –ü–æ–ª—É—á–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        total_documents = 0
        categories = ["general", "legislation", "jurisprudence", "government", "civil_rights", "scraped"]
        vector_db_info = None
        vector_db_error = None
        
        if document_service:
            try:
                vector_stats = await document_service.get_stats()
                total_documents = vector_stats.get("total_documents", 0)
                
                # –û–±–Ω–æ–≤–ª—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —Ä–µ–∞–ª—å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
                real_categories = vector_stats.get("categories", [])
                if real_categories:
                    categories = real_categories
                
                vector_db_info = vector_stats
                    
            except Exception as e:
                logger.error(f"Error getting vector stats: {e}")
                vector_db_error = str(e)
        
        # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ü—Ä–∞–≤–∏–ª—å–Ω–æ —Ñ–æ—Ä–º–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
        stats_data = AdminStats(
            total_documents=total_documents,
            total_chats=len(chat_history),
            categories=categories,
            services_status=services_status,  # –ü–µ—Ä–µ–¥–∞–µ–º –∫–∞–∫ –µ—Å—Ç—å - Dict[str, Any]
            vector_db_info=vector_db_info,
            vector_db_error=vector_db_error,
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
            initialization_summary={
                "services_ready": services_status.get("services_ready", False),
                "total_errors": services_status.get("total_errors", 0),
                "platform": services_status.get("platform", "Unknown")
            },
            
            system_info={
                "database_type": vector_db_info.get("database_type", "Unknown") if vector_db_info else "Unknown",
                "chromadb_enabled": services_status.get("chromadb_enabled", False),
                "demo_mode": services_status.get("demo_mode", False),
                "environment": services_status.get("environment", "unknown")
            },
            
            recommendations=_generate_admin_recommendations(services_status, total_documents, vector_db_error)
        )
        
        return stats_data
        
    except Exception as e:
        logger.error(f"Stats error: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to get stats: {str(e)}")

@router.get("/stats/detailed")
async def get_detailed_stats(
    document_service = Depends(get_document_service),
    services_status = Depends(get_services_status)
):
    """–î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª—è dashboard"""
    try:
        # –ë–∞–∑–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        base_stats = await get_admin_stats(document_service, services_status)
        
        # –ê–Ω–∞–ª–∏–∑ —á–∞—Ç–æ–≤
        chat_stats = _analyze_chat_history()
        
        # –ê–Ω–∞–ª–∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        category_stats = await _analyze_document_categories(document_service)
        
        # –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å–∏—Å—Ç–µ–º—ã
        performance_stats = await _get_performance_stats(document_service)
        
        return {
            "base": base_stats.dict(),
            "chat_analytics": chat_stats,
            "document_analytics": category_stats,
            "performance": performance_stats,
            "generated_at": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"Detailed stats error: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to get detailed stats: {str(e)}")

@router.get("/stats/usage")
async def get_usage_stats():
    """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Å–∏—Å—Ç–µ–º—ã"""
    try:
        # –ê–Ω–∞–ª–∏–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 24 —á–∞—Å–∞
        now = time.time()
        day_ago = now - (24 * 60 * 60)
        
        recent_chats = [
            chat for chat in chat_history 
            if chat.get("timestamp", 0) > day_ago
        ]
        
        # –ê–Ω–∞–ª–∏–∑ –ø–æ —á–∞—Å–∞–º
        hourly_usage = {}
        for chat in recent_chats:
            timestamp = chat.get("timestamp", 0)
            hour = datetime.fromtimestamp(timestamp).hour
            hourly_usage[hour] = hourly_usage.get(hour, 0) + 1
        
        # –ê–Ω–∞–ª–∏–∑ —è–∑—ã–∫–æ–≤
        language_usage = {}
        for chat in recent_chats:
            lang = chat.get("language", "unknown")
            language_usage[lang] = language_usage.get(lang, 0) + 1
        
        # –ê–Ω–∞–ª–∏–∑ –∑–∞–ø—Ä–æ—Å–æ–≤ —Å –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏
        queries_with_sources = len([
            chat for chat in recent_chats 
            if chat.get("sources")
        ])
        
        return {
            "period": "last_24_hours",
            "total_queries": len(recent_chats),
            "queries_with_sources": queries_with_sources,
            "success_rate": (queries_with_sources / len(recent_chats) * 100) if recent_chats else 0,
            "hourly_distribution": hourly_usage,
            "language_distribution": language_usage,
            "average_query_length": sum(len(chat.get("message", "")) for chat in recent_chats) / len(recent_chats) if recent_chats else 0
        }
        
    except Exception as e:
        logger.error(f"Usage stats error: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to get usage stats: {str(e)}")

@router.get("/stats/system")
async def get_system_stats(services_status = Depends(get_services_status)):
    """–°–∏—Å—Ç–µ–º–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏ —Å—Ç–∞—Ç—É—Å –∑–¥–æ—Ä–æ–≤—å—è"""
    try:
        import psutil
        import platform
        
        # –°–∏—Å—Ç–µ–º–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
        system_info = {
            "platform": platform.platform(),
            "python_version": platform.python_version(),
            "cpu_count": psutil.cpu_count(),
            "memory_total": psutil.virtual_memory().total,
            "memory_available": psutil.virtual_memory().available,
            "memory_percent": psutil.virtual_memory().percent,
            "disk_usage": psutil.disk_usage('.').percent
        }
        
        # –°—Ç–∞—Ç—É—Å —Å–µ—Ä–≤–∏—Å–æ–≤
        service_health = {
            "all_services_healthy": all(services_status.values()) if isinstance(services_status, dict) else False,
            "services_detail": services_status,
            "database_type": "ChromaDB" if CHROMADB_ENABLED else "SimpleVectorDB"
        }
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        recommendations = []
        
        if system_info["memory_percent"] > 80:
            recommendations.append("High memory usage detected. Consider restarting the application.")
        
        if system_info["disk_usage"] > 90:
            recommendations.append("Low disk space. Consider cleaning up old files.")
        
        if not service_health["all_services_healthy"]:
            recommendations.append("Some services are unavailable. Check logs for details.")
        
        return {
            "system": system_info,
            "services": service_health,
            "recommendations": recommendations,
            "timestamp": datetime.now().isoformat()
        }
        
    except ImportError:
        # psutil –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω
        return {
            "system": {"status": "psutil not installed"},
            "services": services_status,
            "recommendations": ["Install psutil for detailed system monitoring"],
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        logger.error(f"System stats error: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to get system stats: {str(e)}")

@router.get("/debug/chromadb-search")
async def debug_chromadb_search(
    query: str = "Statutory Rules",
    document_service = Depends(get_document_service)
):
    """–î–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–∏–π endpoint –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏ ChromaDB –ø–æ–∏—Å–∫–∞"""
    try:
        logger.info(f"üîç Debug ChromaDB search for: {query}")
        
        # –ü–æ–ª—É—á–∞–µ–º –ø—Ä—è–º–æ–π –¥–æ—Å—Ç—É–ø –∫ ChromaDB
        chroma_service = document_service.vector_db
        collection = chroma_service.collection
        
        # 1. –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        total_count = collection.count()
        
        # 2. –ü–æ–ª—É—á–∞–µ–º –í–°–ï –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ –∫–æ–ª–ª–µ–∫—Ü–∏–∏
        all_docs = collection.get(
            include=["documents", "metadatas", "embeddings"]
        )
        
        # 3. –ü—Ä—è–º–æ–π –ø–æ–∏—Å–∫ –≤ ChromaDB –±–µ–∑ —Ñ–∏–ª—å—Ç—Ä–æ–≤
        raw_search = collection.query(
            query_texts=[query],
            n_results=10,
            include=["documents", "metadatas", "distances"]
        )
        
        # 4. –ü–æ–∏—Å–∫ —Å –æ—á–µ–Ω—å –Ω–∏–∑–∫–∏–º –ø–æ—Ä–æ–≥–æ–º
        try:
            low_threshold_results = await document_service.search(
                query=query,
                limit=10,
                min_relevance=0.01  # 1%
            )
        except Exception as e:
            low_threshold_results = f"Error: {e}"
        
        # 5. –ò—â–µ–º –Ω–∞—à –¥–æ–∫—É–º–µ–Ω—Ç –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö
        target_doc_id = "tmpj4r82ypb.txt_c758312a"
        target_found = False
        target_info = None
        
        if all_docs["ids"] and target_doc_id in all_docs["ids"]:
            idx = all_docs["ids"].index(target_doc_id)
            target_info = {
                "id": target_doc_id,
                "content_preview": all_docs["documents"][idx][:200] + "...",
                "metadata": all_docs["metadatas"][idx],
                "has_embedding": all_docs["embeddings"] is not None and len(all_docs["embeddings"]) > idx
            }
            target_found = True
        
        # 6. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞
        search_analysis = {
            "total_results": len(raw_search["documents"][0]) if raw_search["documents"] and raw_search["documents"][0] else 0,
            "results_details": []
        }
        
        if raw_search["documents"] and raw_search["documents"][0]:
            for i in range(len(raw_search["documents"][0])):
                result_detail = {
                    "id": raw_search["ids"][0][i],
                    "distance": raw_search["distances"][0][i],
                    "relevance_score": 1 - raw_search["distances"][0][i],
                    "content_preview": raw_search["documents"][0][i][:100] + "...",
                    "metadata": raw_search["metadatas"][0][i],
                    "is_target_doc": raw_search["ids"][0][i] == target_doc_id
                }
                search_analysis["results_details"].append(result_detail)
        
        # 7. –ü—Ä–æ–≤–µ—Ä—è–µ–º embedding —Ñ—É–Ω–∫—Ü–∏—é
        try:
            test_embedding = chroma_service.embedding_function([query])
            embedding_works = True
            embedding_dimension = len(test_embedding[0]) if test_embedding else 0
        except Exception as e:
            embedding_works = False
            embedding_dimension = 0
            test_embedding = f"Error: {e}"
        
        # 8. –¢–µ–∫—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫ (–ø—Ä–æ—Å—Ç–æ–π)
        simple_text_matches = []
        if all_docs["documents"]:
            for i, doc_content in enumerate(all_docs["documents"]):
                if query.lower() in doc_content.lower():
                    simple_text_matches.append({
                        "id": all_docs["ids"][i],
                        "filename": all_docs["metadatas"][i].get("filename", "Unknown"),
                        "match_position": doc_content.lower().find(query.lower()),
                        "content_preview": doc_content[max(0, doc_content.lower().find(query.lower())-50):doc_content.lower().find(query.lower())+150]
                    })
        
        return {
            "debug_info": {
                "query": query,
                "chromadb_status": {
                    "total_documents_in_collection": total_count,
                    "collection_name": collection.name,
                    "embedding_function": str(chroma_service.embedding_function),
                    "embedding_works": embedding_works,
                    "embedding_dimension": embedding_dimension
                },
                "target_document": {
                    "found_in_collection": target_found,
                    "document_info": target_info
                },
                "raw_chromadb_search": search_analysis,
                "simple_text_search": {
                    "matches_found": len(simple_text_matches),
                    "matches": simple_text_matches
                },
                "service_search_results": {
                    "low_threshold_results": low_threshold_results if isinstance(low_threshold_results, str) else f"Found {len(low_threshold_results)} results"
                }
            },
            "recommendations": []
        }
        
    except Exception as e:
        logger.error(f"Debug ChromaDB error: {e}")
        return {
            "debug_info": {"error": str(e)},
            "recommendations": ["Check ChromaDB service status", "Verify embedding function"]
        }

# ====================================
# –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò
# ====================================

def _generate_admin_recommendations(services_status: dict, total_documents: int, vector_db_error: str) -> list:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è –∞–¥–º–∏–Ω–∞"""
    recommendations = []
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∞—Ç—É—Å —Å–µ—Ä–≤–∏—Å–æ–≤
    if not services_status.get("services_ready", False):
        recommendations.append("Some services are not ready - check logs for details")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã
    if total_documents == 0:
        recommendations.append("No documents in database - upload documents to enable AI responses")
    elif total_documents < 5:
        recommendations.append("Few documents available - add more for better AI responses")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–µ–∫—Ç–æ—Ä–Ω—É—é –ë–î
    if vector_db_error:
        recommendations.append(f"Vector database issue: {vector_db_error}")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º LLM
    if not services_status.get("llm_available", False):
        recommendations.append("LLM service not available - check HF_TOKEN configuration")
    elif services_status.get("demo_mode", False):
        recommendations.append("Demo mode active - set LLM_DEMO_MODE=false and configure HF_TOKEN for full AI")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ—à–∏–±–∫–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏
    if services_status.get("total_errors", 0) > 0:
        recommendations.append("Service initialization errors detected - check logs")
    
    # –û–±—â–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
    if not recommendations:
        recommendations.append("System is running optimally")
    
    return recommendations

def _analyze_chat_history():
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∏—Å—Ç–æ—Ä–∏—é —á–∞—Ç–æ–≤"""
    if not chat_history:
        return {
            "total_messages": 0,
            "average_length": 0,
            "languages": {},
            "success_rate": 0
        }
    
    total_length = sum(len(chat.get("message", "")) for chat in chat_history)
    languages = {}
    successful_queries = 0
    
    for chat in chat_history:
        # –ê–Ω–∞–ª–∏–∑ —è–∑—ã–∫–æ–≤
        lang = chat.get("language", "unknown")
        languages[lang] = languages.get(lang, 0) + 1
        
        # –£—Å–ø–µ—à–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã (—Å –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏)
        if chat.get("sources"):
            successful_queries += 1
    
    return {
        "total_messages": len(chat_history),
        "average_length": total_length / len(chat_history),
        "languages": languages,
        "success_rate": (successful_queries / len(chat_history)) * 100
    }

async def _analyze_document_categories(document_service):
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º"""
    try:
        if not document_service:
            return {"error": "Document service unavailable"}
        
        stats = await document_service.get_stats()
        categories = stats.get("categories", [])
        
        # –ï—Å–ª–∏ –µ—Å—Ç—å –¥–æ—Å—Ç—É–ø –∫ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º, –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∏—Ö
        category_counts = {}
        
        if CHROMADB_ENABLED:
            try:
                documents = await document_service.get_all_documents()
                for doc in documents:
                    category = doc.get("category", "unknown")
                    category_counts[category] = category_counts.get(category, 0) + 1
            except:
                # Fallback –∫ –±–∞–∑–æ–≤–æ–º—É —Å–ø–∏—Å–∫—É –∫–∞—Ç–µ–≥–æ—Ä–∏–π
                for cat in categories:
                    category_counts[cat] = 0
        else:
            # SimpleVectorDB –≤–µ—Ä—Å–∏—è
            try:
                import os, json
                db_file = os.path.join(document_service.vector_db.persist_directory, "documents.json")
                if os.path.exists(db_file):
                    with open(db_file, 'r', encoding='utf-8') as f:
                        documents = json.load(f)
                    
                    for doc in documents:
                        category = doc.get("category", "unknown")
                        category_counts[category] = category_counts.get(category, 0) + 1
            except:
                pass
        
        return {
            "total_categories": len(categories),
            "category_distribution": category_counts,
            "most_popular": max(category_counts, key=category_counts.get) if category_counts else None
        }
        
    except Exception as e:
        logger.error(f"Category analysis error: {e}")
        return {"error": str(e)}

async def _get_performance_stats(document_service):
    """–ü–æ–ª—É—á–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
    try:
        start_time = time.time()
        
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø–æ–∏—Å–∫
        search_start = time.time()
        try:
            await document_service.search("test query", limit=1)
            search_time = time.time() - search_start
        except:
            search_time = -1
        
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        stats_start = time.time()
        try:
            await document_service.get_stats()
            stats_time = time.time() - stats_start
        except:
            stats_time = -1
        
        total_time = time.time() - start_time
        
        return {
            "search_response_time": search_time,
            "stats_response_time": stats_time,
            "total_test_time": total_time,
            "database_type": "ChromaDB" if CHROMADB_ENABLED else "SimpleVectorDB",
            "performance_rating": "good" if search_time < 1.0 and search_time > 0 else "needs_improvement"
        }
        
    except Exception as e:
        return {
            "error": str(e),
            "performance_rating": "unknown"
        }