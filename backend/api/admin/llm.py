# ====================================
# –§–ê–ô–õ: backend/api/admin/llm.py (–ù–û–í–´–ô –§–ê–ô–õ)
# –°–æ–∑–¥–∞—Ç—å –Ω–æ–≤—ã–π —Ñ–∞–π–ª –¥–ª—è –∞–¥–º–∏–Ω—Å–∫–∏—Ö endpoints —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è LLM
# ====================================

"""
Admin LLM Endpoints - –ê–¥–º–∏–Ω—Å–∫–∏–µ endpoints –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ LLM
"""

from fastapi import APIRouter, HTTPException, Depends
import logging
import time
from typing import Optional, List, Dict, Any

from models.responses import SuccessResponse
from app.dependencies import get_llm_service, get_services_status
from app.config import settings, get_llm_config, validate_llm_config

router = APIRouter()
logger = logging.getLogger(__name__)

@router.get("/llm/status")
async def get_llm_status(llm_service = Depends(get_llm_service)):
    """–ü–æ–ª—É—á–∏—Ç—å –ø–æ–¥—Ä–æ–±–Ω—ã–π —Å—Ç–∞—Ç—É—Å LLM —Å–µ—Ä–≤–∏—Å–∞"""
    try:
        # –ü–æ–ª—É—á–∞–µ–º —Å—Ç–∞—Ç—É—Å –æ—Ç —Å–µ—Ä–≤–∏—Å–∞
        service_status = await llm_service.get_service_status()
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        config = get_llm_config()
        config_validation = validate_llm_config()
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Å–∏—Å—Ç–µ–º–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
        system_info = {
            "service_type": getattr(llm_service, 'service_type', 'real'),
            "fallback_mode": service_status.get("service_type") == "fallback",
            "config_valid": config_validation["valid"],
            "timestamp": time.time()
        }
        
        return {
            "service_status": service_status,
            "configuration": config,
            "config_validation": config_validation,
            "system_info": system_info
        }
        
    except Exception as e:
        logger.error(f"Error getting LLM status: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to get LLM status: {str(e)}")

@router.get("/llm/models")
async def get_available_models(llm_service = Depends(get_llm_service)):
    """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∞—Ç—É—Å Ollama
        if hasattr(llm_service, 'ollama'):
            health = await llm_service.ollama.check_service_health()
            
            if health["available"]:
                return {
                    "available": True,
                    "models": health["models"],
                    "default_model": health["default_model"],
                    "base_url": health["base_url"],
                    "fallback_models": settings.OLLAMA_FALLBACK_MODELS,
                    "recommended_models": {
                        "fast": "llama3.2:1b",
                        "balanced": "llama3.2",
                        "quality": "llama3.1:8b",
                        "current": health["default_model"]
                    }
                }
            else:
                return {
                    "available": False,
                    "error": health.get("error"),
                    "base_url": health["base_url"],
                    "fallback_models": settings.OLLAMA_FALLBACK_MODELS,
                    "instructions": [
                        "Install Ollama from https://ollama.ai",
                        "Run: ollama pull llama3.2",
                        "Ensure Ollama is running on " + settings.OLLAMA_BASE_URL
                    ]
                }
        else:
            # Fallback service
            return {
                "available": False,
                "error": "LLM service in fallback mode",
                "recommendations": [
                    "Check Ollama installation",
                    "Verify OLLAMA_ENABLED=true in configuration",
                    "Restart the server after fixing issues"
                ]
            }
            
    except Exception as e:
        logger.error(f"Error getting models: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to get models: {str(e)}")

@router.post("/llm/models/pull")
async def pull_model(
    model_name: str,
    llm_service = Depends(get_llm_service)
):
    """–ó–∞–≥—Ä—É–∑–∏—Ç—å –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å –≤ Ollama"""
    try:
        if not hasattr(llm_service, 'ollama'):
            raise HTTPException(status_code=503, detail="Ollama service not available")
        
        logger.info(f"üîÑ Pulling model: {model_name}")
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –∑–∞–≥—Ä—É–∑–∫—É –º–æ–¥–µ–ª–∏
        result = await llm_service.ollama.pull_model(model_name)
        
        if result["success"]:
            logger.info(f"‚úÖ Successfully pulled model: {model_name}")
            return SuccessResponse(
                message=f"Model '{model_name}' pulled successfully",
                data={
                    "model": model_name,
                    "pull_time": time.time(),
                    "available_models": await _get_model_list(llm_service)
                }
            )
        else:
            logger.error(f"‚ùå Failed to pull model {model_name}: {result['error']}")
            raise HTTPException(status_code=400, detail=f"Failed to pull model: {result['error']}")
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error pulling model: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to pull model: {str(e)}")

@router.post("/llm/test")
async def test_llm_generation(
    prompt: str = "What is law?",
    model: Optional[str] = None,
    temperature: Optional[float] = None,
    max_tokens: Optional[int] = None,
    llm_service = Depends(get_llm_service)
):
    """–¢–µ—Å—Ç–æ–≤—ã–π –≤—ã–∑–æ–≤ LLM –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏"""
    try:
        logger.info(f"üß™ Testing LLM generation with prompt: '{prompt[:50]}...'")
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω—ã
        test_model = model or settings.OLLAMA_DEFAULT_MODEL
        test_temperature = temperature if temperature is not None else settings.LLM_TEMPERATURE
        test_max_tokens = max_tokens or settings.LLM_MAX_TOKENS
        
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º —á–µ—Ä–µ–∑ –æ–±—â–∏–π –º–µ—Ç–æ–¥
        if hasattr(llm_service, 'ollama'):
            response = await llm_service.ollama.generate_response(
                prompt=prompt,
                model=test_model,
                temperature=test_temperature,
                max_tokens=test_max_tokens
            )
            
            return {
                "test_successful": response.success,
                "request": {
                    "prompt": prompt,
                    "model": test_model,
                    "temperature": test_temperature,
                    "max_tokens": test_max_tokens
                },
                "response": {
                    "content": response.content,
                    "model": response.model,
                    "tokens_used": response.tokens_used,
                    "response_time": response.response_time,
                    "success": response.success,
                    "error": response.error
                },
                "recommendations": _get_test_recommendations(response)
            }
        else:
            return {
                "test_successful": False,
                "error": "LLM service not available",
                "recommendations": [
                    "Check Ollama installation and configuration",
                    "Ensure Ollama is running on " + settings.OLLAMA_BASE_URL,
                    "Verify model is available: ollama list"
                ]
            }
            
    except Exception as e:
        logger.error(f"LLM test error: {e}")
        return {
            "test_successful": False,
            "error": str(e),
            "recommendations": [
                "Check Ollama service health",
                "Verify model exists",
                "Check server logs for details"
            ]
        }

@router.get("/llm/usage-stats")
async def get_llm_usage_stats():
    """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è LLM"""
    try:
        # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –∏—Å—Ç–æ—Ä–∏—é —á–∞—Ç–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        from api.user.chat import chat_history
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ AI
        total_messages = len(chat_history)
        ai_responses = 0
        total_tokens = 0
        total_time = 0.0
        models_used = {}
        errors = 0
        
        for entry in chat_history:
            ai_stats = entry.get("ai_stats", {})
            
            if ai_stats.get("ai_used", False):
                ai_responses += 1
                total_tokens += ai_stats.get("tokens_used", 0)
                total_time += ai_stats.get("response_time", 0)
                
                model = ai_stats.get("model", "unknown")
                models_used[model] = models_used.get(model, 0) + 1
            
            if ai_stats.get("error"):
                errors += 1
        
        # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        ai_usage_rate = (ai_responses / total_messages * 100) if total_messages > 0 else 0
        avg_tokens = total_tokens / ai_responses if ai_responses > 0 else 0
        avg_time = total_time / ai_responses if ai_responses > 0 else 0
        error_rate = (errors / total_messages * 100) if total_messages > 0 else 0
        
        return {
            "usage_summary": {
                "total_messages": total_messages,
                "ai_responses": ai_responses,
                "ai_usage_rate": ai_usage_rate,
                "error_rate": error_rate
            },
            "performance": {
                "total_tokens_used": total_tokens,
                "average_tokens_per_response": avg_tokens,
                "total_generation_time": total_time,
                "average_response_time": avg_time
            },
            "models_usage": models_used,
            "recommendations": _get_usage_recommendations(ai_usage_rate, error_rate, avg_time)
        }
        
    except Exception as e:
        logger.error(f"Error getting usage stats: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to get usage stats: {str(e)}")

@router.post("/llm/clear-cache")
async def clear_llm_cache():
    """–û—á–∏—Å—Ç–∏—Ç—å –∫—ç—à LLM (–µ—Å–ª–∏ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω)"""
    try:
        # –ü–æ–∫–∞ —á—Ç–æ –ø—Ä–æ—Å—Ç–∞—è –∑–∞–≥–ª—É—à–∫–∞
        # –í –±—É–¥—É—â–µ–º –∑–¥–µ—Å—å –º–æ–∂–µ—Ç –±—ã—Ç—å –ª–æ–≥–∏–∫–∞ –æ—á–∏—Å—Ç–∫–∏ –∫—ç—à–∞ –æ—Ç–≤–µ—Ç–æ–≤
        
        logger.info("üßπ LLM cache cleared")
        
        return SuccessResponse(
            message="LLM cache cleared successfully",
            data={
                "cleared_at": time.time(),
                "note": "Cache clearing functionality to be implemented"
            }
        )
        
    except Exception as e:
        logger.error(f"Error clearing cache: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to clear cache: {str(e)}")

@router.get("/llm/config")
async def get_llm_configuration():
    """–ü–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—É—â—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é LLM"""
    try:
        config = get_llm_config()
        validation = validate_llm_config()
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—É—â–∏–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ —Å–µ—Å—Å–∏–∏
        runtime_info = {
            "current_time": time.time(),
            "settings_source": "config.py",
            "environment_overrides": _get_env_overrides()
        }
        
        return {
            "configuration": config,
            "validation": validation,
            "runtime_info": runtime_info,
            "editable_settings": {
                "temperature": "LLM response creativity (0.0-1.0)",
                "max_tokens": "Maximum response length",
                "timeout": "Request timeout in seconds",
                "cache_enabled": "Enable response caching",
                "demo_mode": "Use demo responses instead of real AI"
            }
        }
        
    except Exception as e:
        logger.error(f"Error getting config: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to get config: {str(e)}")

@router.post("/llm/health-check")
async def run_llm_health_check(llm_service = Depends(get_llm_service)):
    """–ó–∞–ø—É—Å—Ç–∏—Ç—å –ø–æ–ª–Ω—É—é –ø—Ä–æ–≤–µ—Ä–∫—É –∑–¥–æ—Ä–æ–≤—å—è LLM —Å–∏—Å—Ç–µ–º—ã"""
    try:
        logger.info("üè• Running comprehensive LLM health check...")
        
        health_results = {
            "timestamp": time.time(),
            "overall_status": "unknown",
            "checks": {}
        }
        
        # 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ Ollama
        if hasattr(llm_service, 'ollama'):
            ollama_health = await llm_service.ollama.check_service_health()
            health_results["checks"]["ollama_service"] = {
                "status": "pass" if ollama_health["available"] else "fail",
                "details": ollama_health
            }
        else:
            health_results["checks"]["ollama_service"] = {
                "status": "fail",
                "details": {"error": "Ollama service not initialized"}
            }
        
        # 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ –º–æ–¥–µ–ª–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        try:
            test_response = await llm_service.ollama.generate_response(
                prompt="Test prompt for health check",
                model=settings.OLLAMA_DEFAULT_MODEL,
                max_tokens=50
            )
            
            health_results["checks"]["default_model"] = {
                "status": "pass" if test_response.success else "fail",
                "details": {
                    "model": settings.OLLAMA_DEFAULT_MODEL,
                    "response_time": test_response.response_time,
                    "error": test_response.error
                }
            }
        except Exception as e:
            health_results["checks"]["default_model"] = {
                "status": "fail",
                "details": {"error": str(e)}
            }
        
        # 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        config_validation = validate_llm_config()
        health_results["checks"]["configuration"] = {
            "status": "pass" if config_validation["valid"] else "warn",
            "details": config_validation
        }
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ–±—â–∏–π —Å—Ç–∞—Ç—É—Å
        statuses = [check["status"] for check in health_results["checks"].values()]
        if all(status == "pass" for status in statuses):
            health_results["overall_status"] = "healthy"
        elif any(status == "fail" for status in statuses):
            health_results["overall_status"] = "unhealthy"
        else:
            health_results["overall_status"] = "degraded"
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        health_results["recommendations"] = _get_health_recommendations(health_results["checks"])
        
        logger.info(f"üè• Health check completed: {health_results['overall_status']}")
        
        return health_results
        
    except Exception as e:
        logger.error(f"Health check error: {e}")
        raise HTTPException(status_code=500, detail=f"Health check failed: {str(e)}")

# ====================================
# UTILITY FUNCTIONS
# ====================================

async def _get_model_list(llm_service) -> List[str]:
    """–ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
    try:
        if hasattr(llm_service, 'ollama'):
            health = await llm_service.ollama.check_service_health()
            return health.get("models", [])
        return []
    except:
        return []

def _get_test_recommendations(response) -> List[str]:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
    recommendations = []
    
    if not response.success:
        recommendations.append("Check Ollama service status")
        recommendations.append("Verify model is available")
        
        if "connection" in str(response.error).lower():
            recommendations.append("Ensure Ollama is running on " + settings.OLLAMA_BASE_URL)
        
        if "model" in str(response.error).lower():
            recommendations.append("Pull the required model: ollama pull " + settings.OLLAMA_DEFAULT_MODEL)
    
    elif response.response_time > 10:
        recommendations.append("Response time is slow - consider using a smaller model")
        recommendations.append("Check system resources (CPU/RAM usage)")
    
    elif response.tokens_used < 10:
        recommendations.append("Response seems very short - consider adjusting max_tokens")
    
    else:
        recommendations.append("LLM is working correctly")
    
    return recommendations

def _get_usage_recommendations(ai_usage_rate: float, error_rate: float, avg_time: float) -> List[str]:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é"""
    recommendations = []
    
    if ai_usage_rate < 50:
        recommendations.append("AI usage rate is low - check if Ollama is consistently available")
    
    if error_rate > 20:
        recommendations.append("High error rate detected - check Ollama stability")
        recommendations.append("Consider increasing timeout settings")
    
    if avg_time > 5:
        recommendations.append("Average response time is high - consider using a faster model")
        recommendations.append("Check system performance and available resources")
    
    if not recommendations:
        recommendations.append("LLM performance is optimal")
    
    return recommendations

def _get_env_overrides() -> Dict[str, str]:
    """–ü–æ–ª—É—á–∞–µ—Ç –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è"""
    import os
    
    env_vars = [
        "OLLAMA_ENABLED", "OLLAMA_BASE_URL", "OLLAMA_DEFAULT_MODEL",
        "LLM_TEMPERATURE", "LLM_MAX_TOKENS", "LLM_DEMO_MODE"
    ]
    
    overrides = {}
    for var in env_vars:
        value = os.getenv(var)
        if value:
            overrides[var] = value
    
    return overrides

def _get_health_recommendations(checks: Dict) -> List[str]:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–æ–≤–µ—Ä–æ–∫ –∑–¥–æ—Ä–æ–≤—å—è"""
    recommendations = []
    
    ollama_check = checks.get("ollama_service", {})
    if ollama_check.get("status") == "fail":
        recommendations.append("Install and start Ollama service")
        recommendations.append("Visit https://ollama.ai for installation instructions")
    
    model_check = checks.get("default_model", {})
    if model_check.get("status") == "fail":
        recommendations.append(f"Pull the default model: ollama pull {settings.OLLAMA_DEFAULT_MODEL}")
        recommendations.append("Ensure sufficient disk space for model storage")
    
    config_check = checks.get("configuration", {})
    if config_check.get("status") == "warn":
        recommendations.append("Review and fix configuration warnings")
    
    if not recommendations:
        recommendations.append("All LLM components are healthy")
    
    return recommendations