# ====================================
# –§–ê–ô–õ: backend/api/user/chat.py (–û–ë–ù–û–í–õ–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø –° LLM)
# –ó–∞–º–µ–Ω–∏—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π —Ñ–∞–π–ª –ø–æ–ª–Ω–æ—Å—Ç—å—é
# ====================================

"""
User Chat Endpoints - –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ endpoints –¥–ª—è —á–∞—Ç–∞ —Å AI –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π
"""

from fastapi import APIRouter, HTTPException, Depends
from typing import List, Dict, Any
import logging
import time

from models.requests import ChatMessage, ChatHistoryRequest
from models.responses import ChatResponse, ChatHistoryResponse, ChatHistoryItem
from app.dependencies import get_document_service, get_llm_service
from app.config import settings

router = APIRouter()
logger = logging.getLogger(__name__)

# –í—Ä–µ–º–µ–Ω–Ω–æ–µ —Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–ª—è –∏—Å—Ç–æ—Ä–∏–∏ —á–∞—Ç–æ–≤
chat_history: List[Dict[str, Any]] = []

@router.post("/chat", response_model=ChatResponse)
async def chat_with_assistant(
    message: ChatMessage,
    document_service = Depends(get_document_service),
    llm_service = Depends(get_llm_service)
):
    """–û—Å–Ω–æ–≤–Ω–æ–π endpoint –¥–ª—è —á–∞—Ç–∞ —Å —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–º –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–º —Å AI –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π"""
    try:
        search_results = []
        sources = []
        
        # ====================================
        # –≠–¢–ê–ü 1: –ü–û–ò–°–ö –†–ï–õ–ï–í–ê–ù–¢–ù–´–• –î–û–ö–£–ú–ï–ù–¢–û–í
        # ====================================
        try:
            search_results = await document_service.search(
                query=message.message,
                limit=settings.MAX_CONTEXT_DOCUMENTS,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ–Ω—Ñ–∏–≥ –ª–∏–º–∏—Ç
                min_relevance=0.3  # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ 30%
            )
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏ —Ç–æ–ª—å–∫–æ –∏–∑ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            sources = [result.get('filename', 'Unknown') for result in search_results]
            
            logger.info(f"üîç Found {len(search_results)} relevant documents for query: '{message.message[:50]}...'")
            
        except Exception as e:
            logger.error(f"Search error: {e}")
            search_results = []
        
        # ====================================
        # –≠–¢–ê–ü 2: –ì–ï–ù–ï–†–ê–¶–ò–Ø AI –û–¢–í–ï–¢–ê
        # ====================================
        ai_response = None
        response_text = ""
        
        if search_results and len(search_results) > 0:
            try:
                logger.info("ü§ñ Generating AI response based on found documents...")
                
                # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è LLM
                context_documents = []
                for result in search_results:
                    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É –∫–∞–∂–¥–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞
                    content = result.get('content', '')
                    if len(content) > settings.CONTEXT_TRUNCATE_LENGTH:
                        content = content[:settings.CONTEXT_TRUNCATE_LENGTH] + "..."
                    
                    context_doc = {
                        "filename": result.get('filename', 'Unknown'),
                        "content": content,
                        "relevance_score": result.get('relevance_score', 0.0),
                        "metadata": result.get('metadata', {})
                    }
                    context_documents.append(context_doc)
                
                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç —á–µ—Ä–µ–∑ LLM
                ai_response = await llm_service.answer_legal_question(
                    question=message.message,
                    context_documents=context_documents,
                    language=message.language
                )
                
                if ai_response.success and ai_response.content.strip():
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º AI –æ—Ç–≤–µ—Ç
                    response_text = ai_response.content.strip()
                    
                    logger.info(f"‚úÖ AI response generated: {len(response_text)} chars, "
                              f"{ai_response.tokens_used} tokens, {ai_response.response_time:.2f}s")
                else:
                    # AI –Ω–µ —Å–º–æ–≥ –æ—Ç–≤–µ—Ç–∏—Ç—å, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback
                    logger.warning(f"AI response failed: {ai_response.error}")
                    response_text = _generate_fallback_response_with_context(
                        message.message, search_results, message.language, ai_response.error
                    )
                    
            except Exception as e:
                logger.error(f"LLM generation error: {e}")
                # Fallback –µ—Å–ª–∏ AI –ø–æ–ª–Ω–æ—Å—Ç—å—é –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω
                response_text = _generate_fallback_response_with_context(
                    message.message, search_results, message.language, str(e)
                )
        else:
            # –ù–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ - –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç –æ–± –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
            response_text = await _generate_no_context_response(
                message.message, message.language, llm_service
            )
        
        # ====================================
        # –≠–¢–ê–ü 3: –°–û–•–†–ê–ù–ï–ù–ò–ï –í –ò–°–¢–û–†–ò–Æ
        # ====================================
        chat_entry = {
            "message": message.message,
            "response": response_text,
            "language": message.language,
            "sources": sources,
            "timestamp": time.time(),
            "search_stats": {
                "found_documents": len(search_results),
                "has_relevant_results": len(search_results) > 0,
                "search_query": message.message
            },
            "ai_stats": {
                "ai_used": ai_response is not None and ai_response.success,
                "model": ai_response.model if ai_response else "fallback",
                "tokens_used": ai_response.tokens_used if ai_response else 0,
                "response_time": ai_response.response_time if ai_response else 0,
                "error": ai_response.error if ai_response and not ai_response.success else None
            }
        }
        chat_history.append(chat_entry)
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é –ø–æ—Å–ª–µ–¥–Ω–∏–º–∏ 100 —Å–æ–æ–±—â–µ–Ω–∏—è–º–∏
        if len(chat_history) > 100:
            chat_history.pop(0)
        
        # ====================================
        # –≠–¢–ê–ü 4: –õ–û–ì–ò–†–û–í–ê–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–ê
        # ====================================
        if ai_response and ai_response.success:
            logger.info(f"üí¨ Chat response completed with AI: query='{message.message[:30]}...', "
                       f"sources={len(sources)}, model={ai_response.model}")
        else:
            logger.info(f"üí¨ Chat response completed with fallback: query='{message.message[:30]}...', "
                       f"sources={len(sources)}")
        
        return ChatResponse(
            response=response_text,
            sources=sources if sources else None
        )
        
    except Exception as e:
        logger.error(f"Chat error: {e}")
        raise HTTPException(status_code=500, detail=f"Chat service error: {str(e)}")

def _generate_fallback_response_with_context(query: str, search_results: List[Dict], 
                                           language: str, error: str = None) -> str:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç fallback –æ—Ç–≤–µ—Ç –∫–æ–≥–¥–∞ AI –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –Ω–æ –µ—Å—Ç—å –Ω–∞–π–¥–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç"""
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ —Ç–∏–ø–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
    context_snippets = []
    for result in search_results:
        content = result.get('content', '')
        snippet = content[:300] + "..." if len(content) > 300 else content
        filename = result.get('filename', 'Unknown')
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–∏–ø —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
        match_type = result.get('search_info', {}).get('match_type', 'unknown')
        relevance = result.get('relevance_score', 0.0)
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
        if match_type == "exact":
            if language == "uk":
                match_description = "üìç –¢–æ—á–Ω–µ —Å–ø—ñ–≤–ø–∞–¥—ñ–Ω–Ω—è"
            else:
                match_description = "üìç Exact match"
        elif match_type == "semantic":
            if language == "uk":
                match_description = "üîç –°–µ–º–∞–Ω—Ç–∏—á–Ω–µ —Å–ø—ñ–≤–ø–∞–¥—ñ–Ω–Ω—è"
            else:
                match_description = "üîç Semantic match"
        else:
            if language == "uk":
                match_description = f"üìä –†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ñ—Å—Ç—å: {relevance:.1%}"
            else:
                match_description = f"üìä Relevance: {relevance:.1%}"
        
        context_snippets.append(f"üìÑ {filename} ({match_description}): {snippet}")
    
    context = "\n\n".join(context_snippets)
    
    if language == "uk":
        response_text = f"""üîç –†–µ–∑—É–ª—å—Ç–∞—Ç–∏ –ø–æ—à—É–∫—É –¥–ª—è –∑–∞–ø–∏—Ç–∞–Ω–Ω—è: "{query}"

üìö –ó–Ω–∞–π–¥–µ–Ω–æ {len(search_results)} —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤ —É –±–∞–∑—ñ –∑–Ω–∞–Ω—å:

{context}

‚ö†Ô∏è AI-–ø–æ–º—ñ—á–Ω–∏–∫ —Ç–∏–º—á–∞—Å–æ–≤–æ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∏–π"""
        
        if error:
            response_text += f" (–ø—Ä–∏—á–∏–Ω–∞: {error})"
        
        response_text += """.

üí° –ù–∞ –æ—Å–Ω–æ–≤—ñ –∑–Ω–∞–π–¥–µ–Ω–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤ –≤–∏ –º–æ–∂–µ—Ç–µ:
‚Ä¢ –ü–µ—Ä–µ–≥–ª—è–Ω—É—Ç–∏ –ø–æ–≤–Ω–∏–π —Ç–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤ —á–µ—Ä–µ–∑ –∞–¥–º—ñ–Ω –ø–∞–Ω–µ–ª—å
‚Ä¢ –£—Ç–æ—á–Ω–∏—Ç–∏ –∑–∞–ø–∏—Ç –¥–ª—è –∫—Ä–∞—â–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤  
‚Ä¢ –°–ø—Ä–æ–±—É–≤–∞—Ç–∏ –ø–æ—à—É–∫ —ñ–Ω—à–∏–º–∏ –∫–ª—é—á–æ–≤–∏–º–∏ —Å–ª–æ–≤–∞–º–∏

üîß –î–ª—è –∞–¥–º—ñ–Ω—ñ—Å—Ç—Ä–∞—Ç–æ—Ä–∞: –ø–µ—Ä–µ–≤—ñ—Ä—Ç–µ —Å—Ç–∞—Ç—É—Å Ollama —Å–µ—Ä–≤—ñ—Å—É –Ω–∞ http://localhost:11434"""
    else:
        response_text = f"""üîç Search results for query: "{query}"

üìö Found {len(search_results)} relevant documents in the knowledge base:

{context}

‚ö†Ô∏è AI assistant temporarily unavailable"""
        
        if error:
            response_text += f" (reason: {error})"
            
        response_text += """.

üí° Based on the found documents, you can:
‚Ä¢ Review the full document text through the admin panel
‚Ä¢ Refine your query for better results
‚Ä¢ Try searching with different keywords

üîß For administrator: check Ollama service status at http://localhost:11434"""
    
    return response_text

async def _generate_no_context_response(query: str, language: str, llm_service) -> str:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –∫–æ–≥–¥–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
    
    # –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å AI –æ—Ç–≤–µ—Ç –¥–∞–∂–µ –±–µ–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ (–æ–±—â–∏–µ –∑–Ω–∞–Ω–∏—è)
    try:
        ai_response = await llm_service.answer_legal_question(
            question=query,
            context_documents=[],
            language=language
        )
        
        if ai_response.success and ai_response.content.strip():
            # AI —Å–º–æ–≥ –æ—Ç–≤–µ—Ç–∏—Ç—å –±–µ–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            if language == "uk":
                return f"""ü§ñ –í—ñ–¥–ø–æ–≤—ñ–¥—å –Ω–∞ –æ—Å–Ω–æ–≤—ñ –∑–∞–≥–∞–ª—å–Ω–∏—Ö –∑–Ω–∞–Ω—å:

{ai_response.content}

‚ö†Ô∏è –ó–≤–µ—Ä–Ω—ñ—Ç—å —É–≤–∞–≥—É: —Ü—è –≤—ñ–¥–ø–æ–≤—ñ–¥—å –±–∞–∑—É—î—Ç—å—Å—è –Ω–∞ –∑–∞–≥–∞–ª—å–Ω–∏—Ö –∑–Ω–∞–Ω–Ω—è—Ö AI, –∞ –Ω–µ –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö —É –≤–∞—à—ñ–π –±–∞–∑—ñ –∑–Ω–∞–Ω—å. –î–ª—è –±—ñ–ª—å—à —Ç–æ—á–Ω–æ—ó —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó —Ä–µ–∫–æ–º–µ–Ω–¥—É—î–º–æ –¥–æ–¥–∞—Ç–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ñ –¥–æ–∫—É–º–µ–Ω—Ç–∏ —á–µ—Ä–µ–∑ –∞–¥–º—ñ–Ω –ø–∞–Ω–µ–ª—å."""
            else:
                return f"""ü§ñ Response based on general knowledge:

{ai_response.content}

‚ö†Ô∏è Note: This response is based on the AI's general knowledge, not on documents in your knowledge base. For more accurate information, we recommend adding relevant documents through the admin panel."""
        
    except Exception as e:
        logger.debug(f"AI general knowledge response failed: {e}")
    
    # Fallback –µ—Å–ª–∏ AI –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω —Å–æ–≤—Å–µ–º
    try:
        # –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
        # (–ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º —á—Ç–æ document_service –¥–æ—Å—Ç—É–ø–µ–Ω –∏–∑ –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞)
        stats = {"total_documents": "–Ω–µ–≤—ñ–¥–æ–º–æ" if language == "uk" else "unknown"}
    except:
        stats = {"total_documents": "–Ω–µ–≤—ñ–¥–æ–º–æ" if language == "uk" else "unknown"}
    
    if language == "uk":
        response_text = f"""üîç –†–µ–∑—É–ª—å—Ç–∞—Ç–∏ –ø–æ—à—É–∫—É –¥–ª—è –∑–∞–ø–∏—Ç–∞–Ω–Ω—è: "{query}"

‚ùå –ù–∞ –∂–∞–ª—å, –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤ —É –±–∞–∑—ñ –∑–Ω–∞–Ω—å.

ü§ñ AI-–ø–æ–º—ñ—á–Ω–∏–∫ —Ç–∞–∫–æ–∂ —Ç–∏–º—á–∞—Å–æ–≤–æ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∏–π –¥–ª—è –Ω–∞–¥–∞–Ω–Ω—è –∑–∞–≥–∞–ª—å–Ω–æ—ó –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ.

ü§î –ú–æ–∂–ª–∏–≤—ñ –ø—Ä–∏—á–∏–Ω–∏:
‚Ä¢ –ó–∞–ø–∏—Ç–∞–Ω–Ω—è –∑–∞–Ω–∞–¥—Ç–æ —Å–ø–µ—Ü–∏—Ñ—ñ—á–Ω–µ –∞–±–æ –º—ñ—Å—Ç–∏—Ç—å —Ç–µ—Ä–º—ñ–Ω–∏, —è–∫–∏—Ö –Ω–µ–º–∞—î –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö
‚Ä¢ –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—Ç—å—Å—è —Å–∏–Ω–æ–Ω—ñ–º–∏ –∞–±–æ —ñ–Ω—à–∞ —Ç–µ—Ä–º—ñ–Ω–æ–ª–æ–≥—ñ—è
‚Ä¢ –î–æ–∫—É–º–µ–Ω—Ç–∏ –∑ —Ç–∞–∫–æ—é —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—î—é —â–µ –Ω–µ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω—ñ –≤ —Å–∏—Å—Ç–µ–º—É
‚Ä¢ Ollama —Å–µ—Ä–≤—ñ—Å –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∏–π

üí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó:
‚Ä¢ –°–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª—é–≤–∞—Ç–∏ –∑–∞–ø–∏—Ç –±—ñ–ª—å—à –∑–∞–≥–∞–ª—å–Ω–∏–º–∏ —Ç–µ—Ä–º—ñ–Ω–∞–º–∏
‚Ä¢ –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π—Ç–µ –∫–ª—é—á–æ–≤—ñ —Å–ª–æ–≤–∞ –∑–∞–º—ñ—Å—Ç—å —Ü—ñ–ª–∏—Ö —Ä–µ—á–µ–Ω—å
‚Ä¢ –î–æ–¥–∞–π—Ç–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ñ –¥–æ–∫—É–º–µ–Ω—Ç–∏ —á–µ—Ä–µ–∑ –ø–∞–Ω–µ–ª—å –∫–µ—Ä—É–≤–∞–Ω–Ω—è
‚Ä¢ –ü–µ—Ä–µ–≤—ñ—Ä—Ç–µ —Ä–æ–±–æ—Ç—É Ollama –Ω–∞ http://localhost:11434

üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –±–∞–∑–∏ –∑–Ω–∞–Ω—å:
‚Ä¢ –î–æ—Å—Ç—É–ø–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤: {stats.get('total_documents', '–Ω–µ–≤—ñ–¥–æ–º–æ')}"""
    else:
        response_text = f"""üîç Search results for query: "{query}"

‚ùå Unfortunately, no relevant documents found in the knowledge base.

ü§ñ AI assistant is also temporarily unavailable to provide a general response.

ü§î Possible reasons:
‚Ä¢ The query is too specific or contains terms not present in documents
‚Ä¢ Using synonyms or different terminology than in documents
‚Ä¢ Relevant documents haven't been uploaded to the system yet
‚Ä¢ Ollama service is unavailable

üí° Recommendations:
‚Ä¢ Try rephrasing the query with more general terms
‚Ä¢ Use keywords instead of full sentences
‚Ä¢ Add relevant documents through the admin panel
‚Ä¢ Check Ollama service at http://localhost:11434

üìä Knowledge base statistics:
‚Ä¢ Available documents: {stats.get('total_documents', 'unknown')}"""
    
    return response_text

@router.get("/chat/history", response_model=ChatHistoryResponse)
async def get_chat_history(request: ChatHistoryRequest = ChatHistoryRequest()):
    """–ü–æ–ª—É—á–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é —á–∞—Ç–∞"""
    try:
        # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ N —Å–æ–æ–±—â–µ–Ω–∏–π
        recent_history = chat_history[-request.limit:] if chat_history else []
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ –Ω—É–∂–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
        formatted_history = []
        for entry in recent_history:
            formatted_entry = ChatHistoryItem(
                message=entry["message"],
                response=entry["response"],
                language=entry["language"],
                sources=entry.get("sources"),
                timestamp=entry.get("timestamp")
            )
            formatted_history.append(formatted_entry)
        
        return ChatHistoryResponse(
            history=formatted_history,
            total_messages=len(chat_history)
        )
        
    except Exception as e:
        logger.error(f"Chat history error: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to get chat history: {str(e)}")

@router.delete("/chat/history")
async def clear_chat_history():
    """–û—á–∏—Å—Ç–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é —á–∞—Ç–æ–≤"""
    try:
        global chat_history
        old_count = len(chat_history)
        chat_history.clear()
        
        logger.info(f"Chat history cleared: {old_count} messages removed")
        
        return {
            "message": f"Cleared {old_count} chat messages",
            "remaining": len(chat_history)
        }
        
    except Exception as e:
        logger.error(f"Clear history error: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to clear chat history: {str(e)}")

@router.get("/chat/stats")
async def get_chat_stats():
    """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —á–∞—Ç–æ–≤ —Å AI –º–µ—Ç—Ä–∏–∫–∞–º–∏"""
    try:
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —è–∑—ã–∫–∏
        languages = {}
        sources_used = 0
        successful_searches = 0
        total_search_results = 0
        ai_responses = 0
        total_tokens = 0
        total_ai_time = 0.0
        
        for entry in chat_history:
            lang = entry.get("language", "unknown")
            languages[lang] = languages.get(lang, 0) + 1
            
            if entry.get("sources"):
                sources_used += 1
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ–∏—Å–∫–∞
            search_stats = entry.get("search_stats", {})
            if search_stats.get("has_relevant_results", False):
                successful_searches += 1
                total_search_results += search_stats.get("found_documents", 0)
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º AI —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            ai_stats = entry.get("ai_stats", {})
            if ai_stats.get("ai_used", False):
                ai_responses += 1
                total_tokens += ai_stats.get("tokens_used", 0)
                total_ai_time += ai_stats.get("response_time", 0)
        
        return {
            "total_messages": len(chat_history),
            "languages": languages,
            "messages_with_sources": sources_used,
            "successful_searches": successful_searches,
            "success_rate": (successful_searches / len(chat_history) * 100) if chat_history else 0,
            "average_sources_per_message": sources_used / len(chat_history) if chat_history else 0,
            "average_results_per_successful_search": total_search_results / successful_searches if successful_searches > 0 else 0,
            
            # AI —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            "ai_responses": ai_responses,
            "ai_usage_rate": (ai_responses / len(chat_history) * 100) if chat_history else 0,
            "total_tokens_used": total_tokens,
            "average_tokens_per_ai_response": total_tokens / ai_responses if ai_responses > 0 else 0,
            "total_ai_time": total_ai_time,
            "average_ai_response_time": total_ai_time / ai_responses if ai_responses > 0 else 0
        }
        
    except Exception as e:
        logger.error(f"Chat stats error: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to get chat stats: {str(e)}")

@router.get("/chat/search-test")
async def test_search_functionality(
    query: str = "test",
    min_relevance: float = 0.3,
    document_service = Depends(get_document_service)
):
    """–¢–µ—Å—Ç–æ–≤—ã–π endpoint –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø–æ–∏—Å–∫–∞"""
    try:
        logger.info(f"Testing search with query: '{query}', min_relevance: {min_relevance}")
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫
        results = await document_service.search(
            query=query,
            limit=5,
            min_relevance=min_relevance
        )
        
        # –ü–æ–ª—É—á–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –±–∞–∑—ã
        try:
            stats = await document_service.get_stats()
        except:
            stats = {"error": "Could not get stats"}
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –¥–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç
        test_result = {
            "query": query,
            "min_relevance_threshold": min_relevance,
            "found_results": len(results),
            "database_stats": stats,
            "results_details": []
        }
        
        for result in results:
            result_detail = {
                "filename": result.get('filename', 'Unknown'),
                "relevance_score": result.get('relevance_score', 0.0),
                "match_type": result.get('search_info', {}).get('match_type', 'unknown'),
                "confidence": result.get('search_info', {}).get('confidence', 'unknown'),
                "content_preview": result.get('content', '')[:100] + "..." if result.get('content') else "No content"
            }
            test_result["results_details"].append(result_detail)
        
        return {
            "test_successful": True,
            "message": f"Search test completed for query '{query}'",
            "test_result": test_result
        }
        
    except Exception as e:
        logger.error(f"Search test error: {e}")
        return {
            "test_successful": False,
            "message": f"Search test failed for query '{query}'",
            "error": str(e)
        }

@router.get("/chat/llm-direct-test")
async def test_llm_direct():
    """–ü—Ä—è–º–æ–π —Ç–µ—Å—Ç LLM –±–µ–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏"""
    try:
        import aiohttp
        import json
        
        logger.info("üß™ Testing direct Ollama API call...")
        
        # –ü—Ä—è–º–æ–π –≤—ã–∑–æ–≤ –∫ Ollama API
        payload = {
            "model": "llama3:latest",
            "prompt": "Hello, respond with just 'Working!'",
            "stream": False
        }
        
        timeout = aiohttp.ClientTimeout(total=30)
        async with aiohttp.ClientSession(timeout=timeout) as session:
            start_time = time.time()
            
            async with session.post(
                "http://localhost:11434/api/generate",
                json=payload,
                headers={"Content-Type": "application/json"}
            ) as response:
                end_time = time.time()
                
                if response.status == 200:
                    data = await response.json()
                    content = data.get("response", "No response")
                    
                    return {
                        "test": "direct_ollama_api",
                        "success": True,
                        "response_time": end_time - start_time,
                        "status_code": response.status,
                        "response_content": content,
                        "payload_sent": payload
                    }
                else:
                    error_text = await response.text()
                    return {
                        "test": "direct_ollama_api", 
                        "success": False,
                        "response_time": end_time - start_time,
                        "status_code": response.status,
                        "error": error_text
                    }
                    
    except Exception as e:
        return {
            "test": "direct_ollama_api",
            "success": False,
            "error": str(e),
            "recommendation": "Check if Ollama is running on http://localhost:11434"
        }
async def test_llm_functionality(
    question: str = "What is law?",
    language: str = "en",
    llm_service = Depends(get_llm_service)
):
    """–¢–µ—Å—Ç–æ–≤—ã–π endpoint –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ LLM"""
    try:
        logger.info(f"Testing LLM with question: '{question}', language: {language}")
        
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º LLM –±–µ–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        ai_response = await llm_service.answer_legal_question(
            question=question,
            context_documents=[],
            language=language
        )
        
        # –ü–æ–ª—É—á–∞–µ–º —Å—Ç–∞—Ç—É—Å —Å–µ—Ä–≤–∏—Å–∞
        service_status = await llm_service.get_service_status()
        
        return {
            "test_successful": ai_response.success,
            "question": question,
            "language": language,
            "ai_response": {
                "content": ai_response.content,
                "model": ai_response.model,
                "tokens_used": ai_response.tokens_used,
                "response_time": ai_response.response_time,
                "success": ai_response.success,
                "error": ai_response.error
            },
            "service_status": service_status
        }
        
    except Exception as e:
        logger.error(f"LLM test error: {e}")
        return {
            "test_successful": False,
            "question": question,
            "error": str(e)
        }