# backend/api/user/chat.py - –£–ü–†–û–©–ï–ù–ù–´–ô –ß–ê–¢ API
"""
–£–ø—Ä–æ—â–µ–Ω–Ω—ã–π —á–∞—Ç endpoint –¥–ª—è –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π RAG —Å–∏—Å—Ç–µ–º—ã
–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω –ø–æ–¥ FLAN-T5 Small –∏ –±—ã—Å—Ç—Ä—ã–µ –æ—Ç–≤–µ—Ç—ã
"""

from fastapi import APIRouter, HTTPException, Depends
from typing import List, Dict, Any
import logging
import time
import asyncio

from models.requests import ChatMessage, ChatHistoryRequest
from models.responses import ChatResponse, ChatHistoryResponse, ChatHistoryItem
from app.dependencies import get_document_service, get_llm_service
from app.config import settings

router = APIRouter()
logger = logging.getLogger(__name__)

# –í—Ä–µ–º–µ–Ω–Ω–æ–µ —Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–ª—è –∏—Å—Ç–æ—Ä–∏–∏ —á–∞—Ç–æ–≤ (—É–ø—Ä–æ—â–µ–Ω–Ω–æ)
chat_history: List[Dict[str, Any]] = []

# –£–ø—Ä–æ—â–µ–Ω–Ω—ã–µ —Ç–∞–π–º–∞—É—Ç—ã –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –º–æ–¥–µ–ª–∏
SEARCH_TIMEOUT = 5.0    # 5 —Å–µ–∫—É–Ω–¥ –Ω–∞ –ø–æ–∏—Å–∫
LLM_TIMEOUT = 20.0      # 20 —Å–µ–∫—É–Ω–¥ –Ω–∞ FLAN-T5
TOTAL_TIMEOUT = 30.0    # 30 —Å–µ–∫—É–Ω–¥ –æ–±—â–∏–π —Ç–∞–π–º–∞—É—Ç

@router.post("/chat", response_model=ChatResponse)
async def chat_with_assistant(
    message: ChatMessage,
    document_service = Depends(get_document_service),
    llm_service = Depends(get_llm_service)
):
    """–û—Å–Ω–æ–≤–Ω–æ–π endpoint –¥–ª—è —á–∞—Ç–∞ —Å –±—ã—Å—Ç—Ä—ã–º–∏ —Ç–∞–π–º–∞—É—Ç–∞–º–∏"""
    
    try:
        return await asyncio.wait_for(
            _process_chat_message(message, document_service, llm_service),
            timeout=TOTAL_TIMEOUT
        )
    except asyncio.TimeoutError:
        logger.error(f"‚ùå Chat request timeout after {TOTAL_TIMEOUT}s")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∏—Å—Ç–æ—Ä–∏—é –¥–∞–∂–µ –ø—Ä–∏ —Ç–∞–π–º–∞—É—Ç–µ
        timeout_entry = {
            "message": message.message,
            "response": "‚è∞ Request timeout - please try a shorter question",
            "language": message.language,
            "sources": [],
            "timestamp": time.time(),
            "timeout": True
        }
        chat_history.append(timeout_entry)
        
        timeout_response = "‚è∞ –ó–∞–ø–∏—Ç –ø–µ—Ä–µ–≤–∏—â–∏–≤ —á–∞—Å –æ—á—ñ–∫—É–≤–∞–Ω–Ω—è. –°–ø—Ä–æ–±—É–π—Ç–µ –∫–æ—Ä–æ—Ç—à–µ –ø–∏—Ç–∞–Ω–Ω—è." if message.language == "uk" else "‚è∞ Request timeout. Please try a shorter question."
        
        return ChatResponse(
            response=timeout_response,
            sources=[]
        )
    except Exception as e:
        logger.error(f"‚ùå Critical chat error: {e}")
        raise HTTPException(status_code=500, detail=f"Chat service error: {str(e)}")

async def _process_chat_message(message: ChatMessage, document_service, llm_service):
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏–µ —á–∞—Ç–∞ —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –ø–æ–¥ FLAN-T5"""
    
    search_results = []
    sources = []
    search_start_time = time.time()
    
    # ====================================
    # –≠–¢–ê–ü 1: –ë–´–°–¢–†–´–ô –ü–û–ò–°–ö –î–û–ö–£–ú–ï–ù–¢–û–í
    # ====================================
    try:
        logger.info(f"üîç Searching for: '{message.message[:50]}...'")
        
        # –ü–æ–∏—Å–∫ —Å –∫–æ—Ä–æ—Ç–∫–∏–º —Ç–∞–π–º–∞—É—Ç–æ–º
        search_results = await asyncio.wait_for(
            document_service.search(
                query=message.message,
                limit=settings.MAX_CONTEXT_DOCUMENTS,  # –û–±—ã—á–Ω–æ 2 –¥–ª—è FLAN-T5
                min_relevance=0.4  # –ß—É—Ç—å –≤—ã—à–µ –ø–æ—Ä–æ–≥ –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–∞
            ),
            timeout=SEARCH_TIMEOUT
        )
        
        search_time = time.time() - search_start_time
        
        if search_results and isinstance(search_results, list) and len(search_results) > 0:
            sources = [result.get('filename', 'Unknown') for result in search_results]
            logger.info(f"‚úÖ Search completed: {len(search_results)} results in {search_time:.2f}s")
        else:
            logger.info(f"‚ÑπÔ∏è No relevant documents found in {search_time:.2f}s")
            search_results = []
        
    except asyncio.TimeoutError:
        search_time = time.time() - search_start_time
        logger.error(f"‚ùå Search timeout after {SEARCH_TIMEOUT}s")
        search_results = []
        
    except Exception as e:
        search_time = time.time() - search_start_time
        logger.error(f"‚ùå Search error after {search_time:.2f}s: {e}")
        search_results = []
    
    # ====================================
    # –≠–¢–ê–ü 2: FLAN-T5 –ì–ï–ù–ï–†–ê–¶–ò–Ø
    # ====================================
    ai_response = None
    response_text = ""
    llm_start_time = time.time()
    
    try:
        logger.info("ü§ñ Generating FLAN-T5 response...")
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è FLAN-T5 (–∫–æ—Ä–æ—Ç–∫–∏–π)
        context_documents = []
        if search_results:
            for result in search_results[:2]:  # –ú–∞–∫—Å–∏–º—É–º 2 –¥–æ–∫—É–º–µ–Ω—Ç–∞
                content = result.get('content', '')
                # –ö–æ—Ä–æ—Ç–∫–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è T5 Small
                if len(content) > settings.CONTEXT_TRUNCATE_LENGTH:
                    content = content[:settings.CONTEXT_TRUNCATE_LENGTH] + "..."
                
                context_doc = {
                    "filename": result.get('filename', 'Unknown'),
                    "content": content,
                    "relevance_score": result.get('relevance_score', 0.0),
                    "metadata": result.get('metadata', {})
                }
                context_documents.append(context_doc)
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç —á–µ—Ä–µ–∑ FLAN-T5 —Å —Ç–∞–π–º–∞—É—Ç–æ–º
        ai_response = await asyncio.wait_for(
            llm_service.answer_legal_question(
                question=message.message,
                context_documents=context_documents,
                language=message.language
            ),
            timeout=LLM_TIMEOUT
        )
        
        llm_time = time.time() - llm_start_time
        
        if ai_response and ai_response.success and ai_response.content.strip():
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º AI –æ—Ç–≤–µ—Ç
            response_text = ai_response.content.strip()
            logger.info(f"‚úÖ FLAN-T5 response generated: {len(response_text)} chars in {llm_time:.2f}s")
        else:
            # AI –Ω–µ —Å–º–æ–≥ –æ—Ç–≤–µ—Ç–∏—Ç—å, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback
            logger.warning(f"AI response failed: {getattr(ai_response, 'error', 'Unknown error')}")
            response_text = _generate_quick_fallback_response(
                message.message, search_results, message.language
            )
                
    except asyncio.TimeoutError:
        llm_time = time.time() - llm_start_time
        logger.error(f"‚ùå FLAN-T5 timeout after {LLM_TIMEOUT}s")
        response_text = _generate_timeout_fallback_response(
            message.message, search_results, message.language
        )
        
    except Exception as e:
        llm_time = time.time() - llm_start_time
        logger.error(f"‚ùå FLAN-T5 generation error after {llm_time:.2f}s: {e}")
        response_text = _generate_quick_fallback_response(
            message.message, search_results, message.language
        )
    
    # ====================================
    # –≠–¢–ê–ü 3: –°–û–•–†–ê–ù–ï–ù–ò–ï –í –ò–°–¢–û–†–ò–Æ
    # ====================================
    chat_entry = {
        "message": message.message,
        "response": response_text,
        "language": message.language,
        "sources": sources,
        "timestamp": time.time(),
        "search_stats": {
            "found_documents": len(search_results),
            "search_time": search_time if 'search_time' in locals() else 0.0
        },
        "ai_stats": {
            "ai_used": ai_response is not None and getattr(ai_response, 'success', False),
            "model": "google/flan-t5-small",
            "tokens_used": getattr(ai_response, 'tokens_used', 0),
            "response_time": getattr(ai_response, 'response_time', 0.0),
            "llm_time": llm_time if 'llm_time' in locals() else 0.0
        }
    }
    chat_history.append(chat_entry)
    
    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
    if len(chat_history) > 50:  # –ú–µ–Ω—å—à–µ –ª–∏–º–∏—Ç
        chat_history.pop(0)
    
    # ====================================
    # –≠–¢–ê–ü 4: –í–û–ó–í–†–ê–¢ –†–ï–ó–£–õ–¨–¢–ê–¢–ê
    # ====================================
    total_time = time.time() - search_start_time
    
    if ai_response and getattr(ai_response, 'success', False):
        logger.info(f"üí¨ Chat completed with FLAN-T5: '{message.message[:30]}...', "
                   f"sources={len(sources)}, total_time={total_time:.2f}s")
    else:
        logger.info(f"üí¨ Chat completed with fallback: '{message.message[:30]}...', "
                   f"sources={len(sources)}, total_time={total_time:.2f}s")
    
    return ChatResponse(
        response=response_text,
        sources=sources if sources else None
    )

def _generate_quick_fallback_response(query: str, search_results: List[Dict], language: str) -> str:
    """–ë—ã—Å—Ç—Ä—ã–π fallback –æ—Ç–≤–µ—Ç —Å –Ω–∞–π–¥–µ–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º"""
    
    if search_results:
        # –ö—Ä–∞—Ç–∫–∏–π –æ—Ç–≤–µ—Ç —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
        context_preview = search_results[0].get('content', '')[:150] + "..." if search_results[0].get('content') else ""
        filename = search_results[0].get('filename', 'Unknown')
        
        if language == "uk":
            response_text = f"""üîç **–ó–Ω–∞–π–¥–µ–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—É —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é**

**–ü–∏—Ç–∞–Ω–Ω—è:** {query}

üìÑ **–î–∂–µ—Ä–µ–ª–æ:** {filename}
üìù **–í–∏—Ç—è–≥:** {context_preview}

‚ö†Ô∏è FLAN-T5 —Ç–∏–º—á–∞—Å–æ–≤–æ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∏–π. –Ü–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è –∑–Ω–∞–π–¥–µ–Ω–∞ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö."""
        else:
            response_text = f"""üîç **Found Relevant Information**

**Question:** {query}

üìÑ **Source:** {filename}
üìù **Extract:** {context_preview}

‚ö†Ô∏è FLAN-T5 temporarily unavailable. Information found in documents."""
    else:
        # –ù–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        if language == "uk":
            response_text = f"""üîç **–†–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ—à—É–∫—É**

**–ü–∏—Ç–∞–Ω–Ω—è:** {query}

‚ùå –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ.
‚ö†Ô∏è FLAN-T5 —Å–µ—Ä–≤—ñ—Å –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∏–π.

üí° –°–ø—Ä–æ–±—É–π—Ç–µ —ñ–Ω—à—ñ –∫–ª—é—á–æ–≤—ñ —Å–ª–æ–≤–∞ –∞–±–æ –¥–æ–¥–∞–π—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç–∏."""
        else:
            response_text = f"""üîç **Search Result**

**Question:** {query}

‚ùå No relevant documents found.
‚ö†Ô∏è FLAN-T5 service unavailable.

üí° Try different keywords or add documents."""
    
    return response_text

def _generate_timeout_fallback_response(query: str, search_results: List[Dict], language: str) -> str:
    """–û—Ç–≤–µ—Ç –ø—Ä–∏ —Ç–∞–π–º–∞—É—Ç–µ FLAN-T5"""
    
    if language == "uk":
        response_text = f"""‚è∞ **–¢–∞–π–º–∞—É—Ç FLAN-T5**

**–ü–∏—Ç–∞–Ω–Ω—è:** {query}

‚ö†Ô∏è FLAN-T5 Small –º–æ–¥–µ–ª—å –ø–µ—Ä–µ–≤–∏—â–∏–ª–∞ —á–∞—Å –æ—á—ñ–∫—É–≤–∞–Ω–Ω—è.

üìö –ó–Ω–∞–π–¥–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤: {len(search_results)}

üí° **–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó:**
‚Ä¢ –°–ø—Ä–æ–±—É–π—Ç–µ –∫–æ—Ä–æ—Ç—à–µ –ø–∏—Ç–∞–Ω–Ω—è
‚Ä¢ –ü–µ—Ä–µ—Ñ—Ä–∞–∑—É–π—Ç–µ –ø—Ä–æ—Å—Ç—ñ—à–µ
‚Ä¢ –ú–æ–¥–µ–ª—å –º–æ–∂–µ –±—É—Ç–∏ –ø–µ—Ä–µ–≤–∞–Ω—Ç–∞–∂–µ–Ω–∞"""
    else:
        response_text = f"""‚è∞ **FLAN-T5 Timeout**

**Question:** {query}

‚ö†Ô∏è FLAN-T5 Small model exceeded timeout.

üìö Documents found: {len(search_results)}

üí° **Recommendations:**
‚Ä¢ Try a shorter question
‚Ä¢ Rephrase more simply  
‚Ä¢ Model may be overloaded"""
    
    return response_text

@router.get("/chat/history", response_model=ChatHistoryResponse)
async def get_chat_history(request: ChatHistoryRequest = ChatHistoryRequest()):
    """–ü–æ–ª—É—á–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é —á–∞—Ç–∞"""
    try:
        # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ N —Å–æ–æ–±—â–µ–Ω–∏–π
        recent_history = chat_history[-request.limit:] if chat_history else []
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ –Ω—É–∂–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
        formatted_history = []
        for entry in recent_history:
            formatted_entry = ChatHistoryItem(
                message=entry["message"],
                response=entry["response"],
                language=entry["language"],
                sources=entry.get("sources"),
                timestamp=entry.get("timestamp")
            )
            formatted_history.append(formatted_entry)
        
        return ChatHistoryResponse(
            history=formatted_history,
            total_messages=len(chat_history)
        )
        
    except Exception as e:
        logger.error(f"Chat history error: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to get chat history: {str(e)}")

@router.delete("/chat/history")
async def clear_chat_history():
    """–û—á–∏—Å—Ç–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é —á–∞—Ç–æ–≤"""
    try:
        global chat_history
        old_count = len(chat_history)
        chat_history.clear()
        
        logger.info(f"Chat history cleared: {old_count} messages removed")
        
        return {
            "message": f"Cleared {old_count} chat messages",
            "remaining": len(chat_history)
        }
        
    except Exception as e:
        logger.error(f"Clear history error: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to clear chat history: {str(e)}")

@router.get("/chat/stats")
async def get_chat_stats():
    """–ü–æ–ª—É—á–∏—Ç—å —É–ø—Ä–æ—â–µ–Ω–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —á–∞—Ç–æ–≤"""
    try:
        if not chat_history:
            return {
                "total_messages": 0,
                "ai_responses": 0,
                "average_response_time": 0,
                "languages": {},
                "success_rate": 0
            }
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —è–∑—ã–∫–∏
        languages = {}
        sources_used = 0
        successful_searches = 0
        ai_responses = 0
        total_tokens = 0
        total_ai_time = 0.0
        timeouts = 0
        
        for entry in chat_history:
            lang = entry.get("language", "unknown")
            languages[lang] = languages.get(lang, 0) + 1
            
            if entry.get("sources"):
                sources_used += 1
            
            if entry.get("timeout", False):
                timeouts += 1
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ–∏—Å–∫–∞
            search_stats = entry.get("search_stats", {})
            if search_stats.get("found_documents", 0) > 0:
                successful_searches += 1
            
            # AI —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            ai_stats = entry.get("ai_stats", {})
            if ai_stats.get("ai_used", False):
                ai_responses += 1
                total_tokens += ai_stats.get("tokens_used", 0)
                total_ai_time += ai_stats.get("response_time", 0)
        
        return {
            "total_messages": len(chat_history),
            "languages": languages,
            "messages_with_sources": sources_used,
            "successful_searches": successful_searches,
            "success_rate": (successful_searches / len(chat_history) * 100) if chat_history else 0,
            
            # FLAN-T5 —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            "ai_responses": ai_responses,
            "ai_usage_rate": (ai_responses / len(chat_history) * 100) if chat_history else 0,
            "total_tokens_used": total_tokens,
            "average_tokens_per_response": total_tokens / ai_responses if ai_responses > 0 else 0,
            "total_ai_time": total_ai_time,
            "average_ai_response_time": total_ai_time / ai_responses if ai_responses > 0 else 0,
            
            # –¢–∞–π–º–∞—É—Ç—ã
            "timeouts": timeouts,
            "timeout_rate": (timeouts / len(chat_history) * 100) if chat_history else 0,
            
            # –ú–æ–¥–µ–ª—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
            "model_info": {
                "llm_model": "google/flan-t5-small",
                "embedding_model": "sentence-transformers/all-MiniLM-L6-v2",
                "max_context_docs": 2,
                "memory_optimized": True
            }
        }
        
    except Exception as e:
        logger.error(f"Chat stats error: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to get chat stats: {str(e)}")

@router.get("/chat/model-info")
async def get_model_info():
    """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª—è—Ö"""
    try:
        from app.dependencies import get_llm_service
        llm_service = get_llm_service()
        llm_status = await llm_service.get_service_status()
        
        return {
            "llm": {
                "model": "google/flan-t5-small",
                "type": "text2text-generation",
                "parameters": "80M",
                "memory_usage": "~300 MB",
                "ready": llm_status.get("ready", False),
                "service_type": llm_status.get("service_type", "unknown")
            },
            "embedding": {
                "model": "sentence-transformers/all-MiniLM-L6-v2",
                "dimensions": 384,
                "memory_usage": "~90 MB",
                "type": "sentence-transformer"
            },
            "vector_db": {
                "type": "ChromaDB",
                "memory_usage": "~20 MB per 10K docs",
                "features": ["semantic_search", "similarity_threshold"]
            },
            "system": {
                "total_memory_target": "<1GB RAM",
                "optimizations": [
                    "Small model sizes",
                    "Efficient embeddings", 
                    "Short context windows",
                    "Fast inference"
                ]
            }
        }
        
    except Exception as e:
        logger.error(f"Model info error: {e}")
        return {
            "error": str(e),
            "fallback_info": {
                "llm_model": "google/flan-t5-small",
                "embedding_model": "sentence-transformers/all-MiniLM-L6-v2",
                "status": "Model information unavailable"
            }
        }

@router.post("/chat/test")
async def test_chat():
    """–¢–µ—Å—Ç–æ–≤—ã–π endpoint –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —á–∞—Ç–∞"""
    test_message = ChatMessage(
        message="What is law?",
        language="en"
    )
    
    try:
        from app.dependencies import get_document_service, get_llm_service
        doc_service = get_document_service()
        llm_service = get_llm_service()
        
        start_time = time.time()
        result = await _process_chat_message(test_message, doc_service, llm_service)
        end_time = time.time()
        
        return {
            "test_successful": True,
            "response": result.response[:200] + "..." if len(result.response) > 200 else result.response,
            "sources_found": len(result.sources) if result.sources else 0,
            "response_time": end_time - start_time,
            "model": "google/flan-t5-small"
        }
        
    except Exception as e:
        return {
            "test_successful": False,
            "error": str(e),
            "model": "google/flan-t5-small",
            "recommendations": [
                "Check FLAN-T5 model availability",
                "Verify transformers installation",
                "Check HuggingFace Hub access"
            ]
        }