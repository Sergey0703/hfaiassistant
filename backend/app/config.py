# backend/app/config.py - –ü–û–õ–ù–ê–Ø –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø –° –ü–†–ò–û–†–ò–¢–ï–¢–û–ú ENV VARIABLES
"""
–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è HuggingFace Spaces
Environment variables –∏–º–µ—é—Ç –ü–†–ò–û–†–ò–¢–ï–¢ –Ω–∞–¥ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –≤ –∫–æ–¥–µ
"""

import os
from typing import List

# ====================================
# –û–°–ù–û–í–ù–´–ï –ù–ê–°–¢–†–û–ô–ö–ò –ü–†–ò–õ–û–ñ–ï–ù–ò–Ø
# ====================================

class Settings:
    """–ö–ª–∞—Å—Å –Ω–∞—Å—Ç—Ä–æ–µ–∫ —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è"""
    
    # –û—Å–Ω–æ–≤–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ API
    API_V1_PREFIX: str = "/api"
    PROJECT_NAME: str = "Legal Assistant API"
    VERSION: str = "2.0.0"
    DESCRIPTION: str = "AI Legal Assistant with Llama integration"
    
    # CORS
    CORS_ORIGINS: List[str] = [
        "http://localhost:3000",
        "http://127.0.0.1:3000", 
        "*"  # –î–ª—è HuggingFace Spaces
    ]
    
    # ====================================
    # LLM –ù–ê–°–¢–†–û–ô–ö–ò –° –ü–†–ò–û–†–ò–¢–ï–¢–û–ú ENV VARIABLES
    # ====================================
    
    # –ö–†–ò–¢–ò–ß–ù–û: Environment variables –ø–µ—Ä–µ–∑–∞–ø–∏—Å—ã–≤–∞—é—Ç —ç—Ç–∏ –∑–Ω–∞—á–µ–Ω–∏—è!
    # –ß—Ç–æ –≤—ã –≤–∏–¥–∏—Ç–µ –≤ HF Spaces settings –∏–º–µ–µ—Ç –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
    
    # –ü—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã–µ —Ä–∞–±–æ—á–∏–µ –º–æ–¥–µ–ª–∏ (fallback –µ—Å–ª–∏ –Ω–µ –∑–∞–¥–∞–Ω–æ –≤ ENV)
    _DEFAULT_PRIMARY_MODEL = "google/flan-t5-base"  # –ù–∞–¥–µ–∂–Ω–∞—è
    _DEFAULT_FAST_MODEL = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
    _BACKUP_MODELS = [
    "meta-llama/Llama-2-7b-chat-hf",
    "distilgpt2", 
    "gpt2"
]
    
    # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: ENV -> –ö–æ–¥
    @property
    def LLM_PRIMARY_MODEL(self) -> str:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –Ω–∞–∑–≤–∞–Ω–∏–π –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
        model = (
            os.getenv("LLM_MODEL") or           # –í–∞—à–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –≤ HF Spaces
            os.getenv("LLM_PRIMARY_MODEL") or
            os.getenv("HUGGINGFACE_MODEL") or
            self._DEFAULT_PRIMARY_MODEL
        )
        print(f"ü§ñ Using LLM model: {model}")
        return model
    
    @property 
    def LLM_FAST_MODEL(self) -> str:
        return os.getenv("LLM_FAST_MODEL", self._DEFAULT_FAST_MODEL)
    
    @property
    def LLM_BACKUP_MODELS(self) -> List[str]:
        # –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ–∑–µ—Ä–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏ –µ—Å–ª–∏ –æ—Å–Ω–æ–≤–Ω–∞—è –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç
        backup_env = os.getenv("LLM_BACKUP_MODELS")
        if backup_env:
            return backup_env.split(",")
        return self._BACKUP_MODELS
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (—Ç–æ–∂–µ –º–æ–∂–Ω–æ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å —á–µ—Ä–µ–∑ ENV)
    @property
    def LLM_MAX_TOKENS(self) -> int:
        return int(os.getenv("LLM_MAX_TOKENS", "200"))
    
    @property
    def LLM_TEMPERATURE(self) -> float:
        return float(os.getenv("LLM_TEMPERATURE", "0.3"))
    
    @property
    def LLM_TIMEOUT(self) -> int:
        return int(os.getenv("LLM_TIMEOUT", "30"))
    
    # HuggingFace –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
    @property
    def HF_TOKEN(self) -> str:
        return os.getenv("HF_TOKEN", "")
    
    @property
    def LLM_DEMO_MODE(self) -> bool:
        return os.getenv("LLM_DEMO_MODE", "false").lower() == "true"
    
    # ====================================
    # –ë–ê–ó–ê –î–ê–ù–ù–´–• –ò –ü–û–ò–°–ö
    # ====================================
    
    @property
    def USE_CHROMADB(self) -> bool:
        return os.getenv("USE_CHROMADB", "true").lower() == "true"
    
    @property 
    def CHROMADB_PATH(self) -> str:
        return os.getenv("CHROMADB_PATH", "./chromadb_data")
    
    # –≠–º–±–µ–¥–¥–∏–Ω–≥–∏
    EMBEDDING_MODEL: str = "all-MiniLM-L6-v2"
    CHUNK_SIZE: int = 1000
    CHUNK_OVERLAP: int = 200
    
    # –ü–æ–∏—Å–∫
    DEFAULT_SEARCH_LIMIT: int = 5
    MAX_SEARCH_LIMIT: int = 20
    MAX_CONTEXT_DOCUMENTS: int = 2  # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –¥–ª—è HF Spaces
    CONTEXT_TRUNCATE_LENGTH: int = 500  # –°–æ–∫—Ä–∞—â–µ–Ω–æ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
    
    # ====================================
    # –§–ê–ô–õ–´ –ò –ó–ê–ì–†–£–ó–ö–ò
    # ====================================
    
    MAX_FILE_SIZE: int = 10 * 1024 * 1024  # 10MB
    ALLOWED_FILE_TYPES: List[str] = [".txt", ".pdf", ".docx", ".md"]
    
    # ====================================
    # –í–ï–ë-–°–ö–†–ê–ü–ò–ù–ì
    # ====================================
    
    SCRAPING_DELAY: float = 1.5
    SCRAPING_TIMEOUT: int = 15
    MAX_URLS_PER_REQUEST: int = 10
    
    # ====================================
    # –õ–û–ì–ò–†–û–í–ê–ù–ò–ï –ò –ú–û–ù–ò–¢–û–†–ò–ù–ì
    # ====================================
    
    @property
    def LOG_LEVEL(self) -> str:
        return os.getenv("LOG_LEVEL", "INFO")
    
    # ====================================
    # –°–ü–ï–¶–ò–ê–õ–¨–ù–´–ï –†–ï–ñ–ò–ú–´
    # ====================================
    
    @property
    def HUGGINGFACE_SPACES(self) -> bool:
        return os.getenv("SPACE_ID") is not None
    
    # –Ø–∑—ã–∫–∏
    SUPPORTED_LANGUAGES: List[str] = ["en", "uk"]
    DEFAULT_LANGUAGE: str = "en"
    
    def get_active_model_info(self) -> dict:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –∞–∫—Ç–∏–≤–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        return {
            "primary_model": self.LLM_PRIMARY_MODEL,
            "fast_model": self.LLM_FAST_MODEL, 
            "backup_models": self.LLM_BACKUP_MODELS,
            "source": "ENV variable" if os.getenv("LLM_MODEL") else "default config",
            "demo_mode": self.LLM_DEMO_MODE,
            "hf_token_configured": bool(self.HF_TOKEN),
            "parameters": {
                "max_tokens": self.LLM_MAX_TOKENS,
                "temperature": self.LLM_TEMPERATURE,
                "timeout": self.LLM_TIMEOUT
            }
        }
    
    def validate_model_availability(self) -> dict:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –Ω–∞—Å—Ç—Ä–æ–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        validation = {
            "primary_model_set": bool(self.LLM_PRIMARY_MODEL),
            "hf_token_available": bool(self.HF_TOKEN),
            "known_working_models": [
                "microsoft/phi-2",
                "google/flan-t5-base", 
                "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "microsoft/DialoGPT-small"
            ],
            "current_model": self.LLM_PRIMARY_MODEL,
            "is_known_working": self.LLM_PRIMARY_MODEL in [
                "microsoft/phi-2",
                "google/flan-t5-base",
                "TinyLlama/TinyLlama-1.1B-Chat-v1.0", 
                "microsoft/DialoGPT-small",
                "distilgpt2",
                "gpt2"
            ]
        }
        
        return validation

# –°–æ–∑–¥–∞—ë–º –≥–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä
settings = Settings()

# ====================================
# –ö–û–ù–°–¢–ê–ù–¢–´ –ò –°–ü–†–ê–í–û–ß–ù–ò–ö–ò 
# ====================================

# API –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
API_METADATA = {
    "title": settings.PROJECT_NAME,
    "version": settings.VERSION,
    "description": settings.DESCRIPTION,
    "contact": {
        "name": "Legal Assistant Team",
        "email": "support@legalassistant.com"
    }
}

# API —Ç–µ–≥–∏ –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏
API_TAGS = [
    {
        "name": "User Chat",
        "description": "User chat endpoints for legal assistance"
    },
    {
        "name": "User Search",
        "description": "Document search endpoints for users"
    },
    {
        "name": "Admin Documents",
        "description": "Document management for administrators"
    },
    {
        "name": "Admin Scraper", 
        "description": "Web scraping for administrators"
    },
    {
        "name": "Admin Stats",
        "description": "Statistics and analytics"
    },
    {
        "name": "Admin LLM",
        "description": "LLM management and monitoring"
    },
    {
        "name": "System",
        "description": "System health and information"
    }
]

# –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
DOCUMENT_CATEGORIES = [
    "general",
    "legislation",
    "jurisprudence", 
    "government",
    "civil_rights",
    "scraped",
    "ukraine_legal",
    "ireland_legal"
]

# –ü—Ä–µ–¥—É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–µ —Å–∞–π—Ç—ã
UKRAINE_LEGAL_URLS = [
    "https://zakon.rada.gov.ua/laws/main",
    "https://court.gov.ua/",
    "https://minjust.gov.ua/",
    "https://ccu.gov.ua/"
]

IRELAND_LEGAL_URLS = [
    "https://www.irishstatutebook.ie/",
    "https://www.courts.ie/",
    "https://www.citizensinformation.ie/en/",
    "https://www.justice.ie/"
]

# ====================================
# –§–£–ù–ö–¶–ò–ò –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–ò
# ====================================

def get_llm_config() -> dict:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é LLM"""
    return settings.get_active_model_info()

def get_database_config() -> dict:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"""
    return {
        "use_chromadb": settings.USE_CHROMADB,
        "chromadb_path": settings.CHROMADB_PATH,
        "embedding_model": settings.EMBEDDING_MODEL,
        "chunk_size": settings.CHUNK_SIZE,
        "chunk_overlap": settings.CHUNK_OVERLAP,
        "max_context_documents": settings.MAX_CONTEXT_DOCUMENTS
    }

def get_api_config() -> dict:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é API"""
    return {
        "project_name": settings.PROJECT_NAME,
        "version": settings.VERSION,
        "description": settings.DESCRIPTION,
        "cors_origins": settings.CORS_ORIGINS,
        "api_prefix": settings.API_V1_PREFIX
    }

def validate_config() -> dict:
    """–í–∞–ª–∏–¥–∞—Ü–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –º–æ–¥–µ–ª–∏"""
    issues = []
    warnings = []
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–æ–¥–µ–ª—å
    model_validation = settings.validate_model_availability()
    if not model_validation["is_known_working"]:
        warnings.append(f"Model '{settings.LLM_PRIMARY_MODEL}' may not work - consider tested alternatives")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º HF —Ç–æ–∫–µ–Ω
    if not settings.HF_TOKEN and not settings.LLM_DEMO_MODE:
        warnings.append("HF_TOKEN not set - using public inference (rate limited)")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—É—Ç–∏
    if settings.USE_CHROMADB:
        try:
            os.makedirs(settings.CHROMADB_PATH, exist_ok=True)
        except Exception as e:
            issues.append(f"Cannot create ChromaDB directory: {e}")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ LLM
    if settings.LLM_MAX_TOKENS < 50:
        warnings.append("LLM_MAX_TOKENS is very low (< 50)")
    
    if settings.LLM_TEMPERATURE < 0 or settings.LLM_TEMPERATURE > 1:
        issues.append("LLM_TEMPERATURE must be between 0 and 1")
    
    return {
        "valid": len(issues) == 0,
        "issues": issues,
        "warnings": warnings,
        "model_info": model_validation,
        "config_summary": {
            "active_model": settings.LLM_PRIMARY_MODEL,
            "model_source": "ENV variable" if os.getenv("LLM_MODEL") else "default",
            "chromadb_enabled": settings.USE_CHROMADB,
            "hf_spaces": settings.HUGGINGFACE_SPACES,
            "demo_mode": settings.LLM_DEMO_MODE
        }
    }

def get_environment_info() -> dict:
    """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± –æ–∫—Ä—É–∂–µ–Ω–∏–∏"""
    return {
        "huggingface_spaces": settings.HUGGINGFACE_SPACES,
        "space_id": os.getenv("SPACE_ID"),
        "demo_mode": settings.LLM_DEMO_MODE,
        "log_level": settings.LOG_LEVEL,
        "chromadb_enabled": settings.USE_CHROMADB,
        "supported_languages": settings.SUPPORTED_LANGUAGES,
        "active_model": settings.LLM_PRIMARY_MODEL,
        "model_source": "ENV" if os.getenv("LLM_MODEL") else "CONFIG",
        "environment_variables": {
            "HF_TOKEN": "configured" if settings.HF_TOKEN else "not_set",
            "LLM_MODEL": os.getenv("LLM_MODEL", "not_set"),
            "USE_CHROMADB": os.getenv("USE_CHROMADB", "true"),
            "LLM_DEMO_MODE": os.getenv("LLM_DEMO_MODE", "false"),
            "LOG_LEVEL": os.getenv("LOG_LEVEL", "INFO"),
            "SPACE_ID": "configured" if os.getenv("SPACE_ID") else "not_set"
        }
    }

def get_full_config_summary() -> dict:
    """–ü–æ–ª–Ω–∞—è —Å–≤–æ–¥–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
    validation = validate_config()
    
    return {
        "validation": validation,
        "llm": get_llm_config(),
        "database": get_database_config(),
        "api": get_api_config(),
        "environment": get_environment_info(),
        "categories": DOCUMENT_CATEGORIES,
        "predefined_sites": {
            "ukraine": len(UKRAINE_LEGAL_URLS),
            "ireland": len(IRELAND_LEGAL_URLS)
        }
    }

def validate_llm_config() -> dict:
    """–í–∞–ª–∏–¥–∞—Ü–∏—è LLM –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ (–¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å –∞–¥–º–∏–Ω API)"""
    return validate_config()

# ====================================
# –ú–û–î–ï–õ–¨ –ù–ï –†–ê–ë–û–¢–ê–ï–¢? –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò
# ====================================

def get_model_recommendations() -> dict:
    """–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –º–æ–¥–µ–ª—è–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–ø—ã—Ç–∞"""
    return {
        "tested_working": {
            "microsoft/phi-2": "–õ—É—á—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ, —Å—Ç–∞–±–∏–ª—å–Ω–∞—è",
            "google/flan-t5-base": "–°–∞–º–∞—è –Ω–∞–¥–µ–∂–Ω–∞—è, –±—ã—Å—Ç—Ä–∞—è",
            "TinyLlama/TinyLlama-1.1B-Chat-v1.0": "–û—á–µ–Ω—å –±—ã—Å—Ç—Ä–∞—è",
            "microsoft/DialoGPT-small": "–î–ª—è —á–∞—Ç–æ–≤, –º–µ–Ω—å—à–µ medium",
            "distilgpt2": "–ü—Ä–æ—Å—Ç–∞—è –∏ –±—ã—Å—Ç—Ä–∞—è",
            "gpt2": "–ü–æ—Å–ª–µ–¥–Ω–∏–π fallback"
        },
        "not_recommended": {
            "microsoft/DialoGPT-medium": "–ß–∞—Å—Ç–æ –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç –≤ HF API",
            "microsoft/DialoGPT-large": "–°–ª–∏—à–∫–æ–º –±–æ–ª—å—à–∞—è –¥–ª—è HF free tier"
        },
        "current_setup": {
            "your_model": settings.LLM_PRIMARY_MODEL,
            "source": "ENV variable" if os.getenv("LLM_MODEL") else "config default",
            "recommendation": "–ü–æ–ø—Ä–æ–±—É–π—Ç–µ microsoft/phi-2 –∏–ª–∏ google/flan-t5-base –µ—Å–ª–∏ –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç"
        }
    }

# ====================================
# –ê–í–¢–û–î–ò–ê–ì–ù–û–°–¢–ò–ö–ê –ü–†–ò –ó–ê–ü–£–°–ö–ï
# ====================================

def print_config_summary():
    """–í—ã–≤–æ–¥–∏—Ç —Å–≤–æ–¥–∫—É –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ"""
    model_info = settings.get_active_model_info()
    print("\nü§ñ LLM Configuration:")
    print(f"   Primary Model: {model_info['primary_model']}")
    print(f"   Source: {model_info['source']}")
    print(f"   Demo Mode: {model_info['demo_mode']}")
    print(f"   HF Token: {'‚úÖ Set' if model_info['hf_token_configured'] else '‚ùå Not set'}")
    
    if not model_info['hf_token_configured']:
        print("   üí° Tip: Set HF_TOKEN for better rate limits")
    
    if model_info['primary_model'] == "microsoft/DialoGPT-medium":
        print("   ‚ö†Ô∏è Warning: DialoGPT-medium often fails - try microsoft/phi-2")

# –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–∑–æ–≤ –ø—Ä–∏ –∏–º–ø–æ—Ä—Ç–µ (—Ç–æ–ª—å–∫–æ –≤ dev —Ä–µ–∂–∏–º–µ)
if os.getenv("DEBUG") or os.getenv("SHOW_CONFIG_SUMMARY"):
    print_config_summary()

# ====================================
# –≠–ö–°–ü–û–†–¢
# ====================================

__all__ = [
    # –û—Å–Ω–æ–≤–Ω–æ–π –∫–ª–∞—Å—Å –∏ —ç–∫–∑–µ–º–ø–ª—è—Ä
    "Settings", 
    "settings",
    
    # –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã
    "API_METADATA",
    "API_TAGS",
    "DOCUMENT_CATEGORIES",
    "UKRAINE_LEGAL_URLS",
    "IRELAND_LEGAL_URLS",
    
    # –§—É–Ω–∫—Ü–∏–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    "get_llm_config",
    "get_database_config", 
    "get_api_config",
    "validate_config",
    "get_environment_info",
    "get_full_config_summary",
    "validate_llm_config",
    "get_model_recommendations",
    "print_config_summary"
]