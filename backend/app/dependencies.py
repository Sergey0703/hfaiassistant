# ====================================
# –§–ê–ô–õ: backend/app/dependencies.py (–û–ë–ù–û–í–õ–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø –î–õ–Ø HUGGINGFACE SPACES)
# –ó–∞–º–µ–Ω–∏—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π —Ñ–∞–π–ª –ø–æ–ª–Ω–æ—Å—Ç—å—é
# ====================================

"""
–ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Ä–≤–∏—Å–æ–≤ –¥–ª—è HuggingFace Spaces
"""

import logging
import sys
import os
import time
import json
import asyncio
from typing import Optional, List, Dict, Any

from app.config import settings

logger = logging.getLogger(__name__)

# ====================================
# –ö–õ–ê–°–°–´-–ó–ê–ì–õ–£–®–ö–ò –î–õ–Ø –ù–ï–î–û–°–¢–£–ü–ù–´–• –°–ï–†–í–ò–°–û–í
# ====================================

class FallbackDocumentService:
    """–ó–∞–≥–ª—É—à–∫–∞ –¥–ª—è document service –∫–æ–≥–¥–∞ –æ—Å–Ω–æ–≤–Ω–æ–π —Å–µ—Ä–≤–∏—Å –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω"""
    
    def __init__(self):
        self.service_type = "fallback"
        # –î–æ–±–∞–≤–ª—è–µ–º –∞—Ç—Ä–∏–±—É—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ –æ–∂–∏–¥–∞–µ—Ç –∫–æ–¥
        self.vector_db = type('MockVectorDB', (), {
            'persist_directory': './fallback_db'
        })()
        logger.info("üìù Using fallback document service")
    
    async def search(self, query: str, category: str = None, limit: int = 5, min_relevance: float = 0.3):
        """–ó–∞–≥–ª—É—à–∫–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        logger.warning(f"Fallback search called with query: {query}")
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –¥–µ–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        demo_results = [
            {
                "content": f"Demo search result for '{query}'. Install document processing dependencies for real search.",
                "filename": "demo_document.txt",
                "document_id": f"demo_{int(time.time())}",
                "relevance_score": 0.95,
                "metadata": {
                    "status": "demo",
                    "category": category or "general",
                    "service": "fallback"
                }
            }
        ]
        
        return demo_results
    
    async def get_stats(self):
        """–ó–∞–≥–ª—É—à–∫–∞ –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"""
        return {
            "total_documents": 0,
            "categories": ["general", "demo"],
            "database_type": "Fallback Service",
            "error": "Document service not initialized - install dependencies",
            "available_features": [
                "Demo search responses",
                "Basic API structure", 
                "Error handling"
            ],
            "missing_dependencies": [
                "sentence-transformers (for ChromaDB)",
                "ChromaDB or SimpleVectorDB setup"
            ]
        }
    
    async def get_all_documents(self):
        """–ó–∞–≥–ª—É—à–∫–∞ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –≤—Å–µ—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        logger.warning("Fallback get_all_documents called")
        return []
    
    async def delete_document(self, doc_id: str):
        """–ó–∞–≥–ª—É—à–∫–∞ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        logger.warning(f"Fallback delete_document called for ID: {doc_id}")
        return False
    
    async def process_and_store_file(self, file_path: str, category: str = "general"):
        """–ó–∞–≥–ª—É—à–∫–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞"""
        logger.warning(f"Fallback process_and_store_file called for: {file_path}")
        return False
    
    async def update_document(self, doc_id: str, content: str = None, metadata: Dict = None):
        """–ó–∞–≥–ª—É—à–∫–∞ –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        logger.warning(f"Fallback update_document called for ID: {doc_id}")
        return False

class FallbackScraperService:
    """–ó–∞–≥–ª—É—à–∫–∞ –¥–ª—è scraper service –∫–æ–≥–¥–∞ –æ—Å–Ω–æ–≤–Ω–æ–π —Å–µ—Ä–≤–∏—Å –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω"""
    
    def __init__(self):
        self.service_type = "fallback"
        self.legal_sites_config = {}
        logger.info("üåê Using fallback scraper service")
    
    async def scrape_legal_site(self, url: str):
        """–ó–∞–≥–ª—É—à–∫–∞ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å–∞–π—Ç–∞"""
        logger.warning(f"Fallback scraper called for URL: {url}")
        
        # –°–æ–∑–¥–∞–µ–º demo –¥–æ–∫—É–º–µ–Ω—Ç
        demo_content = f"""
DEMO: Legal Document from {url}

‚ö†Ô∏è This is a demonstration document. Real web scraping is unavailable.

To enable real scraping, install the required dependencies:
pip install aiohttp beautifulsoup4

üìã Demo Content:
This document would normally contain the actual content from the website.
In real mode, the scraper would extract legal text, articles, and regulations
from the specified URL using advanced HTML parsing techniques.

üîç Scraped from: {url}
üìÖ Demo generated at: {time.strftime('%Y-%m-%d %H:%M:%S')}
üè∑Ô∏è Status: Fallback mode

For full functionality, please install the scraping dependencies.
"""
        
        # –°–æ–∑–¥–∞–µ–º –æ–±—ä–µ–∫—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞
        return type('DemoDocument', (), {
            'url': url,
            'title': f'DEMO: Legal Document from {url}',
            'content': demo_content.strip(),
            'metadata': {
                'status': 'demo',
                'real_scraping': False,
                'scraped_at': time.time(),
                'service': 'fallback',
                'url': url,
                'demo_version': '2.0'
            },
            'category': 'demo'
        })()
    
    async def scrape_multiple_urls(self, urls: List[str], delay: float = 1.0):
        """–ó–∞–≥–ª—É—à–∫–∞ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö URL"""
        logger.warning(f"Fallback bulk scraper called for {len(urls)} URLs")
        
        results = []
        for i, url in enumerate(urls):
            if i > 0 and delay > 0:
                await asyncio.sleep(delay)
            
            doc = await self.scrape_legal_site(url)
            results.append(doc)
        
        return results

class FallbackLLMService:
    """–ó–∞–≥–ª—É—à–∫–∞ –¥–ª—è LLM service –∫–æ–≥–¥–∞ HuggingFace –º–æ–¥–µ–ª–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã"""
    
    def __init__(self):
        self.service_type = "fallback"
        self.model_loaded = False
        logger.info("ü§ñ Using fallback LLM service")
    
    async def answer_legal_question(self, question: str, context_documents: List[Dict], language: str = "en"):
        """–ó–∞–≥–ª—É—à–∫–∞ –¥–ª—è –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–µ –≤–æ–ø—Ä–æ—Å—ã"""
        logger.warning(f"Fallback LLM called for question: {question[:50]}...")
        
        # –°–æ–∑–¥–∞–µ–º –¥–µ–º–æ –æ—Ç–≤–µ—Ç
        from services.huggingface_llm_service import LLMResponse
        
        if language == "uk":
            demo_content = f"""‚ö†Ô∏è –î–ï–ú–û –†–ï–ñ–ò–ú: LLM –º–æ–¥–µ–ª—å –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞
            
–ù–∞ –æ—Å–Ω–æ–≤—ñ –∑–Ω–∞–π–¥–µ–Ω–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤ —è –±–∏ –≤—ñ–¥–ø–æ–≤—ñ–≤ –Ω–∞ –≤–∞—à–µ –∑–∞–ø–∏—Ç–∞–Ω–Ω—è: "{question}"

üìö –ó–Ω–∞–π–¥–µ–Ω–æ {len(context_documents)} —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤ —É –±–∞–∑—ñ –∑–Ω–∞–Ω—å.

üí° –î–ª—è –æ—Ç—Ä–∏–º–∞–Ω–Ω—è –ø–æ–≤–Ω–æ—Ü—ñ–Ω–Ω–∏—Ö AI-–≤—ñ–¥–ø–æ–≤—ñ–¥–µ–π:
1. –ü–µ—Ä–µ–≤—ñ—Ä—Ç–µ, —â–æ –º–æ–¥–µ–ª—å –∑–∞–≤–∞–Ω—Ç–∞–∂—É—î—Ç—å—Å—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ
2. –ú–æ–∂–ª–∏–≤–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ –±—ñ–ª—å—à–µ –ø–∞–º'—è—Ç—ñ –¥–ª—è –º–æ–¥–µ–ª—ñ
3. –°–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç–∏ Space

üîß –ü–æ—Ç–æ—á–Ω–∏–π —Å—Ç–∞—Ç—É—Å: HuggingFace –º–æ–¥–µ–ª—å –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞"""
        else:
            demo_content = f"""‚ö†Ô∏è DEMO MODE: LLM model unavailable
            
Based on the found documents, I would answer your question: "{question}"

üìö Found {len(context_documents)} relevant documents in the knowledge base.

üí° To get full AI responses:
1. Check that the model is loading correctly
2. More memory might be needed for the model
3. Try restarting the Space

üîß Current status: HuggingFace model unavailable"""
        
        return LLMResponse(
            content=demo_content,
            model="fallback",
            tokens_used=0,
            response_time=0.1,
            success=False,
            error="HuggingFace model not available"
        )
    
    async def get_service_status(self):
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç—É—Å fallback LLM —Å–µ—Ä–≤–∏—Å–∞"""
        return {
            "model_loaded": False,
            "model_name": "fallback",
            "huggingface_available": False,
            "service_type": "fallback",
            "supported_languages": ["en", "uk"],
            "error": "HuggingFace model not available - check model loading",
            "recommendations": [
                "Check that transformers library is installed",
                "Verify model downloads correctly",
                "Check available memory for model loading",
                "Review logs for model initialization errors"
            ]
        }

# ====================================
# –ì–õ–û–ë–ê–õ–¨–ù–´–ï –ü–ï–†–ï–ú–ï–ù–ù–´–ï –°–ï–†–í–ò–°–û–í
# ====================================

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ —Å–µ—Ä–≤–∏—Å—ã
document_service: Optional[object] = None
scraper: Optional[object] = None
llm_service: Optional[object] = None
SERVICES_AVAILABLE: bool = False
CHROMADB_ENABLED: bool = False
LLM_ENABLED: bool = False

async def init_services():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤—Å–µ—Ö —Å–µ—Ä–≤–∏—Å–æ–≤ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è –¥–ª—è HuggingFace Spaces"""
    global document_service, scraper, llm_service, SERVICES_AVAILABLE, CHROMADB_ENABLED, LLM_ENABLED
    
    logger.info("üîß Initializing services for HuggingFace Spaces...")
    
    # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—É—â—É—é –ø–∞–ø–∫—É –≤ Python path
    current_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    if current_dir not in sys.path:
        sys.path.append(current_dir)
    
    # ====================================
    # –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø –°–ï–†–í–ò–°–ê –î–û–ö–£–ú–ï–ù–¢–û–í
    # ====================================
    try:
        if settings.USE_CHROMADB:
            # –ü—ã—Ç–∞–µ–º—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å ChromaDB
            try:
                from services.chroma_service import DocumentService
                document_service = DocumentService(settings.CHROMADB_PATH)
                CHROMADB_ENABLED = True
                logger.info("‚úÖ ChromaDB service initialized")
            except ImportError as e:
                logger.warning(f"ChromaDB not available, falling back to SimpleVectorDB: {e}")
                try:
                    from services.document_processor import DocumentService
                    document_service = DocumentService(settings.SIMPLE_DB_PATH)
                    CHROMADB_ENABLED = False
                    logger.info("‚úÖ SimpleVectorDB service initialized")
                except ImportError as e2:
                    logger.error(f"SimpleVectorDB also not available: {e2}")
                    document_service = None
        else:
            # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º SimpleVectorDB
            try:
                from services.document_processor import DocumentService
                document_service = DocumentService(settings.SIMPLE_DB_PATH)
                CHROMADB_ENABLED = False
                logger.info("‚úÖ SimpleVectorDB service initialized (forced)")
            except ImportError as e:
                logger.error(f"SimpleVectorDB not available: {e}")
                document_service = None
        
        if document_service:
            SERVICES_AVAILABLE = True
        
    except Exception as e:
        logger.error(f"‚ùå Error initializing document service: {e}")
        document_service = None
        SERVICES_AVAILABLE = False
        CHROMADB_ENABLED = False
    
    # ====================================
    # –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø –°–ï–†–í–ò–°–ê –ü–ê–†–°–ò–ù–ì–ê
    # ====================================
    try:
        from services.scraper_service import LegalSiteScraper
        scraper = LegalSiteScraper()
        logger.info("‚úÖ Web scraper service initialized")
    except Exception as e:
        logger.error(f"‚ùå Error initializing scraper service: {e}")
        scraper = None
    
    # ====================================
    # –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø HUGGINGFACE LLM –°–ï–†–í–ò–°–ê
    # ====================================
    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
        if not settings.LLM_DEMO_MODE:
            from services.huggingface_llm_service import create_llm_service
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –º–æ–¥–µ–ª—å –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
            hf_model = getattr(settings, 'HUGGINGFACE_MODEL', "TheBloke/Llama-2-7B-Chat-GPTQ")
            
            logger.info(f"ü§ñ Initializing HuggingFace LLM service with model: {hf_model}")
            
            llm_service = create_llm_service(model_name=hf_model)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∞—Ç—É—Å —Å–µ—Ä–≤–∏—Å–∞
            status = await llm_service.get_service_status()
            
            if status.get("model_loaded", False):
                LLM_ENABLED = True
                logger.info("‚úÖ HuggingFace LLM service initialized successfully")
                logger.info(f"   Model: {status.get('model_name')}")
            else:
                logger.warning(f"‚ö†Ô∏è LLM service created but model not loaded: {status.get('error')}")
                LLM_ENABLED = False
        else:
            logger.info("‚ÑπÔ∏è LLM service disabled (demo mode)")
            LLM_ENABLED = False
            
    except ImportError as e:
        logger.error(f"‚ùå HuggingFace LLM service import failed: {e}")
        llm_service = None
        LLM_ENABLED = False
    except Exception as e:
        logger.error(f"‚ùå Error initializing HuggingFace LLM service: {e}")
        llm_service = None
        LLM_ENABLED = False
    
    # ====================================
    # –§–ò–ù–ê–õ–¨–ù–´–ô –°–¢–ê–¢–£–°
    # ====================================
    logger.info(f"üìä HuggingFace Spaces services status:")
    logger.info(f"   Document service: {'‚úÖ' if document_service else '‚ùå'}")
    logger.info(f"   ChromaDB enabled: {'‚úÖ' if CHROMADB_ENABLED else '‚ùå'}")
    logger.info(f"   Scraper service: {'‚úÖ' if scraper else '‚ùå'}")
    logger.info(f"   HuggingFace LLM: {'‚úÖ' if LLM_ENABLED else '‚ùå'}")
    logger.info(f"   Overall available: {'‚úÖ' if SERVICES_AVAILABLE else '‚ùå'}")

# ====================================
# DEPENDENCY FUNCTIONS
# ====================================

def get_document_service():
    """Dependency –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Å–µ—Ä–≤–∏—Å–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
    if not document_service:
        # –í–º–µ—Å—Ç–æ RuntimeError —Å–æ–∑–¥–∞–µ–º –∑–∞–≥–ª—É—à–∫—É
        logger.debug("Using fallback document service")
        return FallbackDocumentService()
    return document_service

def get_scraper_service():
    """Dependency –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Å–µ—Ä–≤–∏—Å–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞"""
    if not scraper:
        # –í–º–µ—Å—Ç–æ RuntimeError —Å–æ–∑–¥–∞–µ–º –∑–∞–≥–ª—É—à–∫—É
        logger.debug("Using fallback scraper service") 
        return FallbackScraperService()
    return scraper

def get_llm_service():
    """Dependency –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è HuggingFace LLM —Å–µ—Ä–≤–∏—Å–∞"""
    if not llm_service or not LLM_ENABLED:
        # –°–æ–∑–¥–∞–µ–º –∑–∞–≥–ª—É—à–∫—É –µ—Å–ª–∏ LLM –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω
        logger.debug("Using fallback LLM service")
        return FallbackLLMService()
    return llm_service

def get_services_status():
    """Dependency –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç—É—Å–∞ —Å–µ—Ä–≤–∏—Å–æ–≤"""
    return {
        "document_service_available": document_service is not None,
        "scraper_available": scraper is not None,
        "llm_available": LLM_ENABLED,
        "llm_service_created": llm_service is not None,
        "chromadb_enabled": CHROMADB_ENABLED,
        "services_available": SERVICES_AVAILABLE,
        "fallback_mode": document_service is None or scraper is None or not LLM_ENABLED,
        "huggingface_spaces": os.getenv("SPACE_ID") is not None,  # Detect HF Spaces
        "llm_demo_mode": settings.LLM_DEMO_MODE,
        "environment": "huggingface_spaces" if os.getenv("SPACE_ID") else "local"
    }

# ====================================
# –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–ï UTILITY FUNCTIONS
# ====================================

async def get_system_health():
    """–ü–æ–ª—É—á–∞–µ—Ç –¥–µ—Ç–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∑–¥–æ—Ä–æ–≤—å–µ —Å–∏—Å—Ç–µ–º—ã"""
    status = get_services_status()
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∞—Ç—É—Å LLM –µ—Å–ª–∏ –æ–Ω –¥–æ—Å—Ç—É–ø–µ–Ω
    llm_status = {}
    if llm_service and LLM_ENABLED:
        try:
            llm_status = await llm_service.get_service_status()
        except Exception as e:
            llm_status = {"error": str(e), "available": False}
    
    health_info = {
        "overall_status": "healthy" if status["services_available"] and status["llm_available"] else "degraded",
        "services": status,
        "llm_status": llm_status,
        "environment": "HuggingFace Spaces" if status["huggingface_spaces"] else "Local",
        "dependencies": {
            "fastapi": True,  # –ï—Å–ª–∏ –º—ã –¥–æ—à–ª–∏ –¥–æ —Å—é–¥–∞, FastAPI —Ä–∞–±–æ—Ç–∞–µ—Ç
            "pydantic": True, # –ê–Ω–∞–ª–æ–≥–∏—á–Ω–æ –¥–ª—è Pydantic
        },
        "features": {
            "document_processing": status["document_service_available"],
            "web_scraping": status["scraper_available"], 
            "vector_search": status["chromadb_enabled"],
            "ai_responses": status["llm_available"],
            "demo_mode": not status["services_available"] or status["llm_demo_mode"]
        }
    }
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
    optional_deps = {
        "sentence_transformers": False,
        "aiohttp": False,
        "beautifulsoup4": False,
        "chromadb": False,
        "transformers": False,
        "torch": False
    }
    
    for dep in optional_deps:
        try:
            __import__(dep)
            optional_deps[dep] = True
        except ImportError:
            pass
    
    health_info["dependencies"].update(optional_deps)
    
    return health_info

async def cleanup_services():
    """–ü—Ä–∞–≤–∏–ª—å–Ω–æ –∑–∞–∫—Ä—ã–≤–∞–µ—Ç –≤—Å–µ —Å–µ—Ä–≤–∏—Å—ã –ø—Ä–∏ –≤—ã–∫–ª—é—á–µ–Ω–∏–∏"""
    global llm_service, scraper
    
    logger.info("üßπ Cleaning up services...")
    
    try:
        if llm_service and hasattr(llm_service, 'close'):
            await llm_service.close()
            logger.info("‚úÖ LLM service closed")
    except Exception as e:
        logger.error(f"Error closing LLM service: {e}")
    
    try:
        if scraper and hasattr(scraper, 'close'):
            await scraper.close()
            logger.info("‚úÖ Scraper service closed")
    except Exception as e:
        logger.error(f"Error closing scraper service: {e}")
    
    logger.info("‚úÖ Services cleanup completed")

# ====================================
# –≠–ö–°–ü–û–†–¢
# ====================================

__all__ = [
    "init_services",
    "cleanup_services",
    "get_document_service", 
    "get_scraper_service",
    "get_llm_service",
    "get_services_status",
    "get_system_health",
    "FallbackDocumentService",
    "FallbackScraperService",
    "FallbackLLMService",
    "SERVICES_AVAILABLE",
    "CHROMADB_ENABLED",
    "LLM_ENABLED",
    "document_service",
    "scraper",
    "llm_service"
]