# app/dependencies.py - –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø –î–õ–Ø HUGGINGFACE SPACES

"""
–ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Ä–≤–∏—Å–æ–≤ –¥–ª—è HuggingFace Spaces
"""

import logging
import sys
import os
import time
import json
import asyncio
from typing import Optional, List, Dict, Any

from app.config import settings

logger = logging.getLogger(__name__)

# ====================================
# FALLBACK –°–ï–†–í–ò–°–´ –î–õ–Ø HF SPACES
# ====================================

class FallbackDocumentService:
    """–ü—Ä–æ—Å—Ç–æ–π document service –¥–ª—è HF Spaces –±–µ–∑ sentence-transformers"""
    
    def __init__(self):
        self.service_type = "hf_spaces_fallback"
        self.vector_db = type('MockVectorDB', (), {
            'persist_directory': './fallback_db'
        })()
        logger.info("üìù Using HF Spaces fallback document service")
    
    async def search(self, query: str, category: str = None, limit: int = 5, min_relevance: float = 0.3):
        """–ü—Ä–æ—Å—Ç–æ–π —Ç–µ–∫—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫ –±–µ–∑ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏"""
        logger.info(f"üîç Fallback search for: {query}")
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –¥–µ–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–º –∫–æ–Ω—Ç–µ–Ω—Ç–æ–º
        demo_results = [
            {
                "content": f"""Legal Information Related to: "{query}"

This is a demonstration response for HuggingFace Spaces deployment. 
In a full deployment, this would search through your legal document database.

Key Points:
‚Ä¢ Your query "{query}" would be processed using semantic search
‚Ä¢ Multiple legal documents would be analyzed for relevance
‚Ä¢ Results would be ranked by relevance score
‚Ä¢ Specific legal references and citations would be provided

To enable full functionality:
1. Complete the sentence-transformers installation
2. Upload legal documents to the database
3. Configure vector search parameters

For demonstration purposes, this shows the API structure and response format.""",
                "filename": f"demo_legal_doc_{query.replace(' ', '_')[:20]}.txt",
                "document_id": f"demo_{int(time.time())}",
                "relevance_score": 0.85,
                "metadata": {
                    "status": "demo",
                    "category": category or "general",
                    "service": "hf_spaces_fallback",
                    "query": query,
                    "demo_mode": True
                }
            }
        ]
        
        return demo_results
    
    async def get_stats(self):
        """–î–µ–º–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª—è HF Spaces"""
        return {
            "total_documents": 0,
            "categories": ["general", "demo", "legislation"],
            "database_type": "HF Spaces Fallback",
            "status": "Demo mode for HuggingFace Spaces deployment",
            "message": "Full functionality requires sentence-transformers setup",
            "available_features": [
                "‚úÖ FastAPI REST API structure",
                "‚úÖ Document upload endpoints", 
                "‚úÖ Search API (demo responses)",
                "‚úÖ Admin panel endpoints",
                "‚ö†Ô∏è Vector search (requires setup)",
                "‚ö†Ô∏è Real document processing (requires setup)"
            ],
            "setup_requirements": [
                "Fix sentence-transformers installation",
                "Configure vector database",
                "Upload legal documents"
            ]
        }
    
    async def get_all_documents(self):
        """–î–µ–º–æ —Å–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        return []
    
    async def delete_document(self, doc_id: str):
        """–î–µ–º–æ —É–¥–∞–ª–µ–Ω–∏–µ"""
        logger.info(f"Demo delete: {doc_id}")
        return False
    
    async def process_and_store_file(self, file_path: str, category: str = "general"):
        """–î–µ–º–æ –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–∞"""
        logger.info(f"Demo file processing: {file_path}")
        return False

class FallbackLLMService:
    """LLM service –¥–ª—è HF Spaces —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ –¥–µ–º–æ-–æ—Ç–≤–µ—Ç–∞–º–∏"""
    
    def __init__(self):
        self.service_type = "hf_spaces_demo"
        self.model_loaded = False
        logger.info("ü§ñ Using HF Spaces demo LLM service")
    
    async def answer_legal_question(self, question: str, context_documents: List[Dict], language: str = "en"):
        """–£–ª—É—á—à–µ–Ω–Ω—ã–µ –¥–µ–º–æ –æ—Ç–≤–µ—Ç—ã –¥–ª—è HF Spaces"""
        from services.huggingface_llm_service import LLMResponse
        
        if language == "uk":
            demo_content = f"""üèõÔ∏è **–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü—ñ–π–Ω–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥—å Legal Assistant**

**–í–∞—à–µ –ø–∏—Ç–∞–Ω–Ω—è:** {question}

**–ê–Ω–∞–ª—ñ–∑:** –ù–∞ –æ—Å–Ω–æ–≤—ñ {len(context_documents)} –∑–Ω–∞–π–¥–µ–Ω–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤ —É –±–∞–∑—ñ –∑–Ω–∞–Ω—å.

**–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü—ñ–π–Ω–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥—å:**
–¶–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü—ñ–π–Ω–∞ –≤–µ—Ä—Å—ñ—è Legal Assistant –¥–ª—è HuggingFace Spaces. –£ –ø–æ–≤–Ω—ñ–π –≤–µ—Ä—Å—ñ—ó —Å–∏—Å—Ç–µ–º–∞ –±–∏:

1. **–ü—Ä–æ–∞–Ω–∞–ª—ñ–∑—É–≤–∞–ª–∞** –≤–∞—à —é—Ä–∏–¥–∏—á–Ω–∏–π –∑–∞–ø–∏—Ç –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—á–∏ –®–Ü
2. **–ó–Ω–∞–π—à–ª–∞** —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ñ –¥–æ–∫—É–º–µ–Ω—Ç–∏ –≤ –±–∞–∑—ñ –∑–∞–∫–æ–Ω–æ–¥–∞–≤—Å—Ç–≤–∞
3. **–ù–∞–¥–∞–ª–∞** —Ç–æ—á–Ω—É –≤—ñ–¥–ø–æ–≤—ñ–¥—å –∑ –ø–æ—Å–∏–ª–∞–Ω–Ω—è–º–∏ –Ω–∞ –∑–∞–∫–æ–Ω–∏
4. **–í–∫–ª—é—á–∏–ª–∞** —Å–ø–µ—Ü–∏—Ñ—ñ—á–Ω—ñ —Å—Ç–∞—Ç—Ç—ñ —Ç–∞ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ñ –∞–∫—Ç–∏

**–ü–æ—Ç–æ—á–Ω–∏–π —Å—Ç–∞—Ç—É—Å:** –î–µ–º–æ —Ä–µ–∂–∏–º –Ω–∞ HuggingFace Spaces
**–î–ª—è –ø–æ–≤–Ω–æ–≥–æ —Ñ—É–Ω–∫—Ü—ñ–æ–Ω–∞–ª—É:** –ù–µ–æ–±—Ö—ñ–¥–Ω–æ –Ω–∞–ª–∞—à—Ç—É–≤–∞—Ç–∏ sentence-transformers —Ç–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ —é—Ä–∏–¥–∏—á–Ω—ñ –¥–æ–∫—É–º–µ–Ω—Ç–∏.

üìö –¶—è –≤–µ—Ä—Å—ñ—è –¥–µ–º–æ–Ω—Å—Ç—Ä—É—î –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä—É API —Ç–∞ —Ñ–æ—Ä–º–∞—Ç –≤—ñ–¥–ø–æ–≤—ñ–¥–µ–π."""
        else:
            demo_content = f"""üèõÔ∏è **Legal Assistant Demo Response**

**Your Question:** {question}

**Analysis:** Based on {len(context_documents)} documents found in the knowledge base.

**Demo Response:**
This is a demonstration version of Legal Assistant running on HuggingFace Spaces. In the full version, the system would:

1. **Analyze** your legal query using AI
2. **Search** through relevant legal documents
3. **Provide** precise answers with legal references
4. **Include** specific articles and regulations

**Current Status:** Demo mode on HuggingFace Spaces
**For Full Functionality:** Requires sentence-transformers setup and legal document uploads.

üìö This version demonstrates the API architecture and response format."""
        
        return LLMResponse(
            content=demo_content,
            model="hf_spaces_demo",
            tokens_used=len(demo_content.split()),
            response_time=0.5,
            success=True,
            error=None
        )
    
    async def get_service_status(self):
        """–°—Ç–∞—Ç—É—Å LLM –¥–ª—è HF Spaces"""
        return {
            "model_loaded": False,
            "model_name": "hf_spaces_demo",
            "huggingface_available": True,
            "service_type": "demo",
            "environment": "HuggingFace Spaces",
            "status": "Demo mode - showing API structure",
            "supported_languages": ["en", "uk"],
            "demo_features": [
                "‚úÖ REST API endpoints",
                "‚úÖ Multi-language support",
                "‚úÖ Legal document structure",
                "‚úÖ Admin panel integration",
                "‚ö†Ô∏è AI responses (demo only)",
                "‚ö†Ô∏è Vector search (requires setup)"
            ],
            "recommendations": [
                "This is a working demo of the Legal Assistant API",
                "Full AI functionality requires model loading",
                "Upload legal documents for real search capabilities",
                "Configure HuggingFace Transformers for AI responses"
            ]
        }

class FallbackScraperService:
    """Web scraper –¥–ª—è HF Spaces"""
    
    def __init__(self):
        self.service_type = "hf_spaces_scraper"
        self.legal_sites_config = {
            "irishstatutebook.ie": {"title": "h1", "content": ".content"},
            "citizensinformation.ie": {"title": "h1", "content": ".content"},
            "zakon.rada.gov.ua": {"title": "h1", "content": ".content"}
        }
        logger.info("üåê HF Spaces scraper service initialized")
    
    async def scrape_legal_site(self, url: str):
        """–î–µ–º–æ —Å–∫—Ä–∞–ø–∏–Ω–≥ –¥–ª—è HF Spaces"""
        logger.info(f"üîç Demo scraping: {url}")
        
        demo_content = f"""üìÑ **Legal Document from {url}**

This is a demonstration of web scraping functionality on HuggingFace Spaces.

**Document Source:** {url}
**Scraped Content:** In the full version, this would contain the actual legal text from the website.

**Sample Legal Content:**
"Article 1. This demonstration shows how the Legal Assistant would extract and process legal documents from official government websites, legal databases, and court systems.

The system would automatically:
- Extract relevant legal text
- Identify document structure
- Categorize by legal domain
- Process for semantic search
- Store in vector database"

**Metadata:**
- URL: {url}
- Language: Auto-detected
- Category: Determined by domain
- Processing: Demo mode

üîß **For Full Functionality:** Enable web scraping libraries (aiohttp, beautifulsoup4)
"""
        
        return type('DemoDocument', (), {
            'url': url,
            'title': f'Demo Legal Document from {url}',
            'content': demo_content,
            'metadata': {
                'status': 'demo',
                'real_scraping': False,
                'scraped_at': time.time(),
                'service': 'hf_spaces_demo',
                'url': url
            },
            'category': 'demo'
        })()
    
    async def scrape_multiple_urls(self, urls: List[str], delay: float = 1.0):
        """–î–µ–º–æ –º–∞—Å—Å–æ–≤—ã–π —Å–∫—Ä–∞–ø–∏–Ω–≥"""
        results = []
        for url in urls:
            doc = await self.scrape_legal_site(url)
            results.append(doc)
        return results

# ====================================
# –ì–õ–û–ë–ê–õ–¨–ù–´–ï –ü–ï–†–ï–ú–ï–ù–ù–´–ï
# ====================================

document_service: Optional[object] = None
scraper: Optional[object] = None
llm_service: Optional[object] = None
SERVICES_AVAILABLE: bool = False
CHROMADB_ENABLED: bool = False
LLM_ENABLED: bool = False

async def init_services():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Ä–≤–∏—Å–æ–≤ –¥–ª—è HuggingFace Spaces"""
    global document_service, scraper, llm_service, SERVICES_AVAILABLE, CHROMADB_ENABLED, LLM_ENABLED
    
    logger.info("üöÄ Initializing services for HuggingFace Spaces...")
    
    # –î–µ—Ç–µ–∫—Ç–∏—Ä—É–µ–º —Å—Ä–µ–¥—É HF Spaces
    is_hf_spaces = os.getenv("SPACE_ID") is not None
    if is_hf_spaces:
        logger.info("ü§ó Running on HuggingFace Spaces - using optimized setup")
    
    # ====================================
    # –ü–û–ü–´–¢–ö–ê –†–ï–ê–õ–¨–ù–û–ì–û DOCUMENT SERVICE
    # ====================================
    try:
        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º —Ä–µ–∞–ª—å–Ω—ã–π —Å–µ—Ä–≤–∏—Å
        if settings.USE_CHROMADB:
            try:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å sentence-transformers
                import sentence_transformers
                from services.chroma_service import DocumentService
                document_service = DocumentService(settings.CHROMADB_PATH)
                CHROMADB_ENABLED = True
                SERVICES_AVAILABLE = True
                logger.info("‚úÖ Real ChromaDB service initialized")
            except ImportError:
                logger.warning("‚ö†Ô∏è sentence-transformers not available, using fallback")
                raise ImportError("sentence-transformers not available")
        else:
            from services.document_processor import DocumentService
            document_service = DocumentService(settings.SIMPLE_DB_PATH)
            SERVICES_AVAILABLE = True
            logger.info("‚úÖ SimpleVectorDB service initialized")
    
    except Exception as e:
        logger.info(f"‚ÑπÔ∏è Using fallback document service: {e}")
        document_service = FallbackDocumentService()
        SERVICES_AVAILABLE = True  # Fallback —Å—á–∏—Ç–∞–µ—Ç—Å—è –¥–æ—Å—Ç—É–ø–Ω—ã–º
        CHROMADB_ENABLED = False
    
    # ====================================
    # SCRAPER SERVICE
    # ====================================
    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –±–∏–±–ª–∏–æ—Ç–µ–∫ –¥–ª—è —Å–∫—Ä–∞–ø–∏–Ω–≥–∞
        import aiohttp
        import bs4
        from services.scraper_service import LegalSiteScraper
        scraper = LegalSiteScraper()
        logger.info("‚úÖ Real scraper service initialized")
    except ImportError:
        logger.info("‚ÑπÔ∏è Using fallback scraper service (aiohttp/bs4 not available)")
        scraper = FallbackScraperService()
    
    # ====================================
    # LLM SERVICE
    # ====================================
    try:
        if not settings.LLM_DEMO_MODE:
            # –ü—Ä–æ–±—É–µ–º —Ä–µ–∞–ª—å–Ω—ã–π LLM —Å–µ—Ä–≤–∏—Å
            from services.huggingface_llm_service import create_llm_service
            llm_service = create_llm_service()
            
            status = await llm_service.get_service_status()
            if status.get("model_loaded", False):
                LLM_ENABLED = True
                logger.info("‚úÖ Real HuggingFace LLM service initialized")
            else:
                raise Exception("Model not loaded")
        else:
            raise Exception("Demo mode enabled")
    
    except Exception as e:
        logger.info(f"‚ÑπÔ∏è Using demo LLM service: {e}")
        llm_service = FallbackLLMService()
        LLM_ENABLED = False
    
    # ====================================
    # –§–ò–ù–ê–õ–¨–ù–´–ô –°–¢–ê–¢–£–°
    # ====================================
    logger.info(f"üìä HuggingFace Spaces services initialized:")
    logger.info(f"   Document service: {'‚úÖ Real' if isinstance(document_service, FallbackDocumentService) == False else 'üîÑ Demo'}")
    logger.info(f"   Scraper service: {'‚úÖ Real' if isinstance(scraper, FallbackScraperService) == False else 'üîÑ Demo'}")
    logger.info(f"   LLM service: {'‚úÖ Real' if LLM_ENABLED else 'üîÑ Demo'}")
    logger.info(f"   Environment: {'ü§ó HuggingFace Spaces' if is_hf_spaces else 'üíª Local'}")

# ====================================
# DEPENDENCY FUNCTIONS
# ====================================

def get_document_service():
    """Dependency –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è document service"""
    return document_service or FallbackDocumentService()

def get_scraper_service():
    """Dependency –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è scraper service"""
    return scraper or FallbackScraperService()

def get_llm_service():
    """Dependency –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è LLM service"""
    return llm_service or FallbackLLMService()

def get_services_status():
    """–°—Ç–∞—Ç—É—Å –≤—Å–µ—Ö —Å–µ—Ä–≤–∏—Å–æ–≤"""
    return {
        "document_service_available": document_service is not None,
        "scraper_available": scraper is not None,
        "llm_available": LLM_ENABLED,
        "llm_service_created": llm_service is not None,
        "chromadb_enabled": CHROMADB_ENABLED,
        "services_available": SERVICES_AVAILABLE,
        "huggingface_spaces": os.getenv("SPACE_ID") is not None,
        "environment": "hf_spaces" if os.getenv("SPACE_ID") else "local",
        "demo_mode": isinstance(document_service, FallbackDocumentService),
        "real_features": {
            "vector_search": CHROMADB_ENABLED,
            "web_scraping": not isinstance(scraper, FallbackScraperService),
            "ai_responses": LLM_ENABLED
        }
    }