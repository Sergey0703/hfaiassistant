# backend/app/dependencies.py - –û–ë–ù–û–í–õ–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø –î–õ–Ø LLAMACPP
"""
–ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Ä–≤–∏—Å–æ–≤ –¥–ª—è HuggingFace Spaces
–û–ë–ù–û–í–õ–ï–ù–û: –ü–µ—Ä–µ—Ö–æ–¥ —Å GPTQ –Ω–∞ LlamaCpp –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ–π —Ä–∞–±–æ—Ç—ã
"""

import logging
import sys
import os
import time
import json
import asyncio
import threading
from typing import Optional, List, Dict, Any
from concurrent.futures import ThreadPoolExecutor

from app.config import settings

logger = logging.getLogger(__name__)

# ====================================
# –ì–õ–û–ë–ê–õ–¨–ù–´–ï –ü–ï–†–ï–ú–ï–ù–ù–´–ï –°–ï–†–í–ò–°–û–í
# ====================================

document_service: Optional[object] = None
scraper: Optional[object] = None
llm_service: Optional[object] = None

# –°—Ç–∞—Ç—É—Å —Ñ–ª–∞–≥–∏
SERVICES_AVAILABLE: bool = True  # –í—Å–µ–≥–¥–∞ True –¥–ª—è HF Spaces
CHROMADB_ENABLED: bool = False
LLM_ENABLED: bool = False

# –§–ª–∞–≥–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏
_document_service_initialized = False
_scraper_initialized = False
_llm_service_initialized = False

# Background loading —Å—Ç–∞—Ç—É—Å
_background_loading_started = False
_background_tasks = {}

# Thread pool –¥–ª—è background –æ–ø–µ—Ä–∞—Ü–∏–π
_executor = ThreadPoolExecutor(max_workers=2)

# ====================================
# –£–õ–£–ß–®–ï–ù–ù–´–ï FALLBACK –°–ï–†–í–ò–°–´
# ====================================

class HFSpacesFallbackDocumentService:
    """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π fallback –¥–ª—è HF Spaces"""
    
    def __init__(self):
        self.service_type = "hf_spaces_fallback_v4_llamacpp"
        self.vector_db = type('MockVectorDB', (), {
            'persist_directory': './fallback_db'
        })()
        self.initialization_error = None
        logger.info("üìù HF Spaces document fallback service ready (LlamaCpp era)")
    
    async def search(self, query: str, category: str = None, limit: int = 5, min_relevance: float = 0.3) -> List[Dict]:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ —Å –¥–µ–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏"""
        try:
            logger.info(f"üîç Fallback search: '{query}'")
            await asyncio.sleep(0.1)
            
            demo_result = {
                "content": f"""Legal Analysis for: "{query}"

üèõÔ∏è **Document Summary (LlamaCpp Ready)**
This demonstrates the expected API response structure for legal document search.

üìã **Search Context:**
‚Ä¢ Query: "{query}"
‚Ä¢ Category: {category or "General Legal"}
‚Ä¢ Platform: HuggingFace Spaces
‚Ä¢ LLM Backend: LlamaCpp (stable CPU inference)
‚Ä¢ Status: Document service initializing...

‚öñÔ∏è **Expected Features (when fully loaded):**
‚Ä¢ ChromaDB vector search with semantic similarity
‚Ä¢ Multiple legal document categories  
‚Ä¢ Relevance scoring and ranking
‚Ä¢ Legal citation extraction

üîß **Current Status:**
Document service is loading in background. LlamaCpp LLM is ready for stable inference without hanging issues.

üí° **Note:** Full document search will be available once ChromaDB initialization completes. LlamaCpp provides reliable AI responses.""",
                
                "filename": f"legal_search_{query.replace(' ', '_')[:20]}.txt",
                "document_id": f"demo_{int(time.time())}",
                "relevance_score": 0.95,
                "metadata": {
                    "status": "demo_response_llamacpp_ready",
                    "category": category or "general",
                    "service": "hf_spaces_fallback_v4",
                    "query": query,
                    "platform": "HuggingFace Spaces",
                    "llm_backend": "llamacpp",
                    "background_loading": _background_loading_started,
                    "stable_inference": True
                }
            }
            
            return [demo_result]
            
        except Exception as e:
            logger.error(f"Fallback search error: {e}")
            return []
    
    async def get_stats(self) -> Dict:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞"""
        try:
            await asyncio.sleep(0.05)
            return {
                "total_documents": 0,
                "categories": ["general", "legislation", "jurisprudence", "government"],
                "database_type": "Initializing (ChromaDB loading...)",
                "status": "Background initialization in progress",
                "platform": "HuggingFace Spaces",
                "llm_backend": "llamacpp",
                "background_loading": _background_loading_started,
                "services_available": SERVICES_AVAILABLE,
                "stable_inference": True
            }
        except Exception as e:
            logger.error(f"Fallback stats error: {e}")
            return {
                "total_documents": 0,
                "categories": [],
                "database_type": "Error",
                "error": str(e)
            }
    
    async def get_all_documents(self) -> List[Dict]:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        try:
            await asyncio.sleep(0.05)
            return []
        except Exception as e:
            logger.error(f"Fallback get all documents error: {e}")
            return []
    
    async def delete_document(self, doc_id: str) -> bool:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–µ —É–¥–∞–ª–µ–Ω–∏–µ"""
        try:
            logger.info(f"Demo delete: {doc_id}")
            await asyncio.sleep(0.05)
            return False
        except Exception as e:
            logger.error(f"Fallback delete error: {e}")
            return False
    
    async def process_and_store_file(self, file_path: str, category: str = "general") -> bool:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–∞"""
        try:
            logger.info(f"Demo file processing: {file_path}")
            await asyncio.sleep(0.1)
            return False
        except Exception as e:
            logger.error(f"Fallback process file error: {e}")
            return False

class HFSpacesFallbackScraperService:
    """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π scraper fallback –¥–ª—è HF Spaces"""
    
    def __init__(self):
        self.service_type = "hf_spaces_scraper_fallback"
        self.legal_sites_config = {
            "irishstatutebook.ie": {"title": "h1", "content": ".content"},
            "citizensinformation.ie": {"title": "h1", "content": ".content"},
            "zakon.rada.gov.ua": {"title": "h1", "content": ".content"}
        }
        logger.info("üåê HF Spaces scraper fallback ready")
    
    async def scrape_legal_site(self, url: str):
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –¥–µ–º–æ —Å–∫—Ä–∞–ø–∏–Ω–≥"""
        try:
            logger.info(f"üîç Demo scraping: {url}")
            await asyncio.sleep(0.2)
            
            demo_content = f"""üìÑ **Legal Document from {url}**

This is a demonstration of the web scraping functionality for HuggingFace Spaces.

**Document Source:** {url}
**Status:** Scraper service initializing...
**Platform:** HuggingFace Spaces
**LLM Backend:** LlamaCpp (stable inference)

üîß **Background Loading:**
The real scraping service (aiohttp + beautifulsoup4) is loading in the background.

‚öñÔ∏è **Expected Functionality:**
‚Ä¢ Extract legal content from official sites
‚Ä¢ Parse Ukrainian and Irish legal documents  
‚Ä¢ Intelligent content extraction with CSS selectors
‚Ä¢ Metadata extraction and categorization

üåê **Supported Sites:**
‚Ä¢ zakon.rada.gov.ua (Ukrainian legislation)
‚Ä¢ irishstatutebook.ie (Irish statutory law)
‚Ä¢ citizensinformation.ie (Irish civil information)
‚Ä¢ courts.ie (Irish court decisions)

üí° **Real scraping will be available once background initialization completes. LlamaCpp provides stable AI analysis of scraped content.**"""
            
            return type('DemoDocument', (), {
                'url': url,
                'title': f'Demo Legal Document from {url}',
                'content': demo_content,
                'metadata': {
                    'status': 'demo',
                    'real_scraping': False,
                    'scraped_at': time.time(),
                    'service': 'hf_spaces_fallback',
                    'platform': 'HuggingFace Spaces',
                    'llm_backend': 'llamacpp',
                    'background_loading': _background_loading_started,
                    'url': url
                },
                'category': 'demo'
            })()
            
        except Exception as e:
            logger.error(f"Fallback scraper error: {e}")
            return None
    
    async def scrape_multiple_urls(self, urls: List[str], delay: float = 1.0):
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –º–∞—Å—Å–æ–≤—ã–π —Å–∫—Ä–∞–ø–∏–Ω–≥"""
        results = []
        for url in urls:
            doc = await self.scrape_legal_site(url)
            results.append(doc)
            if delay > 0:
                await asyncio.sleep(delay)
        return results

class HFSpacesLlamaCppFallback:
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π LLM fallback –¥–ª—è –ø–µ—Ä–µ—Ö–æ–¥–Ω–æ–≥–æ –ø–µ—Ä–∏–æ–¥–∞ –∫ LlamaCpp"""
    
    def __init__(self):
        self.service_type = "hf_spaces_llamacpp_fallback"
        self.model_loaded = False
        self.target_model = "TheBloke/Llama-2-7B-Chat-GGUF"
        logger.info(f"ü¶ô HF Spaces LlamaCpp fallback ready for: {self.target_model}")
    
    async def answer_legal_question(self, question: str, context_documents: List[Dict], language: str = "en"):
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–µ –¥–µ–º–æ –æ—Ç–≤–µ—Ç—ã –¥–ª—è LlamaCpp"""
        try:
            # –û–ë–ù–û–í–õ–ï–ù–û: –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –∏–∑ –Ω–æ–≤–æ–≥–æ —Å–µ—Ä–≤–∏—Å–∞
            from services.llamacpp_llm_service import LLMResponse
            
            await asyncio.sleep(0.3)
            
            if language == "uk":
                demo_content = f"""ü¶ô **LlamaCpp –º–æ–¥–µ–ª—å –∑–∞–≤–∞–Ω—Ç–∞–∂—É—î—Ç—å—Å—è...**

**–í–∞—à–µ –ø–∏—Ç–∞–Ω–Ω—è:** {question}

**–ê–Ω–∞–ª—ñ–∑ –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤:** –ó–Ω–∞–π–¥–µ–Ω–æ {len(context_documents)} —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤ —É –±–∞–∑—ñ –∑–Ω–∞–Ω—å.

ü§ñ **–°—Ç–∞—Ç—É—Å LlamaCpp –º–æ–¥–µ–ª—ñ:**
‚Ä¢ –ú–æ–¥–µ–ª—å: `{self.target_model}`
‚Ä¢ –§–æ—Ä–º–∞—Ç: GGUF (–æ–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–∏–π –¥–ª—è CPU)
‚Ä¢ –ü–ª–∞—Ç—Ñ–æ—Ä–º–∞: HuggingFace Spaces
‚Ä¢ –°—Ç–∞—Ç—É—Å: –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Å—Ç–∞–±—ñ–ª—å–Ω–æ—ó CPU –º–æ–¥–µ–ª—ñ...

üìã **–û—á—ñ–∫—É–≤–∞–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ–æ–Ω–∞–ª—å–Ω—ñ—Å—Ç—å:**
‚úÖ –°—Ç–∞–±—ñ–ª—å–Ω–∞ —Ä–æ–±–æ—Ç–∞ –Ω–∞ CPU –±–µ–∑ –∑–∞–≤–∏—Å–∞–Ω—å
‚úÖ –ü—ñ–¥—Ç—Ä–∏–º–∫–∞ —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—ó —Ç–∞ –∞–Ω–≥–ª—ñ–π—Å—å–∫–æ—ó –º–æ–≤
‚úÖ –ö–æ–Ω—Ç–µ–∫—Å—Ç—É–∞–ª—å–Ω—ñ –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ –Ω–∞ –æ—Å–Ω–æ–≤—ñ –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤
‚úÖ –¢–∞–π–º–∞—É—Ç–∏ –¥–ª—è –∑–∞–ø–æ–±—ñ–≥–∞–Ω–Ω—è –∑–∞–≤–∏—Å–∞–Ω–Ω—è–º
‚úÖ –ü—Ä–∞–∫—Ç–∏—á–Ω—ñ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó —Ç–∞ –ø–æ–∫—Ä–æ–∫–æ–≤—ñ –¥—ñ—ó

‚è≥ **–ü—Ä–æ—Ü–µ—Å –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è:**
1. –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è llama-cpp-python
2. –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è GGUF –∫–≤–∞–Ω—Ç–∏–∑–æ–≤–∞–Ω–æ—ó –º–æ–¥–µ–ª—ñ (~4GB)
3. –û–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è –¥–ª—è CPU inference
4. –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∞–≤–æ–≤–æ—ó —Å–∏—Å—Ç–µ–º–∏ –ø—Ä–æ–º–ø—Ç—ñ–≤

üí° **–ü–æ—Ä–∞–¥–∞:** LlamaCpp –∑–∞–±–µ–∑–ø–µ—á–∏—Ç—å —Å—Ç–∞–±—ñ–ª—å–Ω—É —Ä–æ–±–æ—Ç—É –±–µ–∑ –ø—Ä–æ–±–ª–µ–º –∑ –ø–∞–º'—è—Ç—Ç—é. –°–ø—Ä–æ–±—É–π—Ç–µ —â–µ —Ä–∞–∑ —á–µ—Ä–µ–∑ 1 —Ö–≤–∏–ª–∏–Ω—É –¥–ª—è –æ—Ç—Ä–∏–º–∞–Ω–Ω—è –ø–æ–≤–Ω–æ—ó AI –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ.

üîß **–¢–µ—Ö–Ω—ñ—á–Ω—ñ –¥–µ—Ç–∞–ª—ñ:**
‚Ä¢ –ê—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞: Llama-2-7B –∑ GGUF –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü—ñ—î—é
‚Ä¢ –ë–µ–∫–µ–Ω–¥: llama.cpp (–ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã–π –Ω–∞ CPU)
‚Ä¢ –ú–æ–≤–∏: –ê–Ω–≥–ª—ñ–π—Å—å–∫–∞ —Ç–∞ —É–∫—Ä–∞—ó–Ω—Å—å–∫–∞
‚Ä¢ –¢–∞–π–º–∞—É—Ç—ã: 30 —Å–µ–∫—É–Ω–¥ (–±–µ–∑ –∑–∞–≤–∏—Å–∞–Ω—å)
‚Ä¢ –°–ø–µ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è: –ü—Ä–∞–≤–æ–≤—ñ –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü—ñ—ó —Ç–∞ –∞–Ω–∞–ª—ñ–∑"""
            else:
                demo_content = f"""ü¶ô **LlamaCpp Model Loading...**

**Your Question:** {question}

**Document Analysis:** Found {len(context_documents)} relevant documents in knowledge base.

ü§ñ **LlamaCpp Model Status:**
‚Ä¢ Model: `{self.target_model}`
‚Ä¢ Format: GGUF (CPU optimized)
‚Ä¢ Platform: HuggingFace Spaces
‚Ä¢ Status: Loading stable CPU model...

üìã **Expected Functionality:**
‚úÖ Stable CPU inference without hanging
‚úÖ English and Ukrainian language support
‚úÖ Context-aware responses based on documents
‚úÖ Timeouts to prevent hanging
‚úÖ Practical recommendations and step-by-step guidance

‚è≥ **Loading Process:**
1. Initializing llama-cpp-python
2. Loading GGUF quantized model (~4GB)
3. Optimizing for CPU inference
4. Preparing legal prompt system

üí° **Tip:** LlamaCpp will provide stable operation without memory issues. Try again in 1 minute for full AI response.

üîß **Technical Details:**
‚Ä¢ Architecture: Llama-2-7B with GGUF quantization
‚Ä¢ Backend: llama.cpp (CPU proven)
‚Ä¢ Languages: English and Ukrainian
‚Ä¢ Timeouts: 30 seconds (no hanging)
‚Ä¢ Specialization: Legal consultation and analysis"""
            
            return LLMResponse(
                content=demo_content,
                model=self.target_model,
                tokens_used=len(demo_content.split()),
                response_time=0.3,
                success=True,
                error=None
            )
            
        except Exception as e:
            logger.error(f"LLM fallback error: {e}")
            return type('SimpleLLMResponse', (), {
                'content': f"LlamaCpp model is loading. Question: {question}",
                'model': self.target_model,
                'tokens_used': 10,
                'response_time': 0.1,
                'success': True,
                'error': None
            })()
    
    async def get_service_status(self) -> Dict:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π —Å—Ç–∞—Ç—É—Å"""
        try:
            await asyncio.sleep(0.05)
            return {
                "model_loaded": False,
                "model_name": self.target_model,
                "llama_cpp_available": True,
                "service_type": "llamacpp_fallback",
                "environment": "HuggingFace Spaces",
                "status": "LlamaCpp model loading in background",
                "supported_languages": ["en", "uk"],
                "background_loading": _background_loading_started,
                "optimization": "GGUF quantization for CPU",
                "stable_inference": True,
                "timeout_protection": True,
                "target_model": self.target_model,
                "recommendations": [
                    "LlamaCpp provides stable CPU inference",
                    "GGUF format optimized for memory efficiency",
                    "No hanging issues expected",
                    "Timeout protection prevents freezing"
                ]
            }
        except Exception as e:
            logger.error(f"LLM status error: {e}")
            return {
                "model_loaded": False,
                "error": str(e),
                "service_type": "llamacpp_fallback_error"
            }

# ====================================
# –°–ò–ù–•–†–û–ù–ù–ê–Ø –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø –§–£–ù–ö–¶–ò–ô
# ====================================

def _init_document_service_sync():
    """–°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è document service"""
    global document_service, _document_service_initialized, CHROMADB_ENABLED
    
    if _document_service_initialized:
        return document_service
    
    logger.info("üîÑ Sync initializing document service...")
    
    try:
        # –ü—Ä–æ—Å—Ç–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ ChromaDB –±–µ–∑ async
        try:
            import sentence_transformers
            import chromadb
            logger.info("üìö ChromaDB dependencies available, will init in background")
            
        except ImportError as e:
            logger.info(f"ChromaDB dependencies missing: {e}")
        
        # –í—Å–µ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Å—Ç–∞—Ä—Ç–∞
        document_service = HFSpacesFallbackDocumentService()
        CHROMADB_ENABLED = False
        _document_service_initialized = True
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º background –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—é ChromaDB
        _start_background_chromadb_init()
        
        logger.info("‚úÖ Document service ready (fallback + background loading)")
        return document_service
        
    except Exception as e:
        logger.error(f"‚ùå Document service sync init failed: {e}")
        document_service = HFSpacesFallbackDocumentService()
        document_service.initialization_error = str(e)
        _document_service_initialized = True
        return document_service

def _init_scraper_service_sync():
    """–°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è scraper service"""
    global scraper, _scraper_initialized
    
    if _scraper_initialized:
        return scraper
    
    logger.info("üîÑ Sync initializing scraper service...")
    
    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –±–µ–∑ –∏–º–ø–æ—Ä—Ç–∞ (–±—ã—Å—Ç—Ä–æ)
        try:
            import aiohttp
            import bs4
            libraries_available = True
        except ImportError:
            libraries_available = False
        
        if libraries_available:
            logger.info("üåê Scraper libraries available, will init real scraper in background")
            _start_background_scraper_init()
        
        # –í—Å–µ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Å—Ç–∞—Ä—Ç–∞
        scraper = HFSpacesFallbackScraperService()
        _scraper_initialized = True
        
        logger.info("‚úÖ Scraper service ready (fallback + background loading)")
        return scraper
        
    except Exception as e:
        logger.error(f"‚ùå Scraper sync init failed: {e}")
        scraper = HFSpacesFallbackScraperService()
        _scraper_initialized = True
        return scraper

def _init_llm_service_sync():
    """–û–ë–ù–û–í–õ–ï–ù–ù–ê–Ø —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è LLM service –¥–ª—è LlamaCpp"""
    global llm_service, _llm_service_initialized, LLM_ENABLED
    
    if _llm_service_initialized:
        return llm_service
    
    logger.info("üîÑ Sync initializing LlamaCpp LLM service...")
    
    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–µ–º–æ —Ä–µ–∂–∏–º
        if settings.LLM_DEMO_MODE:
            logger.info("üé≠ LLM demo mode enabled")
            llm_service = HFSpacesLlamaCppFallback()
            LLM_ENABLED = False
            _llm_service_initialized = True
            return llm_service
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π LlamaCpp
        try:
            import llama_cpp
            dependencies_available = True
            logger.info("ü¶ô LlamaCpp dependencies available")
        except ImportError as e:
            logger.warning(f"LlamaCpp dependencies missing: {e}")
            dependencies_available = False
        
        # –í—Å–µ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Å—Ç–∞—Ä—Ç–∞
        llm_service = HFSpacesLlamaCppFallback()
        LLM_ENABLED = False
        _llm_service_initialized = True
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º background –∑–∞–≥—Ä—É–∑–∫—É LlamaCpp –º–æ–¥–µ–ª–∏ –µ—Å–ª–∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –µ—Å—Ç—å
        if dependencies_available:
            _start_background_llamacpp_init()
        
        logger.info("‚úÖ LLM service ready (fallback + background LlamaCpp loading)")
        return llm_service
        
    except Exception as e:
        logger.error(f"‚ùå LLM sync init failed: {e}")
        llm_service = HFSpacesLlamaCppFallback()
        LLM_ENABLED = False
        _llm_service_initialized = True
        return llm_service

# ====================================
# BACKGROUND –ê–°–ò–ù–•–†–û–ù–ù–ê–Ø –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø
# ====================================

def _start_background_chromadb_init():
    """–ó–∞–ø—É—Å–∫–∞–µ—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—é ChromaDB –≤ —Ñ–æ–Ω–µ"""
    global _background_tasks
    
    if "chromadb" in _background_tasks:
        return
    
    logger.info("üöÄ Starting background ChromaDB initialization...")
    
    def background_chromadb_worker():
        try:
            time.sleep(2)
            logger.info("üìö Background: Initializing ChromaDB...")
            
            from services.chroma_service import DocumentService
            
            os.makedirs(settings.CHROMADB_PATH, exist_ok=True)
            real_service = DocumentService(settings.CHROMADB_PATH)
            
            global document_service, CHROMADB_ENABLED
            document_service = real_service
            CHROMADB_ENABLED = True
            
            logger.info("‚úÖ Background: ChromaDB initialized successfully")
            
        except Exception as e:
            logger.error(f"‚ùå Background ChromaDB init failed: {e}")
    
    future = _executor.submit(background_chromadb_worker)
    _background_tasks["chromadb"] = future

def _start_background_scraper_init():
    """–ó–∞–ø—É—Å–∫–∞–µ—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—é —Ä–µ–∞–ª—å–Ω–æ–≥–æ scraper –≤ —Ñ–æ–Ω–µ"""
    global _background_tasks
    
    if "scraper" in _background_tasks:
        return
    
    logger.info("üöÄ Starting background scraper initialization...")
    
    def background_scraper_worker():
        try:
            time.sleep(3)
            logger.info("üåê Background: Initializing real scraper...")
            
            from services.scraper_service import LegalSiteScraper
            
            real_scraper = LegalSiteScraper()
            
            global scraper
            scraper = real_scraper
            
            logger.info("‚úÖ Background: Real scraper initialized successfully")
            
        except Exception as e:
            logger.error(f"‚ùå Background scraper init failed: {e}")
    
    future = _executor.submit(background_scraper_worker)
    _background_tasks["scraper"] = future

def _start_background_llamacpp_init():
    """–ù–û–í–ê–Ø —Ñ—É–Ω–∫—Ü–∏—è: –ó–∞–ø—É—Å–∫–∞–µ—Ç –∑–∞–≥—Ä—É–∑–∫—É LlamaCpp –º–æ–¥–µ–ª–∏ –≤ —Ñ–æ–Ω–µ"""
    global _background_tasks
    
    if "llamacpp" in _background_tasks:
        return
    
    logger.info("üöÄ Starting background LlamaCpp model loading...")
    
    def background_llamacpp_worker():
        try:
            time.sleep(5)  # –î–∞–µ–º –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—é –ø–æ–ª–Ω–æ—Å—Ç—å—é –∑–∞–ø—É—Å—Ç–∏—Ç—å—Å—è
            logger.info("ü¶ô Background: Loading LlamaCpp model...")
            
            # –û–ë–ù–û–í–õ–ï–ù–û: –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –Ω–æ–≤—ã–π —Å–µ—Ä–≤–∏—Å
            from services.llamacpp_llm_service import create_llm_service
            
            # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å LlamaCpp –º–æ–¥–µ–ª—å
            real_llm = create_llm_service("TheBloke/Llama-2-7B-Chat-GGUF")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –º–æ–¥–µ–ª—å –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –∑–∞–≥—Ä—É–∑–∏–ª–∞—Å—å
            if hasattr(real_llm, 'model_loaded') and real_llm.model_loaded:
                # –ó–∞–º–µ–Ω—è–µ–º –≥–ª–æ–±–∞–ª—å–Ω—ã–π —Å–µ—Ä–≤–∏—Å
                global llm_service, LLM_ENABLED
                llm_service = real_llm
                LLM_ENABLED = True
                
                logger.info("‚úÖ Background: LlamaCpp model loaded successfully!")
            else:
                logger.warning("‚ö†Ô∏è Background: LlamaCpp model not ready, keeping fallback")
                
        except Exception as e:
            logger.error(f"‚ùå Background LlamaCpp loading failed: {e}")
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º —Ç—Ä–µ–¥–µ
    future = _executor.submit(background_llamacpp_worker)
    _background_tasks["llamacpp"] = future

def _start_all_background_tasks():
    """–ó–∞–ø—É—Å–∫–∞–µ—Ç –≤—Å–µ background –∑–∞–¥–∞—á–∏"""
    global _background_loading_started
    
    if _background_loading_started:
        return
    
    _background_loading_started = True
    logger.info("üöÄ Starting all background initialization tasks...")
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –≤—Å–µ background –∑–∞–¥–∞—á–∏
    _start_background_chromadb_init()
    _start_background_scraper_init() 
    _start_background_llamacpp_init()  # –û–ë–ù–û–í–õ–ï–ù–û: LlamaCpp –≤–º–µ—Å—Ç–æ GPTQ

# ====================================
# DEPENDENCY FUNCTIONS (–°–ò–ù–•–†–û–ù–ù–´–ï)
# ====================================

def get_document_service():
    """Dependency –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è document service - –°–ò–ù–•–†–û–ù–ù–ê–Ø"""
    service = _init_document_service_sync()
    
    if not _background_loading_started:
        _start_all_background_tasks()
    
    return service

def get_scraper_service():
    """Dependency –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è scraper service - –°–ò–ù–•–†–û–ù–ù–ê–Ø"""
    service = _init_scraper_service_sync()
    
    if not _background_loading_started:
        _start_all_background_tasks()
    
    return service

def get_llm_service():
    """Dependency –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è LLM service - –°–ò–ù–•–†–û–ù–ù–ê–Ø"""
    service = _init_llm_service_sync()
    
    if not _background_loading_started:
        _start_all_background_tasks()
    
    return service

def get_services_status():
    """–°—Ç–∞—Ç—É—Å –≤—Å–µ—Ö —Å–µ—Ä–≤–∏—Å–æ–≤ - –°–ò–ù–•–†–û–ù–ù–ê–Ø"""
    return {
        "document_service_available": _document_service_initialized,
        "scraper_available": _scraper_initialized,
        "llm_available": LLM_ENABLED,
        "llm_service_created": _llm_service_initialized,
        "chromadb_enabled": CHROMADB_ENABLED,
        "services_available": SERVICES_AVAILABLE,
        "huggingface_spaces": os.getenv("SPACE_ID") is not None,
        "environment": "hf_spaces" if os.getenv("SPACE_ID") else "local",
        "demo_mode": not LLM_ENABLED,
        "lazy_loading": True,
        "llm_backend": "llamacpp",  # –û–ë–ù–û–í–õ–ï–ù–û
        "target_model": "TheBloke/Llama-2-7B-Chat-GGUF",  # –û–ë–ù–û–í–õ–ï–ù–û
        "background_loading": _background_loading_started,
        "background_tasks": {
            "chromadb_started": "chromadb" in _background_tasks,
            "scraper_started": "scraper" in _background_tasks,
            "llamacpp_started": "llamacpp" in _background_tasks  # –û–ë–ù–û–í–õ–ï–ù–û
        },
        "initialization_status": {
            "document_service": _document_service_initialized,
            "scraper_service": _scraper_initialized,
            "llm_service": _llm_service_initialized
        },
        "real_features": {
            "vector_search": CHROMADB_ENABLED,
            "web_scraping": _scraper_initialized and not isinstance(scraper, HFSpacesFallbackScraperService),
            "ai_responses": LLM_ENABLED,
            "stable_inference": True,  # –ù–û–í–û–ï
            "timeout_protection": True  # –ù–û–í–û–ï
        },
        "llamacpp_migration": True  # –ù–û–í–û–ï: –ò–Ω–¥–∏–∫–∞—Ç–æ—Ä –º–∏–≥—Ä–∞—Ü–∏–∏
    }

# ====================================
# –°–û–í–ú–ï–°–¢–ò–ú–û–°–¢–¨
# ====================================

async def init_services():
    """–£–ø—Ä–æ—â–µ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏"""
    logger.info("üöÄ HF Spaces: Using sync initialization with LlamaCpp backend")
    logger.info("üì¶ Services will initialize on first request + background tasks")
    
    global SERVICES_AVAILABLE
    SERVICES_AVAILABLE = True
    
    logger.info("‚úÖ Sync initialization ready with LlamaCpp migration")

# ====================================
# –§–£–ù–ö–¶–ò–ò –î–õ–Ø –ú–û–ù–ò–¢–û–†–ò–ù–ì–ê
# ====================================

def get_background_tasks_status():
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç—É—Å background –∑–∞–¥–∞—á"""
    status = {}
    
    for task_name, future in _background_tasks.items():
        if future.done():
            if future.exception():
                status[task_name] = {
                    "status": "failed",
                    "error": str(future.exception())
                }
            else:
                status[task_name] = {
                    "status": "completed",
                    "result": "success"
                }
        else:
            status[task_name] = {
                "status": "running",
                "progress": "in_progress"
            }
    
    return {
        "background_loading_started": _background_loading_started,
        "total_tasks": len(_background_tasks),
        "tasks": status,
        "llamacpp_migration": True,
        "stable_inference": True
    }

def force_background_init():
    """–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –∑–∞–ø—É—Å–∫–∞–µ—Ç background –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—é"""
    _start_all_background_tasks()
    return {
        "message": "Background initialization started with LlamaCpp",
        "tasks_started": len(_background_tasks),
        "llamacpp_migration": True
    }

# ====================================
# –≠–ö–°–ü–û–†–¢
# ====================================

__all__ = [
    # –û—Å–Ω–æ–≤–Ω—ã–µ dependency —Ñ—É–Ω–∫—Ü–∏–∏
    "get_document_service",
    "get_scraper_service", 
    "get_llm_service",
    "get_services_status",
    
    # –°—Ç–∞—Ç—É—Å –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥
    "get_background_tasks_status",
    "force_background_init",
    
    # –°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å
    "init_services",
    
    # –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
    "SERVICES_AVAILABLE",
    "CHROMADB_ENABLED",
    "LLM_ENABLED"
]