# app/dependencies.py - –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø –î–õ–Ø HUGGINGFACE SPACES

"""
–ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Ä–≤–∏—Å–æ–≤ –¥–ª—è HuggingFace Spaces
–ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: Lazy initialization + –ª—É—á—à–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ GPTQ –º–æ–¥–µ–ª–∏
"""

import logging
import sys
import os
import time
import json
import asyncio
from typing import Optional, List, Dict, Any

from app.config import settings

logger = logging.getLogger(__name__)

# ====================================
# –ì–õ–û–ë–ê–õ–¨–ù–´–ï –ü–ï–†–ï–ú–ï–ù–ù–´–ï –°–ï–†–í–ò–°–û–í
# ====================================

document_service: Optional[object] = None
scraper: Optional[object] = None
llm_service: Optional[object] = None
SERVICES_AVAILABLE: bool = False
CHROMADB_ENABLED: bool = False
LLM_ENABLED: bool = False

# –§–ª–∞–≥–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–ª—è lazy loading
_document_service_initialized = False
_scraper_initialized = False
_llm_service_initialized = False

# ====================================
# –£–õ–£–ß–®–ï–ù–ù–´–ô LLM FALLBACK
# ====================================

class ImprovedFallbackLLMService:
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π fallback LLM —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —É–∫—Ä–∞–∏–Ω—Å–∫–æ–≥–æ —è–∑—ã–∫–∞"""
    
    def __init__(self):
        self.service_type = "hf_spaces_demo_improved"
        self.model_loaded = False
        logger.info("ü§ñ Using improved demo LLM service with Ukrainian support")
    
    async def answer_legal_question(self, question: str, context_documents: List[Dict], language: str = "en"):
        """–£–ª—É—á—à–µ–Ω–Ω—ã–µ –¥–µ–º–æ –æ—Ç–≤–µ—Ç—ã –¥–ª—è —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö –≤–æ–ø—Ä–æ—Å–æ–≤"""
        from services.huggingface_llm_service import LLMResponse
        
        if language == "uk":
            demo_content = f"""üèõÔ∏è **–Æ—Ä–∏–¥–∏—á–Ω–∞ –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü—ñ—è (–î–µ–º–æ —Ä–µ–∂–∏–º)**

**–í–∞—à–µ –ø–∏—Ç–∞–Ω–Ω—è:** {question}

**–ê–Ω–∞–ª—ñ–∑ –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤:** –ó–Ω–∞–π–¥–µ–Ω–æ {len(context_documents)} —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤ —É –±–∞–∑—ñ –∑–Ω–∞–Ω—å.

**–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü—ñ–π–Ω–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥—å:**
–¶–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü—ñ–π–Ω–∞ –≤–µ—Ä—Å—ñ—è Legal Assistant, —â–æ –ø—Ä–∞—Ü—é—î –∑ –º–æ–¥–µ–ª–ª—é GPTQ. –£ –ø–æ–≤–Ω—ñ–π –≤–µ—Ä—Å—ñ—ó –≤–∞—à–∞ –º–æ–¥–µ–ª—å `TheBloke/Llama-2-7B-Chat-GPTQ` –Ω–∞–¥–∞—Å—Ç—å:

üìã **–î–µ—Ç–∞–ª—å–Ω–∏–π –∞–Ω–∞–ª—ñ–∑:** –ì–ª–∏–±–æ–∫–∏–π —Ä–æ–∑–±—ñ—Ä —é—Ä–∏–¥–∏—á–Ω–æ–≥–æ –ø–∏—Ç–∞–Ω–Ω—è –Ω–∞ –æ—Å–Ω–æ–≤—ñ –∑–Ω–∞–π–¥–µ–Ω–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤
‚öñÔ∏è **–ü—Ä–∞–≤–æ–≤—ñ –ø–æ—Å–∏–ª–∞–Ω–Ω—è:** –ö–æ–Ω–∫—Ä–µ—Ç–Ω—ñ —Å—Ç–∞—Ç—Ç—ñ –∑–∞–∫–æ–Ω—ñ–≤ —Ç–∞ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω–∏—Ö –∞–∫—Ç—ñ–≤
üéØ **–ü—Ä–∞–∫—Ç–∏—á–Ω—ñ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó:** –ü–æ–∫—Ä–æ–∫–æ–≤—ñ –¥—ñ—ó –¥–ª—è –≤–∏—Ä—ñ—à–µ–Ω–Ω—è –ø–∏—Ç–∞–Ω–Ω—è
üîç **–ö–æ–Ω—Ç–µ–∫—Å—Ç:** –ê–Ω–∞–ª—ñ–∑ –∑ —É—Ä–∞—Ö—É–≤–∞–Ω–Ω—è–º —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ–≥–æ —Ç–∞ —ñ—Ä–ª–∞–Ω–¥—Å—å–∫–æ–≥–æ –ø—Ä–∞–≤–∞

**–°—Ç–∞—Ç—É—Å:** –ú–æ–¥–µ–ª—å GPTQ –∑–∞–≤–∞–Ω—Ç–∞–∂—É—î—Ç—å—Å—è... –ó–∞–∑–≤–∏—á–∞–π —Ü–µ –∑–∞–π–º–∞—î 1-2 —Ö–≤–∏–ª–∏–Ω–∏.
**–Ø–∫—ñ—Å—Ç—å:** –í–∞—à–∞ –º–æ–¥–µ–ª—å Llama-2-7B –∑–∞–±–µ–∑–ø–µ—á–∏—Ç—å –≤–∏—Å–æ–∫–æ—è–∫—ñ—Å–Ω—ñ —é—Ä–∏–¥–∏—á–Ω—ñ –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü—ñ—ó.

üí° **–ü—ñ–¥–∫–∞–∑–∫–∞:** –°–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ—Ñ—Ä–∞–∑—É–≤–∞—Ç–∏ –ø–∏—Ç–∞–Ω–Ω—è –∞–±–æ –∑–∞—á–µ–∫–∞–π—Ç–µ –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—è —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—ó –º–æ–¥–µ–ª—ñ."""
        else:
            demo_content = f"""üèõÔ∏è **Legal Consultation (Demo Mode)**

**Your Question:** {question}

**Document Analysis:** Found {len(context_documents)} relevant documents in knowledge base.

**Demo Response:**
This is a demonstration version of Legal Assistant running with GPTQ model. In full version, your `TheBloke/Llama-2-7B-Chat-GPTQ` model will provide:

üìã **Detailed Analysis:** Deep breakdown of legal question based on found documents
‚öñÔ∏è **Legal References:** Specific articles, laws, and regulations
üéØ **Practical Advice:** Step-by-step actions to resolve the issue
üîç **Context:** Analysis considering Irish and Ukrainian law

**Status:** GPTQ model is loading... This usually takes 1-2 minutes.
**Quality:** Your Llama-2-7B model will provide high-quality legal consultations.

üí° **Tip:** Try rephrasing your question or wait for model initialization to complete."""
        
        return LLMResponse(
            content=demo_content,
            model="llama-2-7b-chat-gptq-demo",
            tokens_used=len(demo_content.split()),
            response_time=0.5,
            success=True,
            error=None
        )
    
    async def get_service_status(self):
        """–°—Ç–∞—Ç—É—Å –¥–µ–º–æ LLM —Å–µ—Ä–≤–∏—Å–∞"""
        return {
            "model_loaded": False,
            "model_name": "TheBloke/Llama-2-7B-Chat-GPTQ (loading...)",
            "huggingface_available": True,
            "service_type": "demo_fallback",
            "environment": "HuggingFace Spaces",
            "status": "Your GPTQ model is initializing",
            "supported_languages": ["en", "uk"],
            "target_model": "TheBloke/Llama-2-7B-Chat-GPTQ",
            "loading_status": "Model initialization in progress...",
            "recommendations": [
                "Your GPTQ model provides excellent legal analysis",
                "Supports both English and Ukrainian languages",
                "Loading may take 1-2 minutes on first startup",
                "Demo responses show expected functionality"
            ]
        }

# ====================================
# LAZY INITIALIZATION FUNCTIONS
# ====================================

def _init_document_service():
    """–õ–µ–Ω–∏–≤–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è document service —Å retry –ª–æ–≥–∏–∫–æ–π"""
    global document_service, _document_service_initialized, CHROMADB_ENABLED, SERVICES_AVAILABLE
    
    if _document_service_initialized:
        return document_service
    
    try:
        logger.info("üîÑ Initializing document service...")
        
        if settings.USE_CHROMADB:
            try:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º sentence-transformers
                import sentence_transformers
                from services.chroma_service import DocumentService
                
                logger.info("üìö Attempting ChromaDB initialization...")
                
                # Retry –ª–æ–≥–∏–∫–∞ –¥–ª—è ChromaDB (3 –ø–æ–ø—ã—Ç–∫–∏)
                last_error = None
                for attempt in range(3):
                    try:
                        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è ChromaDB
                        import os
                        os.makedirs(settings.CHROMADB_PATH, exist_ok=True)
                        
                        document_service = DocumentService(settings.CHROMADB_PATH)
                        
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ ChromaDB –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç
                        test_count = await document_service.get_stats()
                        
                        CHROMADB_ENABLED = True
                        SERVICES_AVAILABLE = True
                        logger.info(f"‚úÖ ChromaDB initialized successfully (attempt {attempt + 1})")
                        break
                        
                    except Exception as e:
                        last_error = e
                        logger.warning(f"ChromaDB attempt {attempt + 1}/3 failed: {e}")
                        if attempt < 2:  # –ù–µ –ø–æ—Å–ª–µ–¥–Ω—è—è –ø–æ–ø—ã—Ç–∫–∞
                            import time
                            time.sleep(1)  # –ñ–¥–µ–º —Å–µ–∫—É–Ω–¥—É –ø–µ—Ä–µ–¥ –ø–æ–≤—Ç–æ—Ä–æ–º
                        continue
                else:
                    # –í—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –Ω–µ—É–¥–∞—á–Ω—ã
                    raise last_error or Exception("ChromaDB initialization failed after 3 attempts")
                        
            except ImportError as e:
                logger.warning(f"sentence-transformers not available: {e}")
                logger.info("üîÑ Falling back to SimpleVectorDB...")
                raise ImportError("sentence-transformers missing")
                
        else:
            logger.info("üìÅ Using SimpleVectorDB (ChromaDB disabled)")
            from services.document_processor import DocumentService
            document_service = DocumentService(settings.SIMPLE_DB_PATH)
            SERVICES_AVAILABLE = True
            CHROMADB_ENABLED = False
            logger.info("‚úÖ SimpleVectorDB initialized")
        
        _document_service_initialized = True
        return document_service
        
    except Exception as e:
        logger.error(f"‚ùå Document service initialization failed: {e}")
        logger.info("üîÑ Using fallback document service...")
        
        # –°–æ–∑–¥–∞–µ–º —É–ª—É—á—à–µ–Ω–Ω—ã–π fallback
        try:
            document_service = FallbackDocumentService()
            document_service.initialization_error = str(e)
            SERVICES_AVAILABLE = True
            CHROMADB_ENABLED = False
            _document_service_initialized = True
            
            logger.info("‚úÖ Fallback document service initialized")
            return document_service
            
        except Exception as fallback_error:
            logger.error(f"‚ùå Even fallback service failed: {fallback_error}")
            # –°–æ–∑–¥–∞–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π fallback
            document_service = type('MinimalFallback', (), {
                'search': lambda *args, **kwargs: [],
                'get_stats': lambda: {"error": "Service unavailable"},
                'get_all_documents': lambda: [],
                'delete_document': lambda *args: False,
                'process_and_store_file': lambda *args: False
            })()
            _document_service_initialized = True
            return document_service

def _init_scraper_service():
    """–õ–µ–Ω–∏–≤–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è scraper service"""
    global scraper, _scraper_initialized
    
    if _scraper_initialized:
        return scraper
    
    try:
        logger.info("üîÑ Initializing scraper service...")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –±–∏–±–ª–∏–æ—Ç–µ–∫–∏
        import aiohttp
        import bs4
        from services.scraper_service import LegalSiteScraper
        
        scraper = LegalSiteScraper()
        logger.info("‚úÖ Real scraper service initialized")
        
    except ImportError as e:
        logger.info(f"Scraper libraries not available: {e}")
        from app.dependencies import FallbackScraperService
        scraper = FallbackScraperService()
        logger.info("‚úÖ Fallback scraper service initialized")
    
    _scraper_initialized = True
    return scraper

def _init_llm_service():
    """–õ–µ–Ω–∏–≤–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è LLM service —Å —É–ø—Ä–æ—â–µ–Ω–Ω–æ–π –ª–æ–≥–∏–∫–æ–π –¥–ª—è HF Spaces"""
    global llm_service, _llm_service_initialized, LLM_ENABLED
    
    if _llm_service_initialized:
        return llm_service
    
    try:
        logger.info("üîÑ Initializing LLM service...")
        
        if settings.LLM_DEMO_MODE:
            logger.info("üé≠ LLM demo mode enabled")
            llm_service = ImprovedFallbackLLMService()
            LLM_ENABLED = False
            _llm_service_initialized = True
            return llm_service
        
        # –ü—Ä–æ—Å—Ç–∞—è –ø–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ GPTQ –º–æ–¥–µ–ª–∏
        logger.info("ü§ñ Attempting to load GPTQ model: TheBloke/Llama-2-7B-Chat-GPTQ")
        
        try:
            from services.huggingface_llm_service import create_llm_service
            import time
            
            # –ü—Ä–æ—Å—Ç–∞—è –ø–æ–ø—ã—Ç–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —Å–µ—Ä–≤–∏—Å–∞ —Å timeout
            start_time = time.time()
            timeout_seconds = 30  # 30 —Å–µ–∫—É–Ω–¥ –Ω–∞ –ø–æ–ø—ã—Ç–∫—É –∑–∞–≥—Ä—É–∑–∫–∏
            
            try:
                llm_service = create_llm_service("TheBloke/Llama-2-7B-Chat-GPTQ")
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –º–æ–¥–µ–ª—å –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –∑–∞–≥—Ä—É–∑–∏–ª–∞—Å—å
                if hasattr(llm_service, 'model_loaded') and llm_service.model_loaded:
                    LLM_ENABLED = True
                    logger.info("‚úÖ GPTQ model loaded successfully!")
                else:
                    # –ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∑–∏–ª–∞—Å—å, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback
                    logger.info("‚è≥ GPTQ model not ready yet, using fallback")
                    llm_service = ImprovedFallbackLLMService()
                    LLM_ENABLED = False
                    
            except Exception as model_error:
                elapsed = time.time() - start_time
                if elapsed > timeout_seconds:
                    logger.warning(f"‚è∞ GPTQ model loading timeout ({elapsed:.1f}s), using fallback")
                else:
                    logger.warning(f"‚ö†Ô∏è GPTQ model loading failed: {model_error}")
                
                llm_service = ImprovedFallbackLLMService()
                LLM_ENABLED = False
                
        except ImportError as e:
            logger.warning(f"HuggingFace dependencies not available: {e}")
            llm_service = ImprovedFallbackLLMService()
            LLM_ENABLED = False
            
    except Exception as e:
        logger.error(f"‚ùå LLM service initialization failed: {e}")
        llm_service = ImprovedFallbackLLMService()
        LLM_ENABLED = False
    
    _llm_service_initialized = True
    logger.info(f"‚úÖ LLM service initialized (GPTQ enabled: {LLM_ENABLED})")
    return llm_service

# ====================================
# –£–ë–ò–†–ê–ï–ú –ê–°–ò–ù–•–†–û–ù–ù–£–Æ –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Æ
# ====================================

async def init_services():
    """–£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è - —Ç–æ–ª—å–∫–æ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ñ–ª–∞–≥–∏"""
    global SERVICES_AVAILABLE
    
    logger.info("üöÄ Lazy initialization enabled for HuggingFace Spaces")
    logger.info("üì¶ Services will initialize on first request")
    
    # –ü—Ä–æ—Å—Ç–æ –ø–æ–º–µ—á–∞–µ–º —á—Ç–æ —Å–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞ –∫ lazy loading
    SERVICES_AVAILABLE = True
    
    logger.info("‚úÖ Lazy initialization configured successfully")

# ====================================
# DEPENDENCY FUNCTIONS —Å LAZY LOADING
# ====================================

def get_document_service():
    """Dependency –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è document service —Å lazy loading"""
    return _init_document_service()

def get_scraper_service():
    """Dependency –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è scraper service —Å lazy loading"""
    return _init_scraper_service()

def get_llm_service():
    """Dependency –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è LLM service —Å lazy loading"""
    return _init_llm_service()

def get_services_status():
    """–°—Ç–∞—Ç—É—Å –≤—Å–µ—Ö —Å–µ—Ä–≤–∏—Å–æ–≤ —Å lazy evaluation"""
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —Å–µ—Ä–≤–∏—Å—ã –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã
    doc_service = document_service if _document_service_initialized else None
    scraper_service = scraper if _scraper_initialized else None
    llm_srv = llm_service if _llm_service_initialized else None
    
    return {
        "document_service_available": doc_service is not None,
        "scraper_available": scraper_service is not None,
        "llm_available": LLM_ENABLED,
        "llm_service_created": llm_srv is not None,
        "chromadb_enabled": CHROMADB_ENABLED,
        "services_available": SERVICES_AVAILABLE,
        "huggingface_spaces": os.getenv("SPACE_ID") is not None,
        "environment": "hf_spaces" if os.getenv("SPACE_ID") else "local",
        "demo_mode": not LLM_ENABLED,
        "lazy_loading": True,
        "gptq_model": "TheBloke/Llama-2-7B-Chat-GPTQ",
        "initialization_status": {
            "document_service": _document_service_initialized,
            "scraper_service": _scraper_initialized,
            "llm_service": _llm_service_initialized
        },
        "real_features": {
            "vector_search": CHROMADB_ENABLED,
            "web_scraping": _scraper_initialized and not isinstance(scraper, type(None)),
            "ai_responses": LLM_ENABLED
        }
    }

# ====================================
# FALLBACK –°–ï–†–í–ò–°–´ (–æ—Å—Ç–∞–≤–ª—è–µ–º –∫–∞–∫ –µ—Å—Ç—å)
# ====================================

class FallbackDocumentService:
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π fallback document service –¥–ª—è HF Spaces"""
    
    def __init__(self):
        self.service_type = "hf_spaces_fallback_improved"
        self.vector_db = type('MockVectorDB', (), {
            'persist_directory': './fallback_db'
        })()
        self.initialization_error = None
        self.demo_documents_count = 3
        logger.info("üìù Using improved fallback document service")
    
    async def search(self, query: str, category: str = None, limit: int = 5, min_relevance: float = 0.3):
        """–£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ —Å –±–æ–ª–µ–µ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–º–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏"""
        logger.info(f"üîç Fallback search for: '{query}'")
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –¥–µ–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        demo_results = []
        
        # –ë–∞–∑–æ–≤—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–º –∫–æ–Ω—Ç–µ–Ω—Ç–æ–º
        base_content = f"""Legal Analysis for Query: "{query}"

üèõÔ∏è **Document Summary:**
This document contains legal information relevant to your search query. In a fully operational system, this would be actual content from legal databases.

üìã **Key Points:**
‚Ä¢ Query: "{query}"
‚Ä¢ Category: {category or "General Legal"}
‚Ä¢ Search Method: Semantic vector search (when ChromaDB available)
‚Ä¢ Relevance: High match found

‚öñÔ∏è **Legal Context:**
The system would analyze multiple legal documents, statutes, and case law to provide comprehensive answers. This includes:
- Relevant legislation and regulations
- Court decisions and precedents  
- Administrative guidelines
- Legal commentary and analysis

üîß **Current Status:**
ChromaDB service is initializing. This demo shows the expected response format and structure.

üí° **Note:** Full vector search capabilities will be available once the document service completes initialization."""
        
        demo_results.append({
            "content": base_content,
            "filename": f"legal_analysis_{query.replace(' ', '_')[:20]}.txt",
            "document_id": f"demo_{int(time.time())}",
            "relevance_score": 0.92,
            "metadata": {
                "status": "demo_response",
                "category": category or "general",
                "service": "improved_fallback",
                "query": query,
                "demo_mode": True,
                "expected_features": [
                    "Semantic vector search",
                    "Multi-document analysis", 
                    "Legal citation extraction",
                    "Relevance scoring"
                ],
                "initialization_error": self.initialization_error
            }
        })
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –µ—Å–ª–∏ –Ω—É–∂–Ω–æ –±–æ–ª—å—à–µ
        if limit > 1:
            for i in range(1, min(limit, self.demo_documents_count)):
                secondary_content = f"""Related Legal Document #{i + 1}

Query: "{query}"
Document Type: {["Statute", "Case Law", "Regulation"][i % 3]}

This would be additional relevant legal content found through vector search. Each document would be ranked by semantic similarity to your query.

Status: Demo mode - ChromaDB initializing..."""

                demo_results.append({
                    "content": secondary_content,
                    "filename": f"related_doc_{i+1}_{query.replace(' ', '_')[:15]}.txt", 
                    "document_id": f"demo_{int(time.time())}_{i}",
                    "relevance_score": max(0.4, 0.9 - (i * 0.15)),
                    "metadata": {
                        "status": "demo_response",
                        "category": category or "general",
                        "service": "improved_fallback",
                        "demo_mode": True
                    }
                })
        
        return demo_results
    
    async def get_stats(self):
        """–£–ª—É—á—à–µ–Ω–Ω–∞—è –¥–µ–º–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–æ–π"""
        return {
            "total_documents": 0,
            "categories": ["general", "legislation", "jurisprudence", "government"],
            "database_type": "ChromaDB (initializing...)",
            "status": "Document service starting up",
            "message": "Vector search will be available shortly",
            "initialization_error": self.initialization_error,
            "fallback_info": {
                "demo_responses": True,
                "expected_functionality": [
                    "‚úÖ REST API structure", 
                    "‚úÖ Search endpoint responses",
                    "‚úÖ Document upload endpoints",
                    "‚è≥ ChromaDB vector search",
                    "‚è≥ Real document processing",
                    "‚è≥ Semantic similarity scoring"
                ]
            },
            "troubleshooting": {
                "common_issues": [
                    "sentence-transformers installation",
                    "ChromaDB persistence on HF Spaces", 
                    "Memory limitations during startup",
                    "Model loading timeouts"
                ],
                "recommendations": [
                    "Service will auto-recover when dependencies load",
                    "Demo responses show expected API structure",
                    "Full functionality available after initialization"
                ]
            }
        }
    
    async def get_all_documents(self):
        """–î–µ–º–æ —Å–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        return []
    
    async def delete_document(self, doc_id: str):
        """–î–µ–º–æ —É–¥–∞–ª–µ–Ω–∏–µ"""
        logger.info(f"Demo delete: {doc_id}")
        return False
    
    async def process_and_store_file(self, file_path: str, category: str = "general"):
        """–î–µ–º–æ –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–∞"""
        logger.info(f"Demo file processing: {file_path}")
        return False

class FallbackScraperService:
    """Web scraper fallback"""
    
    def __init__(self):
        self.service_type = "hf_spaces_scraper"
        self.legal_sites_config = {
            "irishstatutebook.ie": {"title": "h1", "content": ".content"},
            "citizensinformation.ie": {"title": "h1", "content": ".content"},
            "zakon.rada.gov.ua": {"title": "h1", "content": ".content"}
        }
        logger.info("üåê HF Spaces scraper service initialized")
    
    async def scrape_legal_site(self, url: str):
        """–î–µ–º–æ —Å–∫—Ä–∞–ø–∏–Ω–≥"""
        logger.info(f"üîç Demo scraping: {url}")
        
        demo_content = f"""üìÑ **Legal Document from {url}**

This is a demonstration of web scraping functionality.
Real scraping service is initializing...

**Document Source:** {url}
**Status:** Scraper loading aiohttp and beautifulsoup4...

The full version will extract actual legal text from websites."""
        
        return type('DemoDocument', (), {
            'url': url,
            'title': f'Demo Legal Document from {url}',
            'content': demo_content,
            'metadata': {
                'status': 'demo',
                'real_scraping': False,
                'scraped_at': time.time(),
                'service': 'hf_spaces_demo',
                'url': url
            },
            'category': 'demo'
        })()
    
    async def scrape_multiple_urls(self, urls: List[str], delay: float = 1.0):
        """–î–µ–º–æ –º–∞—Å—Å–æ–≤—ã–π —Å–∫—Ä–∞–ø–∏–Ω–≥"""
        results = []
        for url in urls:
            doc = await self.scrape_legal_site(url)
            results.append(doc)
        return results