# ====================================
# ФАЙЛ: backend/requirements.txt (ОБНОВЛЕННЫЙ С LLM)
# Заменить существующий файл
# ====================================

# Core FastAPI - обновлены версии
fastapi==0.104.1
uvicorn[standard]==0.24.0
python-multipart==0.0.6
pydantic==2.5.0
pydantic-settings==2.1.0  # ДОБАВЛЕНО для новой версии pydantic
httpx==0.25.2
python-dotenv==1.0.0

# Database
sqlalchemy==2.0.23
databases==0.8.0
aiosqlite==0.19.0

# Vector Database - ChromaDB
chromadb==0.4.18
sentence-transformers==2.2.2

# ====================================
# LLM И AI ЗАВИСИМОСТИ
# ====================================

# Async HTTP клиент для Ollama API
aiohttp==3.9.1
aiofiles==23.2.0  # ДОБАВЛЕНО для работы с файлами

# JSON обработка (быстрее стандартного json)
orjson==3.9.10

# Для обработки HTTP запросов и таймаутов
urllib3==2.1.0

# Опциональные AI библиотеки (раскомментировать при необходимости):
# openai==1.3.5                    # Если нужна поддержка OpenAI API
# anthropic==0.7.7                 # Если нужна поддержка Claude API
# transformers==4.35.2             # Для локальных Hugging Face моделей
# torch>=2.0.0                     # PyTorch для трансформеров (очень большой!)

# Web Scraping - ОБНОВЛЕНО для лучшей совместимости
beautifulsoup4==4.12.2
lxml==4.9.3  # ДОБАВЛЕНО для лучшего парсинга HTML
# selenium==4.15.2  # ЗАКОММЕНТИРОВАНО - слишком тяжелый для базовой версии
# scrapy==2.11.0    # ЗАКОММЕНТИРОВАНО - не используется в текущей реализации

# Document Processing
pypdf2==3.0.1
python-docx==1.1.0
markdown==3.5.1
# python-magic==0.4.27  # ЗАКОММЕНТИРОВАНО - проблемы с установкой на Windows

# Text Processing - оптимизировано
# nltk==3.8.1        # ЗАКОММЕНТИРОВАНО - не используется напрямую
# spacy==3.7.2       # ЗАКОММЕНТИРОВАНО - слишком тяжелый
langdetect==1.0.9

# Optional: For Ukrainian language support
# uk-stemmer==1.2.0  # ЗАКОММЕНТИРОВАНО - нишевая библиотека

# Machine Learning & Embeddings - оптимизировано
numpy>=1.24.0
# torch>=1.13.0      # ЗАКОММЕНТИРОВАНО - очень большой размер, включен в трансформеры

# ====================================
# НОВЫЕ УЛУЧШЕНИЯ ДЛЯ LLM ВЕРСИИ
# ====================================

# System monitoring
psutil==5.9.6

# File type detection (альтернатива python-magic)
filetype==1.2.0

# Better datetime handling
pendulum==2.1.2

# Progress bars for long operations
tqdm==4.66.1

# Improved JSON schema validation
jsonschema==4.20.0

# Better error handling and retry logic
tenacity==8.2.3

# Memory usage optimization
memory-profiler==0.61.0

# Caching для LLM ответов
cachetools==5.3.2

# Rate limiting
limits==3.6.0

# ====================================
# ОПЦИОНАЛЬНЫЕ ЗАВИСИМОСТИ
# ====================================

# Для production deployments:
# gunicorn==21.2.0
# gevent==23.9.1

# Для расширенного логирования:
# structlog==23.2.0
# colorlog==6.8.0

# Для кэширования в production:
# redis==5.0.1
# aioredis==2.0.1

# Для метрик и мониторинга:
# prometheus-client==0.19.0

# Для тестирования:
# pytest==7.4.3
# pytest-asyncio==0.21.1

# ====================================
# ИНСТРУКЦИИ ПО УСТАНОВКЕ OLLAMA
# ====================================

# Для работы с LLM необходимо установить Ollama:
# 
# 1. Скачайте Ollama с https://ollama.ai
# 
# 2. Установите нужную модель:
#    ollama pull llama3.2
#    ollama pull llama3.1  # альтернативная модель
#    ollama pull mistral   # более быстрая модель
# 
# 3. Проверьте что Ollama запущен:
#    curl http://localhost:11434/api/tags
# 
# 4. Перезапустите Legal Assistant:
#    python main.py
#
# ====================================
# РЕКОМЕНДУЕМЫЕ МОДЕЛИ OLLAMA
# ====================================
#
# Быстрые модели (< 4GB RAM):
# - llama3.2:1b         # Очень быстрая, базовое качество
# - phi3:mini           # Маленькая модель Microsoft
#
# Сбалансированные модели (4-8GB RAM):
# - llama3.2            # Рекомендуемая по умолчанию
# - mistral             # Хорошая альтернатива
# - gemma2:2b           # Модель Google
#
# Высокое качество (8GB+ RAM):
# - llama3.1:8b         # Лучше качество
# - llama3:8b           # Проверенная модель
# - codellama:7b        # Специализирована на коде
#
# Команды для установки:
# ollama pull llama3.2     # Базовая установка
# ollama pull mistral      # Альтернатива
# ollama pull llama3.1:8b  # Для лучшего качества
#
# ====================================
# МИНИМАЛЬНЫЕ СИСТЕМНЫЕ ТРЕБОВАНИЯ
# ====================================
#
# Для LLM функциональности:
# - RAM: 4GB+ (8GB+ рекомендуется)
# - CPU: 2+ ядра
# - Диск: 5GB+ свободного места для модели
# - ОС: Windows 10+, macOS 10.15+, Ubuntu 18.04+
#
# Без LLM (только поиск):
# - RAM: 2GB+
# - CPU: 1+ ядро
# - Диск: 1GB+ свободного места
#
# ====================================
# ПЕРЕМЕННЫЕ ОКРУЖЕНИЯ
# ====================================
#
# Создайте файл .env в корне проекта:
#
# # Ollama настройки
# OLLAMA_ENABLED=true
# OLLAMA_BASE_URL=http://localhost:11434
# OLLAMA_DEFAULT_MODEL=llama3.2
# 
# # LLM параметры
# LLM_TEMPERATURE=0.3
# LLM_MAX_TOKENS=1500
# LLM_DEMO_MODE=false
# 
# # Общие настройки
# LOG_LEVEL=INFO
# USE_CHROMADB=true
#
# ====================================
# УСТАНОВКА И ЗАПУСК
# ====================================
#
# 1. Установите Python зависимости:
#    pip install -r requirements.txt
#
# 2. Установите и запустите Ollama (см. инструкции выше)
#
# 3. Запустите сервер:
#    python main.py
#
# 4. Откройте в браузере:
#    http://localhost:8000/docs
#
# 5. Тестируйте чат:
#    POST http://localhost:8000/api/user/chat
#    {
#      "message": "What are statutory rules?",
#      "language": "en"
#    }