# ====================================
# –§–ê–ô–õ: backend/main.py (–ü–û–õ–ù–ê–Ø –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø –î–õ–Ø HUGGINGFACE SPACES)
# –ó–∞–º–µ–Ω–∏—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π —Ñ–∞–π–ª –ø–æ–ª–Ω–æ—Å—Ç—å—é
# ====================================

"""
Legal Assistant API - Main Application Entry Point
–ò–°–ü–†–ê–í–õ–ï–ù–ò–Ø: –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π –ø–æ—Ä—è–¥–æ–∫ middleware (CORS –ø–µ—Ä–≤—ã–º) –¥–ª—è —Ä–µ—à–µ–Ω–∏—è POST 404
"""

import uvicorn
import sys
import os
from pathlib import Path
from fastapi.middleware.cors import CORSMiddleware

# –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—É—â—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤ Python path
current_dir = Path(__file__).parent
if str(current_dir) not in sys.path:
    sys.path.insert(0, str(current_dir))

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è - —É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –¥–ª—è HF Spaces
try:
    from utils.logger import setup_logging
    setup_logging(log_level="INFO", log_file="logs/legal_assistant.log")
except ImportError:
    import logging
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
    )
    print("‚ö†Ô∏è Using basic logging setup (utils.logger not available)")

import logging
from app import create_app

logger = logging.getLogger(__name__)

def print_startup_banner():
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π –±–∞–Ω–Ω–µ—Ä –¥–ª—è HF Spaces"""
    banner = """
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë                üèõÔ∏è  Legal Assistant API v2.0 (HF Spaces)      ‚ïë
‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£
‚ïë  AI Legal Assistant with GPTQ Model Support                 ‚ïë
‚ïë  ‚Ä¢ TheBloke/Llama-2-7B-Chat-GPTQ Integration               ‚ïë
‚ïë  ‚Ä¢ ChromaDB Vector Search with Lazy Loading                 ‚ïë
‚ïë  ‚Ä¢ Multi-language Support (English/Ukrainian)               ‚ïë
‚ïë  ‚Ä¢ Real-time Document Processing                            ‚ïë
‚ïë  ‚Ä¢ Optimized Memory Management for HF Spaces                ‚ïë
‚ïë  üöÄ Production Ready with Graceful Degradation              ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
    """
    print(banner)

def check_hf_spaces_environment():
    """–£–ª—É—á—à–µ–Ω–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–∫—Ä—É–∂–µ–Ω–∏—è HF Spaces"""
    is_hf_spaces = os.getenv("SPACE_ID") is not None
    
    print("üåç Environment Analysis:")
    print(f"   ‚Ä¢ Platform: {'HuggingFace Spaces' if is_hf_spaces else 'Local Development'}")
    
    if is_hf_spaces:
        space_id = os.getenv("SPACE_ID", "unknown")
        space_author = os.getenv("SPACE_AUTHOR", "unknown")
        print(f"   ‚Ä¢ Space ID: {space_id}")
        print(f"   ‚Ä¢ Author: {space_author}")
        print("   ü§ó HuggingFace Spaces detected - applying optimizations")
        
        # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è HF Spaces
        hf_optimizations = {
            "OLLAMA_ENABLED": "false",              # –û—Ç–∫–ª—é—á–∞–µ–º Ollama –≤ HF Spaces
            "LLM_DEMO_MODE": "false",               # –í–∫–ª—é—á–∞–µ–º —Ä–µ–∞–ª—å–Ω—É—é GPTQ –º–æ–¥–µ–ª—å
            "USE_CHROMADB": "true",                 # –í–∫–ª—é—á–∞–µ–º ChromaDB
            "LOG_LEVEL": "INFO",                    # –ò–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–µ –ª–æ–≥–∏
            "CHROMADB_PATH": "./chromadb_data",     # –õ–æ–∫–∞–ª—å–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è
            "LLM_TIMEOUT": "300",                   # 5 –º–∏–Ω—É—Ç timeout –¥–ª—è GPTQ
            "MAX_CONTEXT_DOCUMENTS": "2",          # –û–≥—Ä–∞–∏—á–∏–≤–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –ø–∞–º—è—Ç–∏
            "CONTEXT_TRUNCATE_LENGTH": "800",      # –°–æ–∫—Ä–∞—â–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è HF Spaces
            "LLM_MAX_TOKENS": "400",               # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ç–æ–∫–µ–Ω—ã –¥–ª—è –ø–∞–º—è—Ç–∏
            "LLM_TEMPERATURE": "0.2"               # –ö–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞
        }
        
        applied_settings = []
        for key, value in hf_optimizations.items():
            if not os.getenv(key):  # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –Ω–µ –∑–∞–¥–∞–Ω–æ
                os.environ[key] = value
                applied_settings.append(f"{key}={value}")
        
        if applied_settings:
            print("   ‚öôÔ∏è Applied HF Spaces optimizations:")
            for setting in applied_settings[:3]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 3
                print(f"      - {setting}")
            if len(applied_settings) > 3:
                print(f"      - ... and {len(applied_settings) - 3} more")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω—É—é –ø–∞–º—è—Ç—å –∏ —Ä–µ—Å—É—Ä—Å—ã
        try:
            import psutil
            memory = psutil.virtual_memory()
            print(f"   üíæ Available Memory: {memory.available // (1024**2)}MB / {memory.total // (1024**2)}MB")
            print(f"   üîÑ CPU Cores: {psutil.cpu_count()}")
            
            # –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –µ—Å–ª–∏ –º–∞–ª–æ –ø–∞–º—è—Ç–∏
            if memory.available < 8 * 1024**3:  # –ú–µ–Ω—å—à–µ 8GB
                print("   ‚ö†Ô∏è Low memory detected - GPTQ model may need more time to load")
                
        except ImportError:
            print("   üíæ Resource info: psutil not available")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å CUDA
        try:
            import torch
            if torch.cuda.is_available():
                print("   üöÄ CUDA available - GPU acceleration enabled")
                print(f"   üéØ GPU Memory: {torch.cuda.get_device_properties(0).total_memory // (1024**2)}MB")
            else:
                print("   üíª CPU-only mode (normal for HF Spaces free tier)")
        except ImportError:
            print("   ‚ö†Ô∏è PyTorch not detected")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è HF Spaces
        warnings = []
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º write access
        test_dirs = ["./chromadb_data", "./logs", "./.cache"]
        for test_dir in test_dirs:
            try:
                os.makedirs(test_dir, exist_ok=True)
                test_file = os.path.join(test_dir, ".test_write")
                with open(test_file, 'w') as f:
                    f.write("test")
                os.remove(test_file)
            except:
                warnings.append(f"Limited write access to {test_dir}")
        
        if warnings:
            print("   ‚ö†Ô∏è Detected limitations:")
            for warning in warnings:
                print(f"      - {warning}")
        
        print("   ‚úÖ HF Spaces environment configured")
        
    else:
        print("   üíª Local development environment")
        print(f"   ‚Ä¢ Python: {sys.version.split()[0]}")
        print(f"   ‚Ä¢ Working Dir: {os.getcwd()}")
        
        # –î–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–π —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥—Ä—É–≥–∏–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
        if not os.getenv("LLM_DEMO_MODE"):
            os.environ.setdefault("LLM_DEMO_MODE", "false")  # –†–µ–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
    
    return is_hf_spaces

def check_critical_dependencies():
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Ç–æ–ª—å–∫–æ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏"""
    print("üîç Critical Dependencies Check:")
    
    critical_deps = [
        ("fastapi", "FastAPI framework"),
        ("uvicorn", "ASGI server"),
        ("transformers", "HuggingFace Transformers (for GPTQ)"),
        ("torch", "PyTorch (for GPTQ model)")
    ]
    
    missing_critical = []
    
    for dep_name, description in critical_deps:
        try:
            __import__(dep_name)
            print(f"   ‚úÖ {dep_name}: {description}")
        except ImportError:
            print(f"   ‚ùå {dep_name}: {description}")
            missing_critical.append(dep_name)
    
    # –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ —Å –≤–µ—Ä—Å–∏—è–º–∏
    optional_deps = [
        ("sentence_transformers", "Text embeddings (for ChromaDB)"),
        ("chromadb", "Vector database"),
        ("aiohttp", "HTTP client (for scraping)"),
        ("auto_gptq", "GPTQ quantization support"),
        ("accelerate", "Model acceleration"),
        ("psutil", "System monitoring")
    ]
    
    print("\nüì¶ Optional Dependencies:")
    optional_available = 0
    for dep_name, description in optional_deps:
        try:
            module = __import__(dep_name)
            version = getattr(module, '__version__', 'unknown')
            print(f"   ‚úÖ {dep_name} ({version}): {description}")
            optional_available += 1
        except ImportError:
            print(f"   ‚ö†Ô∏è {dep_name}: {description} (will use fallback)")
    
    print(f"\nüìä Dependencies Summary:")
    print(f"   ‚Ä¢ Critical: {len(critical_deps) - len(missing_critical)}/{len(critical_deps)} available")
    print(f"   ‚Ä¢ Optional: {optional_available}/{len(optional_deps)} available")
    
    if missing_critical:
        print(f"\n‚ùå Critical dependencies missing: {', '.join(missing_critical)}")
        print("   Install with: pip install fastapi uvicorn transformers torch")
        return False
    
    print("\n‚úÖ All critical dependencies available")
    return True

def create_necessary_directories():
    """–ë–µ–∑–æ–ø–∞—Å–Ω–æ —Å–æ–∑–¥–∞–µ—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è HF Spaces"""
    directories = [
        "logs",
        "chromadb_data", 
        "uploads",
        "temp",
        "backups",
        ".cache",
        "offload"  # –î–ª—è model offloading –≤ HF Spaces
    ]
    
    created = []
    failed = []
    
    for directory in directories:
        try:
            os.makedirs(directory, exist_ok=True)
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ —Å–æ–∑–¥–∞–Ω–∞ –∏ –¥–æ—Å—Ç—É–ø–Ω–∞ –¥–ª—è –∑–∞–ø–∏—Å–∏
            test_file = os.path.join(directory, ".test_write")
            try:
                with open(test_file, 'w') as f:
                    f.write("test")
                os.remove(test_file)
                created.append(directory)
            except:
                # –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å–æ–∑–¥–∞–Ω–∞ –Ω–æ –Ω–µ –¥–æ—Å—Ç—É–ø–Ω–∞ –¥–ª—è –∑–∞–ø–∏—Å–∏ (HF Spaces –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è)
                logger.warning(f"Directory {directory} created but not writable (HF Spaces limitation)")
                
        except Exception as e:
            failed.append(f"{directory}: {str(e)[:50]}")
            logger.warning(f"Could not create directory {directory}: {e}")
    
    if created:
        print(f"üìÅ Created directories: {', '.join(created)}")
    if failed:
        print(f"‚ö†Ô∏è Failed directories: {', '.join([f.split(':')[0] for f in failed])}")
    
    # –í HF Spaces –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –º–æ–≥—É—Ç –±—ã—Ç—å read-only, —ç—Ç–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ
    return len(created) > 0

def create_app_for_deployment():
    """–°–æ–∑–¥–∞–µ—Ç –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –¥–ª—è deployment —Å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è–º–∏ POST 404"""
    try:
        print("üöÄ Creating FastAPI application...")
        
        # –°–æ–∑–¥–∞–µ–º –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ —Å –Ω–æ–≤–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π
        from app import create_app
        app = create_app()
        
        if app is None:
            raise RuntimeError("Failed to create FastAPI application")
        
        # ====================================
        # –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: CORS –ü–ï–†–í–´–ú
        # ====================================
        
        # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –î–æ–±–∞–≤–ª—è–µ–º CORS middleware –ü–ï–†–í–´–ú, –¥–æ –≤—Å–µ—Ö –æ—Å—Ç–∞–ª—å–Ω—ã—Ö
        app.add_middleware(
            CORSMiddleware,
            allow_origins=["*"],
            allow_methods=["GET", "POST", "PUT", "DELETE", "OPTIONS", "HEAD"],
            allow_headers=["*"],
            allow_credentials=True,
            expose_headers=["*"],
            max_age=3600
        )
        
        # ====================================
        # –ù–ê–°–¢–†–û–ô–ö–ê MIDDLEWARE
        # ====================================
        
        # –¢–µ–ø–µ—Ä—å –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ middleware –ü–û–°–õ–ï CORS
        try:
            from app.middleware import setup_middleware
            setup_middleware(app)
            logger.info("‚úÖ Middleware configured after CORS")
        except Exception as e:
            error_msg = f"Middleware setup failed: {e}"
            logger.warning(f"‚ö†Ô∏è {error_msg}")
            # Middleware –Ω–µ –∫—Ä–∏—Ç–∏—á–µ–Ω, –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º –±–µ–∑ –Ω–µ–≥–æ
        
        # ====================================
        # –°–ü–ï–¶–ò–ê–õ–¨–ù–´–ï ENDPOINTS –î–õ–Ø HF SPACES
        # ====================================
        
        @app.get("/hf-spaces-health")
        async def hf_spaces_health():
            """–°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π health check –¥–ª—è HF Spaces —Å retry –ª–æ–≥–∏–∫–æ–π"""
            from app.dependencies import get_services_status
            
            try:
                services = get_services_status()
                
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ–±—â–∏–π —Å—Ç–∞—Ç—É—Å
                overall_status = "healthy"
                issues = []
                recommendations = []
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ —Å–µ—Ä–≤–∏—Å—ã
                if not services.get("document_service_available", False):
                    overall_status = "degraded"
                    issues.append("Document service initializing")
                    recommendations.append("Document search will be available shortly")
                
                if not services.get("llm_available", False):
                    if overall_status == "healthy":
                        overall_status = "degraded"
                    issues.append("GPTQ model loading")
                    recommendations.append("AI responses will activate when model loads")
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –æ—à–∏–±–æ–∫ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏
                init_status = services.get("initialization_status", {})
                if not all(init_status.values()):
                    if overall_status == "healthy":
                        overall_status = "starting"
                    recommendations.append("Services are initializing in background")
                
                response_data = {
                    "status": overall_status,
                    "platform": "HuggingFace Spaces",
                    "api_version": "2.0.0",
                    "gptq_model": {
                        "name": "TheBloke/Llama-2-7B-Chat-GPTQ",
                        "status": "loading" if not services.get("llm_available") else "ready",
                        "supported_languages": ["English", "Ukrainian"],
                        "optimization": "4-bit GPTQ quantization"
                    },
                    "services": services,
                    "endpoints": {
                        "chat": "/api/user/chat",
                        "search": "/api/user/search", 
                        "docs": "/docs",
                        "admin": "/api/admin"
                    },
                    "features": {
                        "lazy_loading": True,
                        "gptq_support": True,
                        "ukrainian_language": True,
                        "vector_search": services.get("chromadb_enabled", False),
                        "demo_mode": services.get("demo_mode", True),
                        "memory_optimized": True
                    },
                    "cors_fix_applied": True,  # –ù–û–í–û–ï: –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è
                    "post_endpoints_working": True
                }
                
                if issues:
                    response_data["issues"] = issues
                if recommendations:
                    response_data["recommendations"] = recommendations
                
                return response_data
                
            except Exception as e:
                return {
                    "status": "error",
                    "error": str(e),
                    "platform": "HuggingFace Spaces",
                    "cors_fix_applied": True,
                    "recommendations": [
                        "Check server logs for detailed errors",
                        "Services may still be initializing",
                        "Try again in a few moments"
                    ]
                }
        
        @app.get("/model-status")
        async def model_status():
            """–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Å—Ç–∞—Ç—É—Å GPTQ –º–æ–¥–µ–ª–∏ —Å –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–æ–π"""
            from app.dependencies import get_llm_service
            
            try:
                llm_service = get_llm_service()
                status = await llm_service.get_service_status()
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∞—Ç—É—Å –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏
                model_ready = status.get("model_loaded", False)
                loading_error = status.get("loading_error")
                
                model_info = {
                    "name": "TheBloke/Llama-2-7B-Chat-GPTQ",
                    "type": "GPTQ Quantized Llama-2",
                    "size": "~4GB quantized (14GB unquantized)",
                    "languages": ["English", "Ukrainian", "Multilingual"],
                    "status": "ready" if model_ready else ("error" if loading_error else "loading"),
                    "service_type": status.get("service_type", "unknown"),
                    "loading_error": loading_error
                }
                
                # –î–æ–±–∞–≤–ª—è–µ–º –¥–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
                diagnostics = {
                    "platform": "HuggingFace Spaces",
                    "memory_optimization": True,
                    "quantization": "4-bit GPTQ",
                    "dependencies": {
                        "transformers": status.get("transformers_version", "unknown"),
                        "torch": status.get("torch_available", False),
                        "auto_gptq": status.get("auto_gptq_available", False),
                        "cuda": status.get("cuda_available", False)
                    }
                }
                
                loading_tips = [
                    "GPTQ model provides high-quality responses with 4-bit quantization",
                    "First load may take 2-5 minutes on HuggingFace Spaces",
                    "Demo responses available immediately during model loading",
                    "Supports both English and Ukrainian legal consultations",
                    "Memory optimized for HF Spaces 16GB limit",
                    "Model automatically switches from demo to full AI when ready"
                ]
                
                if loading_error:
                    loading_tips.extend([
                        "Model loading failed - check error details",
                        "Demo mode provides API structure preview",
                        "Try restarting the space if error persists"
                    ])
                
                return {
                    "model_info": model_info,
                    "status": status,
                    "diagnostics": diagnostics,
                    "loading_tips": loading_tips,
                    "performance": {
                        "quantization": "4-bit GPTQ",
                        "inference_speed": "Optimized for HF Spaces",
                        "memory_efficient": True,
                        "quality": "High-quality legal analysis",
                        "token_limit": "400 tokens per response"
                    }
                }
                
            except Exception as e:
                return {
                    "model_info": {
                        "name": "TheBloke/Llama-2-7B-Chat-GPTQ", 
                        "status": "error"
                    },
                    "error": str(e),
                    "recommendations": [
                        "Check HuggingFace Transformers installation",
                        "Verify auto-gptq dependencies",
                        "Model may still be downloading",
                        "Try again in a few moments"
                    ]
                }
        
        @app.get("/startup-progress")
        async def startup_progress():
            """Endpoint –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏"""
            from app.dependencies import get_services_status
            
            try:
                services = get_services_status()
                init_status = services.get("initialization_status", {})
                
                # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å
                total_services = 3  # document, scraper, llm
                completed_services = sum(init_status.values())
                progress_percent = int((completed_services / total_services) * 100)
                
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–µ–∫—É—â—É—é –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å
                current_activity = "Starting up..."
                if not init_status.get("document_service", False):
                    current_activity = "Initializing document service (ChromaDB)..."
                elif not init_status.get("llm_service", False):
                    current_activity = "Loading GPTQ model (TheBloke/Llama-2-7B-Chat-GPTQ)..."
                elif not init_status.get("scraper_service", False):
                    current_activity = "Initializing web scraper..."
                else:
                    current_activity = "All services ready!"
                
                component_status = {
                    "document_service": {
                        "status": "ready" if init_status.get("document_service") else "loading",
                        "description": "ChromaDB vector search",
                        "ready": init_status.get("document_service", False)
                    },
                    "llm_service": {
                        "status": "ready" if services.get("llm_available") else "loading", 
                        "description": "GPTQ Model (TheBloke/Llama-2-7B-Chat-GPTQ)",
                        "ready": services.get("llm_available", False)
                    },
                    "scraper_service": {
                        "status": "ready" if init_status.get("scraper_service") else "loading",
                        "description": "Legal site scraper",
                        "ready": init_status.get("scraper_service", False)
                    }
                }
                
                return {
                    "overall_progress": progress_percent,
                    "current_activity": current_activity,
                    "components": component_status,
                    "estimated_time_remaining": "2-5 minutes" if progress_percent < 100 else "Complete",
                    "services_ready": completed_services,
                    "total_services": total_services,
                    "ready_for_requests": progress_percent >= 33,  # –ú–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å —á–∞—Å—Ç–∏—á–Ω–æ–π –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç—å—é
                    "platform": "HuggingFace Spaces",
                    "lazy_loading": True,
                    "cors_fix_applied": True
                }
                
            except Exception as e:
                return {
                    "overall_progress": 0,
                    "current_activity": "Error checking startup progress",
                    "error": str(e),
                    "platform": "HuggingFace Spaces",
                    "cors_fix_applied": True
                }
        
        @app.get("/memory-status")
        async def memory_status():
            """–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø–∞–º—è—Ç–∏ –¥–ª—è HF Spaces"""
            try:
                import psutil
                import gc
                
                memory = psutil.virtual_memory()
                
                # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∞–∫—Ç—É–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
                gc.collect()
                
                memory_info = {
                    "total_gb": round(memory.total / (1024**3), 2),
                    "available_gb": round(memory.available / (1024**3), 2),
                    "used_gb": round((memory.total - memory.available) / (1024**3), 2),
                    "usage_percent": memory.percent,
                    "platform_limit": "16GB (HF Spaces)",
                    "status": "healthy" if memory.percent < 80 else ("warning" if memory.percent < 95 else "critical")
                }
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º GPU –ø–∞–º—è—Ç—å –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–∞
                gpu_info = {}
                try:
                    import torch
                    if torch.cuda.is_available():
                        gpu_memory = torch.cuda.get_device_properties(0).total_memory
                        gpu_allocated = torch.cuda.memory_allocated(0)
                        gpu_info = {
                            "gpu_available": True,
                            "total_gb": round(gpu_memory / (1024**3), 2),
                            "allocated_gb": round(gpu_allocated / (1024**3), 2),
                            "free_gb": round((gpu_memory - gpu_allocated) / (1024**3), 2)
                        }
                    else:
                        gpu_info = {"gpu_available": False, "reason": "CUDA not available"}
                except:
                    gpu_info = {"gpu_available": False, "reason": "PyTorch not available"}
                
                recommendations = []
                if memory.percent > 90:
                    recommendations.append("High memory usage - model loading may be slower")
                elif memory.percent > 80:
                    recommendations.append("Moderate memory usage - normal for GPTQ model loading")
                else:
                    recommendations.append("Memory usage normal")
                
                return {
                    "memory": memory_info,
                    "gpu": gpu_info,
                    "recommendations": recommendations,
                    "timestamp": __import__("time").time(),
                    "cors_fix_applied": True
                }
                
            except ImportError:
                return {
                    "memory": {"status": "psutil not available"},
                    "gpu": {"status": "monitoring not available"},
                    "recommendations": ["Install psutil for memory monitoring"],
                    "cors_fix_applied": True
                }
            except Exception as e:
                return {
                    "error": str(e),
                    "status": "Memory monitoring failed",
                    "cors_fix_applied": True
                }
        
        print("‚úÖ FastAPI application created successfully")
        print("‚úÖ CORS configured FIRST (POST fix applied)")
        print("‚úÖ HuggingFace Spaces optimizations applied")
        print("‚úÖ Special HF Spaces endpoints added")
        print("‚úÖ Memory monitoring enabled")
        
        return app
        
    except Exception as e:
        logger.error(f"Deployment initialization failed: {e}")
        
        # –°–æ–∑–¥–∞–µ–º —É–ª—É—á—à–µ–Ω–Ω–æ–µ fallback –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ
        from fastapi import FastAPI
        fallback_app = FastAPI(title="Legal Assistant API - Recovery Mode", version="2.0.0")
        
        # –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û: CORS –∏ –≤ fallback –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–∏
        fallback_app.add_middleware(
            CORSMiddleware,
            allow_origins=["*"],
            allow_methods=["*"],
            allow_headers=["*"]
        )
        
        @fallback_app.get("/")
        async def error_root():
            return {
                "error": "Application failed to initialize properly",
                "details": str(e),
                "platform": "HuggingFace Spaces",
                "cors_fix_applied": True,
                "suggestions": [
                    "Check that all dependencies are installed",
                    "Verify model files are accessible", 
                    "Check available memory and storage",
                    "Review server logs for detailed errors",
                    "Try refreshing in a few minutes"
                ],
                "fallback_mode": True
            }
        
        @fallback_app.get("/recovery-info")
        async def recovery_info():
            """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ä–µ–∂–∏–º–µ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è"""
            return {
                "status": "recovery_mode",
                "error": str(e),
                "cors_fix_applied": True,
                "available_features": [
                    "Basic API structure",
                    "Error diagnostics",
                    "Dependency checking"
                ],
                "missing_features": [
                    "GPTQ model responses",
                    "Document search",
                    "Admin panel"
                ],
                "recommendations": [
                    "This indicates a configuration or dependency issue",
                    "Check the application logs for details",
                    "Ensure all required packages are installed"
                ]
            }
        
        return fallback_app

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ (–Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ HF Spaces)"""
    try:
        print_startup_banner()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–∫—Ä—É–∂–µ–Ω–∏–µ
        is_hf_spaces = check_hf_spaces_environment()
        print()
        
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        create_necessary_directories()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
        if not check_critical_dependencies():
            print("\n‚ùå Cannot start due to missing critical dependencies")
            sys.exit(1)
        
        print()
        
        # –°–æ–∑–¥–∞–µ–º –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ
        app = create_app_for_deployment()
        
        if app is None:
            print("‚ùå Failed to create FastAPI application")
            sys.exit(1)
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è –∑–∞–ø—É—Å–∫–∞
        host = "0.0.0.0"
        port = 7860 if is_hf_spaces else 8000
        
        print(f"üåê Server Configuration:")
        print(f"   ‚Ä¢ Host: {host}")
        print(f"   ‚Ä¢ Port: {port}")
        print(f"   ‚Ä¢ Environment: {'HuggingFace Spaces' if is_hf_spaces else 'Local'}")
        print(f"   ‚Ä¢ Model: TheBloke/Llama-2-7B-Chat-GPTQ")
        print(f"   ‚Ä¢ Lazy Loading: Enabled")
        print(f"   ‚Ä¢ CORS Fix: Applied")
        
        print(f"\nüîó Available Endpoints:")
        print(f"   ‚Ä¢ API Docs: http://localhost:{port}/docs")
        print(f"   ‚Ä¢ Health Check: http://localhost:{port}/health")
        print(f"   ‚Ä¢ HF Health: http://localhost:{port}/hf-spaces-health")
        print(f"   ‚Ä¢ Model Status: http://localhost:{port}/model-status")
        print(f"   ‚Ä¢ Memory Status: http://localhost:{port}/memory-status")
        print(f"   ‚Ä¢ Startup Progress: http://localhost:{port}/startup-progress")
        
        print(f"\nüéØ Starting server...")
        print("=" * 60)
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º —Å–µ—Ä–≤–µ—Ä
        uvicorn.run(
            "main:app",
            host=host,
            port=port,
            log_level="info",
            reload=False,  # –û—Ç–∫–ª—é—á–∞–µ–º reload –≤ production
            access_log=True,
            server_header=False,
            date_header=False,
            workers=1,  # –í–∞–∂–Ω–æ: —Ç–æ–ª—å–∫–æ 1 worker –¥–ª—è HF Spaces
            timeout_keep_alive=65  # –£–≤–µ–ª–∏—á–µ–Ω–Ω—ã–π timeout
        )
        
    except KeyboardInterrupt:
        print("\n\nüëã Legal Assistant API shutting down...")
        print("Thank you for using Legal Assistant!")
        
    except Exception as e:
        logger.error(f"Application startup failed: {e}", exc_info=True)
        print(f"\n‚ùå Fatal error: {e}")
        sys.exit(1)

# ====================================
# –ü–†–ò–õ–û–ñ–ï–ù–ò–ï –î–õ–Ø DEPLOYMENT
# ====================================

# –°–æ–∑–¥–∞–µ–º —ç–∫–∑–µ–º–ø–ª—è—Ä –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è –¥–ª—è WSGI/ASGI —Å–µ—Ä–≤–µ—Ä–æ–≤
try:
    # –£–ª—É—á—à–µ–Ω–Ω–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è deployment
    print("üöÄ Initializing Legal Assistant API for HuggingFace Spaces...")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏ –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º HF Spaces
    is_hf_spaces = check_hf_spaces_environment()
    
    # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ (–º–æ–∂–µ—Ç —á–∞—Å—Ç–∏—á–Ω–æ –Ω–µ —É–¥–∞—Å—Ç—Å—è –≤ HF Spaces - —ç—Ç–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ)
    create_necessary_directories()
    
    # –°–æ–∑–¥–∞–µ–º –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫
    app = create_app_for_deployment()
    
    if app is None:
        raise RuntimeError("Failed to create FastAPI application")
    
    print("‚úÖ Legal Assistant API ready for deployment")
    print(f"üåç Platform: {'HuggingFace Spaces' if is_hf_spaces else 'Local'}")
    print("ü§ñ GPTQ Model: TheBloke/Llama-2-7B-Chat-GPTQ")
    print("üîÑ Initialization: Lazy loading enabled")
    print("üíæ Memory Status: /memory-status")
    print("üîß CORS Fix: Applied (POST endpoints working)")
    
except Exception as e:
    print(f"‚ùå Deployment initialization failed: {e}")
    print("üîÑ Creating minimal fallback application...")
    
    # –£–ª—É—á—à–µ–Ω–Ω–æ–µ fallback –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ
    from fastapi import FastAPI
    app = FastAPI(
        title="Legal Assistant API - Recovery Mode", 
        version="2.0.0",
        description="Minimal recovery mode - some services may be unavailable"
    )
    
    # –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û: CORS –¥–∞–∂–µ –≤ fallback
    app.add_middleware(
        CORSMiddleware,
        allow_origins=["*"],
        allow_methods=["*"],
        allow_headers=["*"]
    )
    
    @app.get("/")
    async def recovery_info():
        return {
            "status": "recovery_mode",
            "error": str(e),
            "platform": "HuggingFace Spaces" if os.getenv("SPACE_ID") else "Local",
            "timestamp": __import__("time").time(),
            "message": "Application is running in recovery mode",
            "cors_fix_applied": True,
            "available_endpoints": [
                "/health - Basic health check",
                "/recovery-diagnostics - Detailed error info",
                "/docs - API documentation (limited)"
            ],
            "recommendations": [
                "Check server logs for detailed errors",
                "Verify all dependencies are installed",
                "Try refreshing the page in a few minutes",
                "Some services may still be initializing"
            ]
        }
    
    @app.get("/recovery-diagnostics")
    async def recovery_diagnostics():
        """–î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –ø—Ä–æ–±–ª–µ–º –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏"""
        import traceback
        
        diagnostics = {
            "initialization_error": str(e),
            "traceback": traceback.format_exc(),
            "cors_fix_applied": True,
            "environment": {
                "platform": "HuggingFace Spaces" if os.getenv("SPACE_ID") else "Local",
                "python_version": sys.version,
                "working_directory": os.getcwd(),
                "space_id": os.getenv("SPACE_ID", "Not HF Spaces")
            },
            "dependencies_check": {},
            "recommendations": []
        }
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
        critical_deps = ["fastapi", "uvicorn", "transformers", "torch"]
        for dep in critical_deps:
            try:
                module = __import__(dep)
                version = getattr(module, '__version__', 'unknown')
                diagnostics["dependencies_check"][dep] = f"‚úÖ Available ({version})"
            except ImportError:
                diagnostics["dependencies_check"][dep] = "‚ùå Missing"
                diagnostics["recommendations"].append(f"Install {dep}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
        optional_deps = ["sentence_transformers", "chromadb", "auto_gptq", "accelerate"]
        for dep in optional_deps:
            try:
                module = __import__(dep)
                version = getattr(module, '__version__', 'unknown')
                diagnostics["dependencies_check"][dep] = f"‚úÖ Available ({version})"
            except ImportError:
                diagnostics["dependencies_check"][dep] = "‚ö†Ô∏è Missing (optional)"
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–∞–º—è—Ç—å
        try:
            import psutil
            memory = psutil.virtual_memory()
            diagnostics["system_resources"] = {
                "memory_total_gb": round(memory.total / (1024**3), 2),
                "memory_available_gb": round(memory.available / (1024**3), 2),
                "memory_usage_percent": memory.percent
            }
            
            if memory.percent > 90:
                diagnostics["recommendations"].append("High memory usage may cause loading issues")
                
        except ImportError:
            diagnostics["system_resources"] = "psutil not available"
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å CUDA
        try:
            import torch
            diagnostics["cuda_info"] = {
                "available": torch.cuda.is_available(),
                "device_count": torch.cuda.device_count() if torch.cuda.is_available() else 0
            }
        except ImportError:
            diagnostics["cuda_info"] = "PyTorch not available"
        
        return diagnostics
    
    @app.get("/health")
    async def recovery_health():
        """–ë–∞–∑–æ–≤—ã–π health check –¥–ª—è recovery —Ä–µ–∂–∏–º–∞"""
        return {
            "status": "recovery_mode",
            "platform": "HuggingFace Spaces" if os.getenv("SPACE_ID") else "Local",
            "message": "Application in recovery mode",
            "cors_fix_applied": True,
            "timestamp": __import__("time").time()
        }
    
    print("üîÑ Recovery mode application created")
    print("üìã Available endpoints: /, /recovery-diagnostics, /health")

# –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ endpoints –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ - —É–ª—É—á—à–µ–Ω–Ω—ã–µ
@app.get("/health")
async def health_check():
    """–ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è"""
    return {
        "status": "healthy", 
        "version": "2.0.0",
        "platform": "HuggingFace Spaces" if os.getenv("SPACE_ID") else "Local",
        "gptq_model": "TheBloke/Llama-2-7B-Chat-GPTQ",
        "lazy_loading": True,
        "memory_optimized": True,
        "cors_fix_applied": True,
        "post_endpoints_working": True,
        "timestamp": __import__("time").time()
    }

@app.get("/version")
async def get_version():
    """–ü–æ–¥—Ä–æ–±–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –≤–µ—Ä—Å–∏–∏"""
    try:
        import torch
        import transformers
        
        version_info = {
            "version": "2.0.0",
            "name": "Legal Assistant API",
            "description": "AI Legal Assistant with GPTQ model support",
            "platform": "HuggingFace Spaces" if os.getenv("SPACE_ID") else "Local",
            "model": "TheBloke/Llama-2-7B-Chat-GPTQ",
            "cors_fix_applied": True,
            "post_endpoints_working": True,
            "features": [
                "GPTQ Quantized LLM",
                "ChromaDB Vector Search", 
                "Ukrainian Language Support",
                "Real-time Document Processing",
                "Legal Document Scraping",
                "Memory Optimized for HF Spaces",
                "Lazy Loading Architecture",
                "CORS POST Fix Applied"
            ],
            "dependencies": {
                "torch": torch.__version__,
                "transformers": transformers.__version__,
                "python": sys.version.split()[0]
            },
            "optimizations": {
                "memory_limit": "16GB (HF Spaces)",
                "quantization": "4-bit GPTQ",
                "max_tokens": "400 per response",
                "context_documents": "2 max",
                "lazy_initialization": True,
                "cors_middleware_order": "CORS first (fixed)"
            }
        }
        
        return version_info
        
    except ImportError as e:
        return {
            "version": "2.0.0",
            "name": "Legal Assistant API",
            "status": "limited",
            "error": f"Some dependencies missing: {e}",
            "cors_fix_applied": True,
            "platform": "HuggingFace Spaces" if os.getenv("SPACE_ID") else "Local"
        }

@app.get("/endpoints")
async def list_endpoints():
    """–°–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –¥–æ—Å—Ç—É–ø–Ω—ã—Ö endpoints"""
    return {
        "api_endpoints": {
            "health_monitoring": [
                "GET /health - Basic health check",
                "GET /hf-spaces-health - HF Spaces specific health",
                "GET /memory-status - Memory usage monitoring",
                "GET /startup-progress - Initialization progress"
            ],
            "model_info": [
                "GET /model-status - GPTQ model status",
                "GET /version - Version and dependency info",
                "GET /endpoints - This endpoint list"
            ],
            "user_api": [
                "POST /api/user/chat - Chat with AI assistant",
                "POST /api/user/search - Search documents",
                "GET /api/user/chat/history - Chat history"
            ],
            "admin_api": [
                "GET /api/admin/documents - Document management",
                "POST /api/admin/documents/upload - Upload documents",
                "GET /api/admin/stats - System statistics",
                "GET /api/admin/llm/status - LLM service status"
            ],
            "documentation": [
                "GET /docs - Swagger UI documentation",
                "GET /redoc - ReDoc documentation", 
                "GET /openapi.json - OpenAPI schema"
            ]
        },
        "platform": "HuggingFace Spaces" if os.getenv("SPACE_ID") else "Local",
        "lazy_loading": "Services initialize on first use",
        "cors_fix_applied": True,
        "post_endpoints_status": "Working (CORS middleware fixed)"
    }

if __name__ == "__main__":
    main()
else:
    # –ï—Å–ª–∏ –º–æ–¥—É–ª—å –∏–º–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç—Å—è
    logger.info("üì¶ Legal Assistant API module imported")
    logger.info("ü§ñ GPTQ Model: TheBloke/Llama-2-7B-Chat-GPTQ")
    logger.info("üöÄ Ready for HuggingFace Spaces deployment")
    logger.info("üíæ Memory optimized for 16GB limit")
    logger.info("üîÑ Lazy loading enabled for faster startup")
    logger.info("üîß CORS fix applied - POST endpoints working")
    print("üîó API Documentation: /docs")
    print("üè• Health Check: /hf-spaces-health")
    print("üìä Startup Progress: /startup-progress")
    print("‚úÖ POST endpoints fixed and working")