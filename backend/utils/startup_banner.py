# backend/utils/startup_banner.py - Ğ£ĞŸĞ ĞĞ©Ğ•ĞĞĞ«Ğ™ Ğ¡Ğ¢ĞĞ Ğ¢ĞĞŸ
"""
Ğ£Ğ¿Ñ€Ğ¾Ñ‰ĞµĞ½Ğ½Ñ‹Ğµ ÑƒÑ‚Ğ¸Ğ»Ğ¸Ñ‚Ñ‹ ÑÑ‚Ğ°Ñ€Ñ‚Ğ°Ğ¿Ğ° Ğ´Ğ»Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ RAG ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹
Ğ£Ğ±Ñ€Ğ°Ğ½Ñ‹ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ GPTQ, Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ñ‹ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸
"""

import os
import sys
import logging
from pathlib import Path

logger = logging.getLogger(__name__)

def print_minimal_startup_banner():
    """Ğ£Ğ¿Ñ€Ğ¾Ñ‰ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ±Ğ°Ğ½Ğ½ĞµÑ€ Ğ´Ğ»Ñ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ RAG ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹"""
    banner = f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘               ğŸ›ï¸  Minimal Legal RAG System v1.0              â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  ğŸ¤– FLAN-T5 Small + Sentence Transformers + ChromaDB        â•‘
â•‘  ğŸ’¾ Memory Target: <1GB RAM                                  â•‘
â•‘  âš¡ Fast Startup: <60 seconds                                â•‘
â•‘  ğŸŒ Platform: {'HuggingFace Spaces' if _is_hf_spaces() else 'Local Development'}                        â•‘
â•‘  ğŸ“š Features: Semantic Search, Document Upload, Chat        â•‘
â•‘  ğŸ” Embeddings: all-MiniLM-L6-v2 (384D, ~90MB)             â•‘
â•‘  ğŸ—„ï¸ Vector DB: ChromaDB with lazy loading                   â•‘
â•‘  ğŸŒ Multilingual: English + Ukrainian support               â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """
    print(banner)

def check_minimal_environment():
    """Ğ£Ğ¿Ñ€Ğ¾Ñ‰ĞµĞ½Ğ½Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ"""
    is_hf_spaces = _is_hf_spaces()
    
    print("ğŸŒ Environment Check:")
    print(f"   â€¢ Platform: {'HuggingFace Spaces' if is_hf_spaces else 'Local Development'}")
    
    if is_hf_spaces:
        space_id = os.getenv("SPACE_ID", "unknown")
        print(f"   â€¢ Space ID: {space_id}")
        print("   ğŸ¤— HuggingFace Spaces detected - applying optimizations")
        
        # ĞŸÑ€Ğ¾ÑÑ‚Ñ‹Ğµ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ HF Spaces
        optimizations = {
            "USE_CHROMADB": "true",
            "LOG_LEVEL": "INFO",
            "LLM_MODEL": "google/flan-t5-small",
            "LLM_MAX_TOKENS": "150",
            "LLM_TIMEOUT": "20",
            "MAX_CONTEXT_DOCUMENTS": "2",
            "CONTEXT_TRUNCATE_LENGTH": "300"
        }
        
        applied = []
        for key, value in optimizations.items():
            if not os.getenv(key):
                os.environ[key] = value
                applied.append(f"{key}={value}")
        
        if applied:
            print("   âš™ï¸ Applied optimizations:")
            for setting in applied[:3]:  # ĞŸĞ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼ Ğ¿ĞµÑ€Ğ²Ñ‹Ğµ 3
                print(f"      - {setting}")
            if len(applied) > 3:
                print(f"      - ... and {len(applied) - 3} more")
        
        print("   âœ… HF Spaces environment optimized")
        
    else:
        print("   ğŸ’» Local development environment")
        print(f"   â€¢ Python: {sys.version.split()[0]}")
        
        # Ğ”Ğ»Ñ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸
        os.environ.setdefault("LLM_MODEL", "google/flan-t5-small")
        os.environ.setdefault("USE_CHROMADB", "true")
    
    return is_hf_spaces

def check_minimal_dependencies():
    """ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµÑ‚ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸"""
    print("ğŸ” Critical Dependencies Check:")
    
    critical_deps = [
        ("fastapi", "FastAPI framework"),
        ("transformers", "HuggingFace Transformers for FLAN-T5"),
        ("sentence_transformers", "Sentence embeddings"),
        ("chromadb", "Vector database")
    ]
    
    missing_critical = []
    
    for dep_name, description in critical_deps:
        try:
            module = __import__(dep_name)
            version = getattr(module, '__version__', 'unknown')
            print(f"   âœ… {dep_name} ({version}): {description}")
        except ImportError:
            print(f"   âŒ {dep_name}: {description}")
            missing_critical.append(dep_name)
    
    # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ PyTorch
    print("\nğŸ”¥ PyTorch Check:")
    try:
        import torch
        print(f"   âœ… torch ({torch.__version__}): PyTorch framework")
        
        if torch.cuda.is_available():
            print("   ğŸš€ CUDA available - GPU acceleration possible")
        else:
            print("   ğŸ’» CPU-only mode (optimal for minimal setup)")
    except ImportError:
        print("   âŒ torch: PyTorch framework")
        missing_critical.append("torch")
    
    # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
    print("\nğŸ¤– Model Availability Check:")
    model_checks = [
        ("google/flan-t5-small", "LLM model"),
        ("sentence-transformers/all-MiniLM-L6-v2", "Embedding model")
    ]
    
    for model_name, description in model_checks:
        try:
            if "flan-t5" in model_name:
                from transformers import AutoTokenizer
                AutoTokenizer.from_pretrained(model_name)
            else:
                from sentence_transformers import SentenceTransformer
                SentenceTransformer(model_name)
            print(f"   âœ… {model_name}: {description}")
        except Exception as e:
            print(f"   âš ï¸ {model_name}: {description} (will download on first use)")
    
    print(f"\nğŸ“Š Dependencies Summary:")
    print(f"   â€¢ Critical: {len(critical_deps) - len(missing_critical)}/{len(critical_deps)} available")
    print(f"   â€¢ Memory Target: <1GB RAM")
    print(f"   â€¢ Models: FLAN-T5 Small (~300MB) + all-MiniLM-L6-v2 (~90MB)")
    
    if missing_critical:
        print(f"\nâŒ Missing critical dependencies: {', '.join(missing_critical)}")
        print("   Install with: pip install fastapi transformers sentence-transformers chromadb torch")
        return False
    
    print("\nâœ… All critical dependencies available for minimal RAG")
    return True

def create_minimal_directories():
    """Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ½Ğ°Ğ±Ğ¾Ñ€ Ğ´Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¹"""
    directories = [
        "logs",
        "chromadb_data", 
        "uploads",
        "temp",
        ".cache"
    ]
    
    created = []
    failed = []
    
    for directory in directories:
        try:
            os.makedirs(directory, exist_ok=True)
            # ĞŸÑ€Ğ¾ÑÑ‚Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ·Ğ°Ğ¿Ğ¸ÑĞ¸
            test_file = os.path.join(directory, ".test")
            try:
                with open(test_file, 'w') as f:
                    f.write("test")
                os.remove(test_file)
                created.append(directory)
            except:
                # Ğ”Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ° Ğ½Ğ¾ Ğ½Ğµ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ° Ğ´Ğ»Ñ Ğ·Ğ°Ğ¿Ğ¸ÑĞ¸
                logger.warning(f"Directory {directory} created but not writable")
                
        except Exception as e:
            failed.append(f"{directory}: {str(e)[:30]}")
            logger.warning(f"Could not create directory {directory}: {e}")
    
    if created:
        print(f"ğŸ“ Created directories: {', '.join(created)}")
    if failed:
        print(f"âš ï¸ Failed directories: {', '.join([f.split(':')[0] for f in failed])}")
    
    return len(created) > 0

def estimate_memory_usage():
    """ĞÑ†ĞµĞ½ĞºĞ° Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ´Ğ»Ñ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ¸"""
    print("\nğŸ’¾ Memory Usage Estimation:")
    
    components = {
        "FLAN-T5 Small": "~300 MB",
        "all-MiniLM-L6-v2": "~90 MB", 
        "ChromaDB": "~20 MB (per 10K docs)",
        "FastAPI + deps": "~100 MB",
        "Python runtime": "~50 MB",
        "Operating overhead": "~100 MB"
    }
    
    total_mb = 300 + 90 + 20 + 100 + 50 + 100  # 660 MB
    
    for component, usage in components.items():
        print(f"   â€¢ {component}: {usage}")
    
    print(f"\n   ğŸ“Š Total Estimated: ~{total_mb} MB")
    print(f"   ğŸ¯ Target: <1GB (1024 MB)")
    print(f"   âœ… Memory Efficiency: {(total_mb/1024)*100:.1f}% of 1GB target")
    
    return total_mb

def check_model_accessibility():
    """ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµÑ‚ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ HuggingFace"""
    print("\nğŸ”— Model Accessibility Check:")
    
    models_to_check = [
        "google/flan-t5-small",
        "sentence-transformers/all-MiniLM-L6-v2"
    ]
    
    accessible_models = []
    
    for model in models_to_check:
        try:
            # ĞŸÑ€Ğ¾ÑÑ‚Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‡ĞµÑ€ĞµĞ· requests
            import requests
            if "flan-t5" in model:
                url = f"https://huggingface.co/{model}/resolve/main/config.json"
            else:
                url = f"https://huggingface.co/{model}/resolve/main/config.json"
            
            response = requests.head(url, timeout=5)
            if response.status_code == 200:
                print(f"   âœ… {model}: Accessible")
                accessible_models.append(model)
            else:
                print(f"   âš ï¸ {model}: Not accessible (will try download)")
        except Exception as e:
            print(f"   âš ï¸ {model}: Connection check failed")
    
    if len(accessible_models) == len(models_to_check):
        print("   ğŸŒ All models accessible from HuggingFace Hub")
    else:
        print("   âš ï¸ Some models may need to download on first use")
    
    return len(accessible_models) > 0

def print_startup_summary():
    """Ğ’Ñ‹Ğ²Ğ¾Ğ´Ğ¸Ñ‚ Ğ¸Ñ‚Ğ¾Ğ³Ğ¾Ğ²ÑƒÑ ÑĞ²Ğ¾Ğ´ĞºÑƒ ÑÑ‚Ğ°Ñ€Ñ‚Ğ°Ğ¿Ğ°"""
    print("\n" + "="*60)
    print("ğŸš€ Minimal RAG Startup Summary:")
    print("="*60)
    
    # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ
    is_hf_spaces = check_minimal_environment()
    
    # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸
    deps_ok = check_minimal_dependencies()
    
    # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ Ğ´Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸
    dirs_ok = create_minimal_directories()
    
    # ĞÑ†ĞµĞ½ĞºĞ° Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸
    memory_mb = estimate_memory_usage()
    
    # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹
    models_ok = check_model_accessibility()
    
    print("\nğŸ“‹ Final Status:")
    print(f"   â€¢ Environment: {'âœ… Ready' if is_hf_spaces or True else 'âŒ Issues'}")
    print(f"   â€¢ Dependencies: {'âœ… All critical available' if deps_ok else 'âŒ Missing critical'}")
    print(f"   â€¢ Directories: {'âœ… Created' if dirs_ok else 'âš ï¸ Limited access'}")
    print(f"   â€¢ Memory: âœ… {memory_mb} MB (within 1GB target)")
    print(f"   â€¢ Models: {'âœ… Accessible' if models_ok else 'âš ï¸ Will download'}")
    
    overall_status = deps_ok and dirs_ok
    
    if overall_status:
        print("\nğŸ‰ Minimal RAG System ready to start!")
        print("   â€¢ Expected startup time: <60 seconds")
        print("   â€¢ Memory usage: <1GB RAM")
        print("   â€¢ Models: FLAN-T5 Small + all-MiniLM-L6-v2")
    else:
        print("\nâš ï¸ Some issues detected - system may work with limitations")
    
    print("="*60)
    
    return overall_status

def _is_hf_spaces() -> bool:
    """ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµÑ‚ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ğ»Ğ¸ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ HuggingFace Spaces"""
    return os.getenv("SPACE_ID") is not None

def get_startup_recommendations():
    """Ğ’Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ÑÑ‚Ğ°Ñ€Ñ‚Ğ°Ğ¿Ğ°"""
    recommendations = []
    
    # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ
    memory_mb = estimate_memory_usage()
    if memory_mb > 800:  # Ğ•ÑĞ»Ğ¸ Ğ¿Ñ€Ğ¸Ğ±Ğ»Ğ¸Ğ¶Ğ°ĞµĞ¼ÑÑ Ğº Ğ»Ğ¸Ğ¼Ğ¸Ñ‚Ñƒ
        recommendations.append("Consider reducing context length for memory optimization")
    
    # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ
    if _is_hf_spaces():
        recommendations.extend([
            "HF Spaces detected - optimizations applied automatically",
            "Models will download to .cache directory on first use",
            "Expect 2-3 minute first startup for model downloads"
        ])
    else:
        recommendations.extend([
            "Local development - ensure good internet for model downloads",
            "Consider pre-downloading models: huggingface-cli download",
            "Use nvidia-smi to monitor GPU usage if available"
        ])
    
    # ĞĞ±Ñ‰Ğ¸Ğµ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸
    recommendations.extend([
        "Monitor memory usage in production",
        "Use shorter questions for faster responses",
        "Upload relevant documents for better RAG performance"
    ])
    
    return recommendations

# ====================================
# Ğ­ĞšĞ¡ĞŸĞĞ Ğ¢
# ====================================

__all__ = [
    "print_minimal_startup_banner",
    "check_minimal_environment", 
    "check_minimal_dependencies",
    "create_minimal_directories",
    "estimate_memory_usage",
    "check_model_accessibility",
    "print_startup_summary",
    "get_startup_recommendations"
]